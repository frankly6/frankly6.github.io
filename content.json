{"posts":[{"title":"CS61A 笔记","text":"包含简单的 Python, Scheme, SQL 和编程思想的学习 中文课本链接 / 英文版 课程官网 我的 Projects 代码库 函数构建抽象 交互模式的 python: python -i xxx.py 主要是这个 -i 起作用(interact?) 定义函数: 1234def f(x): #f(x, y), f(), ... return ... #eg. &quot;return a, b, c&quot; then you can code &quot;i, j, k=f(x)&quot;#----------OR----------g = f 导入: 123from math import pifrom operator import add, mulfrom math import * #不要全导进来, 关键字太多, 不同包还可能重复 多元素赋值: 12a, b = 1, 2b, a = a+b, b #b=3, a=2 then 当使用一个名称时，程序会在当前的环境中查找该名称绑定的值，如果没有找到，则会向外层环境继续查找，直到找到为止。(local frame -&gt; global frame)(局部 -&gt; 全局) print 与 None: 1234&gt;&gt;&gt; print(print(1), print(2))12None None print() 没有返回量, 返回 None, 最外层 print() 的 None 不会显示, 自己定义没有返回量的函数最后要 return None 整除符号 \\(//\\)​​ , 也可以用 operator 里的 floordiv(a, b) 替代 数的次方符号 \\(**\\) , 例如 \\(10 ** \\;1000 = 10^{1000}\\) 函数传默认参(如果没有传参进来, 用默认参, 参数从左到右依次传): 123def f(x, y=10): return x+ya = f(10) #a = 20a = f(10, 15) #a = 25 假设语句 if/elif/else (elif = else if) , 不加括号, 直接 if &lt;condition&gt;: ... 迭代-while语句: (下面是一个斐波那契数列的程序, 可以玩玩, 我真心希望它能加载出来) 文档测试(Doctests): 文档字符串的第一行应该包含函数的单行描述，接着是一个空行，下面可能是参数和函数意图的详细描述。此外，文档字符串可能包含调用该函数的交互式会话示例: 123456789101112&gt;&gt;&gt; def sum_naturals(n): &quot;&quot;&quot;返回前 n 个自然数的和。 &gt;&gt;&gt; sum_naturals(10) 55 &gt;&gt;&gt; sum_naturals(100) 5050 &quot;&quot;&quot; total, k = 0, 1 while k &lt;= n: total, k = total + k, k + 1 return total 然后，可以通过 doctest 模块 来验证交互，如下: 123&gt;&gt;&gt; from doctest import testmod&gt;&gt;&gt; testmod()TestResults(failed=0, attempted=2) assert语句: 与其在程序运行时崩溃, 不如在出现错误条件时就崩溃。 1assert a&gt;0, 'a 必须为正数' #在a&gt;0时向下运行, 在a&lt;=0时抛出崩溃信息 高阶函数: 例如在求正方形, 正六边形, 圆形的面积时, 其面积都可以表达为 \\(C \\times r^2\\) 其中 \\(C\\) 是一个常数, 我们可以把这个常数在代码中定义为一个函数(同理也有 \\(f(x) \\times r^2\\)), 这个函数可以被传入求面积的函数中, 求面积的函数就称为高阶函数。 函数内定义函数: 1234def make_adder(n): def adder(k): return k+n return adder #make_adder(1)(2) = 3 或者 f=make_adder(1), f(2)=3 Lambda 表达式: 其结果是一个匿名函数(Lambda 函数), 让你不用想一个函数名, 让代码简洁难懂 12def make_adder(n): return lambda k: n + k 123x = 10square = x*x #square 是一个值, square=100square = lambda x: x*x #square 是一个函数, square(10)=100, square(4)=16 1234def compose(f, g): return lambda x: f(g(x))f = compose(lambda x: x*x, lambda y: y+1)ans=f(12) #ans=169, guess why? 12345678910def search(f): x=0 while not f(x): x+=1 return xdef square(x): return x*xdef inverse(f): return lambda y: search(lambda x:f(x)==y) &quot;&quot;&quot;Return g(y) such that g(f(x)) -&gt; x.&quot;&quot;&quot;#----------------------------------------------&gt;&gt;&gt; sqrt = inverse(square)&gt;&gt;&gt; sqrt(256) #y = 256, then search(lambda x:square(x)==256) -&gt; return 1616 与/或(\\(and/or\\))运算是按顺序的, 若前面不满足条件则不会往下运算(可以被\"短路\") 函数调用会先检查所有传参, 所以不能(不建议)用函数来创造三目运算符 a if b else c: 如果 b 为真, 执行 a, 否则执行 c *args:不定参数, 不知道有几个参数会发过来 f(1,2,3) = f(*[1,2,3]) , *起到了压缩/解压参数列表的作用 (不准发键值对, 否则用 **kwargs (接收 N 个关键字参数, 转换成字典 dict 形式) ): 1234567891011121314&gt;&gt;&gt; def printed(f):... def print_and_return(*args):... result = f(*args)... print('Result:', result)... return result... return print_and_return&gt;&gt;&gt; printed_pow = printed(pow)&gt;&gt;&gt; printed_pow(2, 8) # *args represents the arguments (2, 8)Result: 256256&gt;&gt;&gt; printed_abs = printed(abs)&gt;&gt;&gt; printed_abs(-10) # *args represents one argument (-10)Result: 1010 递归函数, 进行自调用: 12345678910def print_sum(x): print(x) def next_sum(y): return print_sum(x+y) return next_sumprint_sum(1)(3)(5) #print_sum(1)/print_sum(1)(3)/print_sum(1)(3)(5) 都是一个next_sum函数 #---------------------------------149 装饰器(decorator): 装饰器本质上是一个函数，它可以接收一个函数作为参数并返回一个新的函数。这个新函数是对原函数的一种包装或增强，可以在不改变原函数代码的前提下，增加额外的功能 (eg. 在 cats 项目中给你的爆搜加个记忆化) 装饰器内部有包装函数, 装饰器在内部定义它并返回它, 使用 @ 语法, 见下例: 12345678910111213def trace(f): # f 即被装饰函数 def traced(x): print('Calling',f,'on argument',x) return f(x) return traced@tracedef square(x): return x*xprint(square(12))#-----------------------------------Calling &lt;function square at 0x000001F0A104EF80&gt; on argument 12144 一道程序填空难题, 题面见下(禁止使用 list, set, 或其他笔记中未提及的): 1234567891011121314151617def repeat(k): &quot;&quot;&quot;When called repeatedly, print each repeated argument &gt;&gt;&gt; f = repeat(1)(7)(7)(3)(4)(2)(5)(1)(6)(5)(1) 7 1 5 1 &quot;&quot;&quot; return ___(k)def detector(f): def g(i): if ___: ___ return ___ return g 一种可能答案见下: 123456789101112131415161718def repeat(k): &quot;&quot;&quot;When called repeatedly, print each repeated argument &gt;&gt;&gt; f = repeat(1)(7)(7)(3)(4)(2)(5)(1)(6)(5)(1) 7 1 5 1 &quot;&quot;&quot; return detector(lambda j: False)(k)def detector(f): #f = have_seen_i_before def g(i): #g = updated_have_seen_i_before if f(i): print(i) return detector(lambda j: j==i or f(j)) #每一次迭代相当于添加了一个 lambda 函数在 list 中 return g 这体现了函数构建抽象的思想, 你知道了 \\(f(i)\\) 的含义, 就不用想它, 直接用就是了 递归(recursion): 自我调用 迭代是递归的特殊情况, 用具体功能理解抽象函数, 在写递归时做出信仰之跃 一行的递归求阶乘算法: 12345def fac(): return (lambda y: y(y))(lambda y: lambda x: 1 if x==1 else x*y(y)(x-1))#----------------------哈哈我也看不懂----&gt;&gt;&gt; fac()(5)120 数据构建抽象 列表(list): 类似 c++ 中的数组, 初始化 a = [a0,a1,a2,a3,...] list 的运算, 例: a=[2,6] b=[5,8], c=a+b*2=[2,6,5,8,5,8] 新的运算符 in: 设 a=[2,6] , 此时 2 in a 为 True 高维 list: p=[[10,20], [30,40]] , 此时 p[1]=[30,40] 二维list创建: p=[[] for _ in range(M)], 创建一个 M 行的空 list, 加东西用 p[i].append(...) 注意！不建议 list 赋值时用 list_a = list_b , 在python中 list_a 相当于 list_b 的指针, 修改 list_a 会改变 list_b list 是一种容器(container), 容器是包含其它数据类型的一种数据结构或数据类型 For循环: for &lt;name&gt; in &lt;expression&gt;: &lt;suite&gt; 以找在 list 中一个数出现的次数为例: 12345678def count(s, val): tot=0 for element in s: if element==val: tot+=1 return tot#------------------------------&gt;&gt;&gt; count([1,2,1,2,1],1)3 这里的 in 有点像 c++ 的 auto 了...... 对于 p[[1,2], [3,4], [2,2], [5,7]], 还可以写 for x, y in p: 这下直接步入 c++17 了 对于 a[1,2,3], a[1:]=a[1:3]=[2,3], 这里的 1: 代表从下标为 1 的元素到最后一个元素 同理有 a[:1]=a[0:1]=[1], 右边界不含 range(1,N) 可以帮助遍历 [1,N) 之间的整数, range(N) = range(0,N) join(): 连接字符串数组。将字符串、元组、列表中的元素以指定的字符(分隔符)连接生成一个新的字符串 语法 'sep'.join(seq) , 其中 sep 为分隔符(可以为空('')), seq 为要连接的元素序列、字符串、元组、字典 返回一个以分隔符 sep 连接各个元素后生成的字符串 1234#eg. 将长整数每一位分割进 list 后再合并num = input()s = [int(d) for d in str(num)]num = int(''.join(str(i) for i in s)) list comprehension: 一种具体/抽象构建 list 的方法, 见下例: 12345&gt;&gt;&gt; odds=[1,3,5,7,9]&gt;&gt;&gt; [x+1 for x in odds][2,4,6,8,10]&gt;&gt;&gt; [x for x in odds if 25%x==0][1,5] 字符串(string):可用 单引号/双引号/三引号(多行常用) 定义, len(s) 求 s 的长度 string 可以直接找子串, list 不行 抽象屏障: 不要直接玩弄抽象下标了, 没人知道那代表什么, 评价为学 OI 学的, 下图是一个经典反例 这样写不利于维护和更改 字典(dictionary): 一个 key 对应一个 value, list 或 dictionary 不能被用来做 key (因为 list 是可变的, 只能用 tuple) 字典是无序的, key 是唯一对应的, 非要让 key 对应多个 value 的话, value 就用 list 吧 字典定义用大括号 {} 字典添加/修改直接写 dict[\"xxx\"]=yyy, 或调用 update 方法 dict.update({\"xxx\",yyy}) 12345678910numerals = {'I':1, 'V':5, 'X':10}---------------&gt;&gt;&gt; numerals['X']10&gt;&gt;&gt; numerals.keys()dict_keys(['X', 'V', 'I'])&gt;&gt;&gt; numerals.items()dict_items([('X',10), ('V',5), ('I',1)]) &gt;&gt;&gt; 'X' in numeralsTrue 元组(tuple): 与 list 不同, 元组是有序且不可修改的集合, 用小括号 () 创建 元组内下标可以为负数, 你可以认为元组是无限循环的, p[-1] 可指向元组的最后一个元素 可以通过 tuple(...) 函数直接将 list 转化为元组 max函数: max(iterable[, key=func]) -&gt; value 或 max(a, b, c, ...[, key=func]) -&gt; value 中括号括起来的参量可以空着不写 1234&gt;&gt;&gt; max(range(5))4&gt;&gt;&gt; max(range(10), key=lambda x: 7-(x-4)*(x-2)) #key 相当于一个 compare 函数3 树结构: 一个树有一个根标签(root label)和一系列分支(branch)。树的每个分支都是一棵树, 没有分支的树称为叶子(leaf)。树中包含的任何树都称为该树的子树(例如分支的分支)。树的每个子树的根称为该树中的一个节点(node)。 python 的 list 套 list 结构可以帮助我们很快构建一颗树: 下图为斐波那契树的构造: 12345678def fib_tree(n): if n&lt;=1: return tree(n) else: left, right = fib_tree(n-2), fib_tree(n-1) return tree(label(left)+label(right),[left, right])-----------------------------------------&gt;&gt;&gt; fib_tree(4)[3, [1, [0], [1]], [2, [1], [1, [0], [1]]]] list 及 子list 的第一项都是树及子树的根 这样建树太抽象了, 我还是用我的 append 吧…… all(): 接收一个 list , 全真为真, 其余为假 any(): 接受一个 list , 全假为假, 其余为真 zip(): 将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表 123456789&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; b = [4,5,6]&gt;&gt;&gt; c = [4,5,6,7,8]&gt;&gt;&gt; zipped = zip(a,b) # 打包为元组的列表[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; zip(a,c) # 元素个数与最短的列表一致[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; zip(*zipped) # 与 zip 相反，*zipped 可理解为解压, 返回二维矩阵式, 每个元组长度相同[(1, 2, 3), (4, 5, 6)] # == zip(s*zip(a,b)) / list(tuple(a),tuple(b)) 一些没用二进制知识的回忆: 原码: 原码是最直观的表示方法, 它直接用二进制数表示一个数, 包括正负号。在原码中, 最高位(最左边的位)是符号位, 0 表示正数, 1 表示负数, 其余位表示数值本身。例如, 十进制数 +5 的原码表示为 0000 0101, 而 -5 的原码表示为 1000 0101。 反码: 反码主要用于表示负数。对于正数, 其反码与其原码相同。对于负数, 其反码是将原码除符号位外的所有位取反(0 变 1, 1 变 0)。例如, 十进制数 -5 的反码表示为 1111 1010。 补码: 补码是计算机中最常用的表示方法, 用于进行二进制加法运算。对于正数, 其补码与其原码相同。对于负数, 其补码是其反码加 1。补码的一个重要特性是, 任何数的补码加上该数本身, 结果总是 0。例如, 十进制数 -5 的补码表示为 1111 1011。 这些抽象东西在做正数与负数的加法上很有用, 以 -3 + 2 为例: 2 的补码为其本身: 0000 0010 -3 的补码为: 1111 1101 两者相加所得补码: 1111 1111 此时补码为负，除符号位取反加1得到原码：1000 0001，即 -1 补码存在的本质大抵是一种模运算, 让负数与对应的正数在 \\(2^n\\) 上同余以此做加减 面向对象编程 (object-oriented programming) 的核心就是向数据添加状态 Python 中所有的值都是对象。也就是说，所有的值都有行为和属性，它们拥有它们所代表的数据的行为 Python 的 ord() 函数用于返回单个字符的 ASCII 数值或 Unicode 数值, 相反的, 函数 chr() 用一个范围在ASCII/Unicode 内的整数作参数，返回一个对应的字符。 1234567891011121314151617181920# list 就是一个可变变量, python 提供了很多函数操作它&gt;&gt;&gt; chinese = ['coin', 'string', 'myriad'] # 一组字符串列表&gt;&gt;&gt; suits = chinese # 为同一个列表指定了两个不同的变量名, 类似 C++ 中的 &amp;suits = chinese &gt;&gt;&gt; suits.pop() # 从列表中移除并返回最后一个元素'myriad'&gt;&gt;&gt; suits.remove('string') # 从列表中移除第一个与参数相同的元素&gt;&gt;&gt; suits.append('cup') # 在列表最后插入一个元素&gt;&gt;&gt; suits.extend(['sword', 'club']) # 将另外一个列表中的所有元素添加到当前列表最后&gt;&gt;&gt; suits[2] = 'spade' # 替换某个元素&gt;&gt;&gt; suits['coin', 'cup', 'spade', 'club']&gt;&gt;&gt; suits[0:2] = ['heart', 'diamond'] # 替换一组数据&gt;&gt;&gt; suits['heart', 'diamond', 'spade', 'club']&gt;&gt;&gt; chinese # 这个变量名与 &quot;suits&quot; 指向的是同一个列表对象['heart', 'diamond', 'spade', 'club']#append 与 extend 与 s=s1+s2 是复制后传参#如果你要写类似于 s=[s1]+[s2] 这样的东西, 注意此处的s1, s2为实参！ 因为传实参的一些特性，你可以玩抽象的递归 list: 12345t = [1,2,3]t[1:3]=[t]t.extend(t)print(t)&gt;&gt;&gt; [1, [...], 1, [...]] 因为 python 默认取地址的特性, 我们想复制一份不关联的形参可能要这么写: 1nest = list(suits) # 复制一个与 suits 相同的列表，并命名为 nest 你可以用 is 运算符看两个名称是否为同一个变量: 1234567a = [10]b = ac = [10] ## a==b==c&gt;&gt;&gt; a is bTrue&gt;&gt;&gt; a is cFalse tuple (用小括号 () 创建)是不可变的，但如果元组中的元素本身是可变数据，那我们也是可以对该元素进行操作的: 123nest = (10, 20, [30, 40])nest[2].pop() #合法nest[2] = 30 #不合法 python 函数的默认值很危险, 因为它默认是 static 的！ python 的函数好像也默认传的实参: 1234def f(s): s.pop()four = [1,2,3,4] #len(four)=4f(four) #len(four)=3 python 的函数也可以是有状态的, 相同的输入可能产生不同的结果 多次调用同一个函数得到的结果却不相同，副作用之所以会出现，是因为函数更改了它所在的栈帧之外的变量 下面以一个取钱函数为例: 1234567891011def make_withdraw(balance): &quot;&quot;&quot;返回一个每次调用都会减少 balance 的 withdraw 函数&quot;&quot;&quot; def withdraw(amount): nonlocal balance # 声明 balance 是非局部的 if amount &gt; balance: return '余额不足' balance = balance - amount # 重新绑定 return balance return withdrawwithdraw = make_withdraw(100)# 如果注释掉 nonlocal balance, 那么 python 在编译时会认为 balance 绑定的是 local frame, 而程序在声明前就调用了它 这里的 nonlocal 是一种非局部声明, 当 balance 属性为声明为 nonlocal 后, 每当它的值发生更改时, 相应的变化都会同步更新到 balance 属性第一次被声明的位置, 如果在声明 nonlocal 之前 balance 还没有赋值, 则 nonlocal 声明将会报错。 其实一般不用这东西，你重新绑一个名字 eg. b=[balance] 就行了 iterator 迭代器 迭代器是可以迭代的对象，这意味着你可以遍历所有值 123456s = [[1,2],3,4,5]t = iter(s)next(t)&gt;&gt;&gt; [1,2]next(t)&gt;&gt;&gt; 3 在字典中迭代顺序是插入顺序, 可以选择迭代 .keys()/.values()/.items() 在 for 循环中遍历 list 的迭代器，只能 for 一次, 第二次因为迭代器到达了末尾而无法进行 map(function, iterable, ...) 会根据提供的函数对指定序列做映射 第一个参数 function 以参数序列中的每一个元素调用 function 函数, 返回包含每次 function 函数返回值的新列表 12&gt;&gt;&gt; map(lambda x:x^2, [1,3,5,7,9])[1,9,25,49,81] filter(function, iterable) 函数用于过滤序列，过滤掉不符合条件的元素, 返回由符合条件元素组成的新列表 该函数接收两个参数, 第一个为函数, 第二个为序列, 序列的每个元素作为参数传递给函数进行判断, 然后返回 True 或 False, 最后将返回 True 的元素放到新列表中 12&gt;&gt;&gt; filter(lambda x:x%2==1, [1,2,3,4,5,6])[1,3,5] 有的时候, python 的函数不会立即计算答案, 而是返回一个迭代器, 用到的时候再计算, 在一些情形下可能会出错 在 Python 中，使用了 yield 关键字的函数被称为生成器(generator), 其为一种特殊的迭代器 当在生成器函数中使用 yield 语句时, 函数的执行将会暂停, 并将 yield 后面的表达式作为当前迭代的值返回 然后, 每次调用生成器的 next() 方法或使用 for 循环进行迭代时, 函数会从上次暂停的地方继续执行, 直到再次遇到 yield 语句。这样, 生成器函数可以逐步产生值, 返回多次, 而普通函数只返回一次结果 1234567891011def countdown(n): while n &gt; 0: yield n n -= 1gen = countdown(5) print(next(gen)) # 输出: 5print(next(gen)) # 输出: 4print(next(gen)) # 输出: 3 # 使用 for 循环迭代生成器for value in gen: print(value) # 输出: 2 1 12345678910111213141516# 一个小例题: 返回给定序列的全排列, 要求用 yield 一个一个传# eg: sorted(permutations((1, 2, 3))) = [[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]]# eg: sorted(permutations(&quot;ab&quot;)) = [['a','b'],['b','a']]def permutations(seq): if not seq: yield [] else: for perm in permutations(seq[1:]): for i in range(len(seq)): yield perm[:i] + [seq[0]] + perm[i:]#————————————————OR————————————————def permutations(seq): if not seq: yield [] else: for i in range(len(seq)): for arr in permutations(seq[:i]+seq[i+1:]): yield [seq[i]] + arr 你还可以用 yield from 一次性把可迭代对象的东西全 yield 出来 类与对象(Class &amp; Objects) 类就像一个模板，对象是按照模板（类）生成的实例 class 语句可以创建自定义类 Python 中有一个特殊的名称 __init__ (“init”的每一侧都有两个下划线), 称为类的构造函数(constructor) Python 类中的每个函数必须有一个额外的第一个参数名称, 按照惯例它的名称是 self, 相当于 C++ 的 *this 类属性在给定类的所有对象之间共享, 类属性的赋值会改变类的所有实例的属性值 类属性由 class 语句套件中的赋值语句创建，位于任何方法定义之外 1234567891011121314151617181920212223class Account: interest = 0.02 #类属性 def __init__(self, account_holder): self.balance = 0 self.holder = account_holder def deposit(self, amount): self.balance += amount return self.balance def withdraw(self, amount): if amount &gt; self.balance: return 'Insufficient funds' self.balance = self.balance - amount return self.balance&gt;&gt;&gt; a = Account(&quot;Stanley&quot;)&gt;&gt;&gt; a.balance0&gt;&gt;&gt; a.holderStanley&gt;&gt;&gt; a.deposit(100)100&gt;&gt;&gt; Account.interest = 0.05&gt;&gt;&gt; a.interest #注意, 若之前单独改变 a.interest = c, 则会输出 c 而不是 0.050.05 类的继承(inheritance) 1234567#eg. 创造继承于 Account 的 CheckingAccount 类, 其会收取固定手续费, 并且利率不同class CheckingAccount(Account): # 继承于 Account withdraw_fee = 1 interest = 0.01 def withdraw(self, amount): # 新的 withdraw 会覆盖基类的 withdraw return Account.withdraw(self, amount + self.withdraw_charge)# 不用写 __init__(), 因为基类写过了 在类中查找名称时先找当前类再找父类(基类) 我们可以用 super() 来使用父类的函数, 比如说 super().withdraw() 我们还可以写多继承, 例如 class A(B, C, D): 但是继承的排序总是令人恼火的, 我们可以通过 A.mro() 获取顺序 有时候你需要的不是继承(inheritance), 而是组合(composition), 例如下文的 Bank 类: 123456789101112131415161718192021222324class Bank: &quot;&quot;&quot;A bank *has* accounts &gt;&gt;&gt; bank = Bank() &gt;&gt;&gt; John = bank.open_account('John',10) &gt;&gt;&gt; Jack = bank.open_account('Jack',5,CheckingAccount) &gt;&gt;&gt; John.interest 0.02 &gt;&gt;&gt; Jack.interest 0.01 &gt;&gt;&gt; bank.pay_interest() &gt;&gt;&gt; John.balance 10.2 &quot;&quot;&quot; def __init__(self): self.accounts = [] def open_account(self, holder, amount, kind=Account): account = kind(holder) account.deposit(amount) self.accounts.append(account) return account def pay_interest(self): for a in self.accounts: a.deposit(a.balance * a.interest) Python 规定所有的对象都应该生成两个不同的字符串表示: 一种是人类可读的文本, 另一种是 Python 可解释的表示式。字符串的构造函数, 即 str, 返回一个人类可读的字符串。如果可能, repr 函数返回一个 Python 可解释的表达式, 该表达式的求值结果一般与原对象相同 专用方法: 例如: __bool__ 方法可以用来覆盖默认真值的行为, 假设我们想让一个只有 0 存款的账号为假值。我们可以为 Account 添加一个 __bool__ 方法来实现这种行为: 1Account.__bool__ = lambda self: self.balance != 0 len() 函数调用 __len__ 方法来确定其长度, 我们也可以直接调用, 有: 'Go'.__len__() = 2 __call__ 方法可以让我们定义一个行为像高阶函数的类: 12345678&gt;&gt;&gt; class Adder(object): def __init__(self, n): self.n = n def __call__(self, k): return self.n + k&gt;&gt;&gt; add_three_obj = Adder(3)&gt;&gt;&gt; add_three_obj(4)7 自己定义这些方法就相当于重载运算符, 例如 + 的方法是 __add__, print 使用方法是 __str__ : 1234567891011121314class Point: def __init__(self, x = 0, y = 0): self.x, self.y = x, y def __str__(self): return &quot;({0},{1})&quot;.format(self.x,self.y) def __add__(self,other): x = self.x + other.x y = self.y + other.y return Point(x,y)------------------------------------------------&gt;&gt;&gt; p1 = Point(2,3)&gt;&gt;&gt; p2 = Point(-1,2)&gt;&gt;&gt; print(p1 + p2)(1,5) 这样我们就重载了 Point 类, 注意这里只重载了左加, 右加 __radd__ 可以通过 __radd__ = __add__ 来重载 链表类(linked list) 12345678910111213#isinstance() 函数用于判断一个对象是否是一个已知的类型, 会认为子类是父类的一个实例class Link: def __init__(self, first, rest=empty): assert rest is Link.empty or isinstance(rest, Link) self.first = first self.rest = rest#--------------------------------------------------&gt;&gt;&gt; s = Link(3, Link(4, Link(5)))&gt;&gt;&gt; s.first3&gt;&gt;&gt; s.restLink(4, Link(5))#当然你也可以测试一些花活, 例如 s.rest.rest = s 来造循环列表 树(Tree)的类实现: 1234567891011121314151617181920class Tree: def __init__(self, label, branches=[]): self.label = label for branch in branches: assert isinstance(branch,Tree) self.branches = list(branches) def __str__(self): return '\\n'.join(self.indented()) def indented(self): lines = [] for b in self.branches: for line in b.indented(): lines.append(' ' + line) return [str(self.label)] + lines#----------------------------------&gt;&gt;&gt; t = Tree(1, [Tree(3), Tree(4)])&gt;&gt;&gt; print(t)1 3 4 程序的效率问题: 比如说求个斐波那契数列, 你可以用高阶函数写记忆化, eg. 1234567def memo(f): cache = {} def memorized(n): if n not in cache: cache[n] = f(n) return cache[n] return memorized 然后让 fib = memo(fib) 来记忆化 或者你也可以写一个装饰器 集合(set) 集合(set)是一个无序的不重复元素序列。 集合中的元素不会重复，并且可以进行交集、并集、差集等常见的集合操作。 可以使用大括号 { } 创建集合，元素之间用逗号 , 分隔， 或者也可以使用 set() 函数创建集合。 12set1 = {1, 2, 3, 4} # 直接使用大括号创建集合set2 = set([4, 5, 6, 7]) # 使用 set() 函数从列表创建集合 1234567891011121314151617181920&gt;&gt;&gt; basket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}&gt;&gt;&gt; print(basket) # 这里演示的是去重功能{'orange', 'banana', 'pear', 'apple'}&gt;&gt;&gt; 'orange' in basket # 快速判断元素是否在集合内True&gt;&gt;&gt; 'crabgrass' in basketFalse&gt;&gt;&gt; a = set('abracadabra') # 下面展示两个集合间的运算&gt;&gt;&gt; b = set('alacazam')&gt;&gt;&gt; a {'a', 'r', 'b', 'c', 'd'}&gt;&gt;&gt; a - b # 集合a中包含而集合b中不包含的元素{'r', 'd', 'b'}&gt;&gt;&gt; a | b # 集合a或b中包含的所有元素{'a', 'c', 'r', 'd', 'b', 'm', 'z', 'l'}&gt;&gt;&gt; a &amp; b # 集合a和b中都包含了的元素{'a', 'c'}&gt;&gt;&gt; a ^ b # 不同时包含于a和b的元素{'r', 'd', 'b', 'm', 'z', 'l'} 添加: s.add(x) 或 s.update(x) 删除: s.remove(x) 或 s.discard(x) 其中 discard 为安全删除, 如果元素不存在, 不会发生错误 计算机程序的解释 在了解函数与数据后, 我们讨论一下程序本身, python 程序是文本的集合, 而解释器决定了编程语言中表达式的含义, 但它只是另一个程序, 这一部分主要要求你写一个 Python 语言的 Scheme 解释器 我们来学习另一门程序语言 Scheme, 你可以在 61A Code 中编写并运行 Scheme 程序 你可以在 Scheme入门教程 速通一下 Scheme Scheme 是 Lisp 的一个变种, 而 Lisp 是继 Fortran 之后仍然广受欢迎的第二古老的编程语言。 Scheme 程序主要是由各种表达式构成的, 这些表达式可以是函数调用或一些特殊的结构。一个函数调用通常由一个操作符和其后面跟随的零个或多个操作数组成, 这点和 Python 是相似的。不过在 Scheme 中, 这些操作符和操作数都被放在一对括号里： 123(quotient 10 2) ; &lt;=&gt; (/ 10 2) &lt;=&gt; 5; quotient 是整除, / 是除; (remainder a b) 返回 a%b 的值 Scheme 的语法一直采用前缀形式。也就是说, 操作符像 + 和 * 都放在前面。函数调用可以互相嵌套, 并且可能会写在多行上: 12(+ (* 3 5) (- 10 6)) ;ans = 19 (15 + 4 = 19)(* 1 2 3 4) ;ans = 24 判断也是前缀的, 回复 #t 为 true, #f 为 false 12(&gt;= 2 1)#t if 表达式结构如下, 为真返回 &lt;consequent&gt;, 否则返回 &lt;alternative&gt;: 1(if &lt;predicate&gt; &lt;consequent&gt; &lt;alternative&gt;) and / or 格式: (and &lt;e1&gt; ... &lt;en&gt;) , (or &lt;e1&gt; ... &lt;en&gt;) 定义值: 1234;(define &lt;symbol&gt; &lt;expression&gt;)&gt; (define pi 3.14)&gt; (* pi 2)6.28 定义函数(函数在 Scheme 中称为过程): 12345678910;(define (&lt;name&gt; &lt;formal parameters&gt;) &lt;body&gt;)&gt; (define (f x) (* x x))&gt; (f 2)4&gt; (define (abs x) (if (&lt; x 0) (-x) x))&gt; (abs -3)3 匿名函数是通过 lambda 特殊形式创建的。Lambda 用于创建过程, 与 define 相似, 但不需要为过程指定名称, 格式为: (lambda (&lt;formal-parameters&gt;) &lt;body&gt;) 以下两种定义方式等效: 12(define (plus4 x) (+ x 4))(define plus4 (lambda (x) (+ x 4))) Scheme 支持与 Python 相同的词法作用域规则, 允许进行局部定义。 下面, 我们使用嵌套定义和递归定义了一个用于计算平方根的迭代过程: 12345678910111213141516(define (square x) (* x x))(define (average x y) (/ (+ x y) 2))(define (sqrt x) (define (good-enough? guess) (&lt; (abs (- (square guess) x)) 0.001)) (define (improve guess) (average guess (/ x guess))) (define (sqrt-iter guess) (if (good-enough? guess) guess (sqrt-iter (improve guess)))) (sqrt-iter 1.0))(sqrt 9)3.00009155413138 复合类型介绍 cond 相当于 if-elif-...-elif-else 结构: 12345678(cond ((&gt; x 10) (print 'big)) ((&gt; x 5) (print 'medium)) (else (print 'small))); 你也可以这么写(print (cond ((&gt; x 10) 'big) ((&gt; x 5) 'medium) (else 'small))) begin 函数将多个表达式并在一起, 如 (begin (display\"Hello, World!\") (newline)) 会输出 Hello, world! 并回车换行 let 函数像一个局部的 define , 用完即扔 scheme 也有类似于 python 的 list, 但是这个 list 更像链表(linked list) 有一些关键词, 例如 cons car cdr nil , 我直接放一下课程截图吧...... eg. (cons (cons 4 (cons 3 nil)) s) = ((4 3) 1 2) 你也可以直接 (list list(4 3) 1 2) 来构造 list 下面是一个嵌套 list 的结构: 在 Scheme 中, 我们通过在 a b 前面加上一个单引号来引用符号 a 和 b 而不是它们的值, 见下例: 12345678&gt; (define a 1)&gt; (define b 2)&gt; (list a b)(1 2)&gt; (list 'a 'b)(a b)&gt; (list 'a b)(a 2) 在 Scheme 中, 任何不被求值的表达式都被称为被引用。在语言中, 引号允许我们讨论语言本身, 而在 Scheme 中也是如此, 引用可以在定义之前进行 Scheme 程序语句也可以是 Scheme list, eval 函数又能将 list 解耦为程序语句: 1234&gt; (list 'quotient 10 2)(quotient 10 2)&gt; (eval (list 'quotient 10 2))5 ` 符号(左单引号) 代表部分引用(quasiquotation), 被部分引用的部分可以通过 ,(逗号) 解引用, 例: 123&gt; (define b 4)&gt; `(a ,(+ b 1))(a 5) 下面给出 homework 08 的四个例题及代码, 有助于你理解这门没有循环, 全靠递归的语言: 123456789101112131415161718192021222324252627282930313233343536373839404142; 询问 list s 是否为不降序列(define (ascending? s) (cond ((or (equal? s nil) (equal? (cdr s) nil)) #t) ((&gt; (car s) (car (cdr s))) #f) (else (ascending? (cdr s))) )); 询问 list s 经历 pred 函数 “过滤” 后的序列; eg. (my-filter even? '(1 2 3 4 5)) = (2 4)(define (my-filter pred s) (cond ((equal? s nil) nil) ((equal? (pred (car s)) #f) (my-filter pred (cdr s))) (else (append (list (car s)) (my-filter pred (cdr s)))) )); 将 lst1, lst2 两个序列互相穿插, lst1 先开始, 一个序列空后将另一个直接接在最后面; eg. (interleave '(1 2 3) '(4 5 6)) = (1 4 2 5 3 6); eg. (interleave '(7 8 9 10) '(11 12)) = (7 11 8 12 9 10)(define (interleave lst1 lst2) (cond ((equal? lst1 nil) lst2) ((equal? lst2 nil) lst1) (else (append (list (car lst1) (car lst2)) (interleave (cdr lst1) (cdr lst2)))) )); 去除 list s 中的重复元素; eg. (no-repeats (list 5 4 5 4 2 2)) = (5 4 2)(define (no-repeats s) (define (filter pre now) (cond ((equal? now nil) nil) ((= pre (car now)) (filter pre (cdr now))) (else (append (list (car now)) (filter pre (cdr now)))) ) ) (cond ((equal? s nil) nil) (else (append (list (car s)) (filter (car s) (no-repeats (cdr s))))) )) 异常(exception) 未处理的异常会导致 Python 程序停止运行, 解释器将打印一个堆栈回溯(stack backtrace) 异常也是一种对象, 其类有对应的构造函数 抛出异常(raising an exception): assert 语句会抛出一个类为 AssertionError 的异常, 格式为 assert &lt;expression&gt;, &lt;string&gt;。 通常情况下, 可以使用 raise 语句来抛出任何异常实例, eg. raise Exception('An error occurred') 处理异常(handling exceptions): 异常可以由封闭的 try 语句来处理。try 语句由多个子句组成；第一个以 try 开头，其余的以 except 开头: 1234try &lt;try suite&gt;except &lt;exception class&gt; as &lt;name&gt;: &lt;except suite&gt; 我们先执行 &lt;try suite&gt;, 如果有抛出异常且异常类型为 &lt;exception class&gt;, 则 &lt;except suite&gt; 会强制执行, 并把 &lt;name&gt; 绑定到异常上 123456789101112131415&gt;&gt;&gt; def invert(x): result = 1/x #抛出一个异常（ZeroDivisionError) 如果 x 为 0 print('Never printed if x is 0') return result&gt;&gt;&gt; def invert_safe(x): try: return invert(x) except ZeroDivisionError as e: return str(e)#-----------------------------------------&gt;&gt;&gt; invert_safe(2)Never printed if x is 00.5&gt;&gt;&gt; invert_safe(0)'division by zero' 让我们回到 scheme, 下一步目标是做一个 scheme 程序的解释器 解析是根据原始文本输入生成表达式树的过程。解析器由两个组件组成: 词法分析器(lexical analyzer)和语法分析器(syntactic analyzer) 首先, 词法分析器将输入字符串划分为标记(token)。标记表示语言的最小语法单元, 比如名称和符号。然后, 语法分析器根据这个标记序列构建一个表达式树。 1234567&gt;&gt;&gt; tokenize_line('(+ 1 (* 2.3 45))') #lexical analyzer part['(', '+', 1, '(', '*', 2.3, 45, ')', ')']&gt;&gt;&gt; expression = scheme_read(Buffer(tokenize_lines('(+ 1 (* 2.3 45))'))) &gt;&gt;&gt; expression #syntactic analyzer partPair('+', Pair(1, Pair(Pair('*', Pair(2.3, Pair(45, nil))), nil)))&gt;&gt;&gt; print(expression)(+ 1 (* 2.3 45)) 让我们先尝试做一个 scheme 计算机, 主要依赖于 Pair 类来实现 将一个表达式变成 pair 的形式有助于解释器(interpreter)处理 scheme_eval 函数用于对 Scheme 中不同形式的表达式进行求值, 包括基元、特殊形式和调用表达式。在 Scheme 中, 组合形式可以通过检查其第一个元素来确定。每种特殊形式都有自己的求值规则 我们要特殊关心那些逻辑语句, 它们会带来一个或多个子表达式, 例如 if,and,or,cond: 再学一下引用(quotation), (quote &lt;expression&gt;) 后编译器回处理出来 &lt;expression&gt;, 再处理一次才能得到表达式的值, 相当于单引号的作用 (quote (1 2)) 和 `(1 2) 是等价的 现在我们已经描述了 Scheme 解释器的结构, 接下来我们来实现构成环境的 Frame 类。每个 Frame 实例代表一个环境, 在这个环境中, 符号与值绑定。一个帧有一个保存绑定(bindings)的字典, 以及一个父(parent)帧。对于全局帧而言, 父帧为 None 绑定不能直接访问，而是通过两种 Frame 方法：lookup 和 define。第一个方法实现了第一章中描述的计算环境模型的查找流程。符号与当前帧的绑定相匹配。如果找到它, 则返回它绑定到的值。如果没有找到, 则继续在父帧中查找。另一方面，define 方法用来将符号绑定到当前帧中的值, 格式为 (define &lt;name&gt; &lt;expression&gt;) 为了说明 lookup 和 define 的用途，请看以下 Scheme 程序示例: 12345(define (factorial n) (if (= n 0) 1 (* n (factorial (- n 1)))))(factorial 5)120 第一个输入表达式是一个 define 形式, 将由 Python 函数 do_define_form 求值。定义一个函数有如下步骤: 检查表达式的格式, 确保它是一个格式良好的 Scheme 列表, 在关键字 define 后面至少有两个元素 分析第一个元素(这里是一个 Pair), 找出函数名称 factorial 和形式参数表 (n) 使用提供的形式参数、函数主体和父环境创建 LambdaProcedure 在当前环境的第一帧中, 将 factorial 符号与此函数绑定。在示例中, 环境只包括全局帧 第二个输入是调用表达式。传递给 scheme_apply 的 procedure 是刚刚创建并绑定到符号 factorial 的 LambdaProcedure。传入的 args 是一个单元素 Scheme 列表 (5)。为了应用该函数, 我们将创建一个新帧来扩展全局帧 (factorial 函数的父环境)。在这帧中，符号 n 被绑定为数值 5。然后, 我们将在该环境中对 factorial 函数主体进行求值, 并返回其值。 本部分笔记最重要的解释器编写在我的 Scheme 大作业中, 缺少它会失去很多乐趣 =) 大作业有一个 Optional Problem: 实现 Scheme 解释器中的尾调用(Tail Call)优化 在计算机学里, 尾调用(tail call)是指一个函数里的最后一个动作是返回一个函数的调用结果的情形, 即最后一步新调用的返回值直接被当前函数的返回结果 尾调用由于是函数的最后一步操作, 所以不需要保留外层函数的调用记录, 因为调用位置、内部变量等信息都不会再用到了, 只要直接用内层函数的调用记录, 取代外层函数的调用记录就可以了 以阶乘函数的两种实现为例: 123456789101112# tail-recursiondef factorial_recursion(n, k): if n == 0: return k else: return factorial(n - 1, k * n)# iterationdef factorial_iteration(n, k): while n &gt; 0: n, k = n - 1, k * n return k 递归版本具有更高的空间复杂度, 在 n 较大时会爆栈 所以我们想到不保留栈帧, 递归到下一层时将变量保存过去即可: 123456789101112def thunk_factorial(n, so_far=1): def thunk(): if n==0: return so_far return thunk_factorial(n-1, so_far*n) return thunkdef factorial(n): value = thunk_factorial(n) while callable(value): value = value() return value 图例是这样的: 在 scheme 解释器中的优化是这样的: 1234567891011121314def optimize_tail_calls(unoptimized_scheme_eval): &quot;&quot;&quot;Return a properly tail recursive version of an eval function.&quot;&quot;&quot; def optimized_eval(expr, env, tail=False): &quot;&quot;&quot;Evaluate Scheme expression EXPR in Frame ENV. If TAIL, return an Unevaluated containing an expression for further evaluation. &quot;&quot;&quot; if tail and not scheme_symbolp(expr) and not self_evaluating(expr): return Unevaluated(expr, env) # Unevaluated 即类似上图的 thunk() result = Unevaluated(expr, env) while isinstance(result, Unevaluated): result = unoptimized_scheme_eval(result.expr, result.env) return result return optimized_eval tail 即代表到达了底部, 返回函数值, 否则先返回一个 Unevaluated 实例回收空间, 在 result 的环境中计算表达式, 反正不能到最后才回溯就是了 数据处理 声明式编程(英语：Declarative programming)或译为声明式编程, 是对与命令式编程不同的编程范型的一种合称。它们建造计算机程序的结构和元素, 表达计算的逻辑而不用描述它的控制流程 即告诉计算机做什么而不是编写代码教计算机怎么做 SQL 是一种声明式编程语言的例子。SQL 语句不直接描述计算过程, 而是描述一些计算的预期结果。数据库系统的查询解释器负责设计和执行计算过程以产生这样的结果 可以使用 SQL 语言中的 select 语句创建一个单行表, 其中行值用逗号分隔, 列名跟在关键字 \"as\" 后面。所有的 SQL 语句都以分号结尾, eg. select [expression] as [name], [expression] as [name] 123select 38 as latitude, 122 as longitude, &quot;Berkeley&quot; as name;38|122|Berkeley-- 当然实际显示没有这么简陋, 可以去 vscode 上下载一个 sqlite 插件, 新建 .sql 文件, run query 玩一下 多行要用 union 连接 123select 38 as latitude, 122 as longitude, &quot;Berkeley&quot; as name unionselect 100,71,&quot;Cambridge&quot; unionselect 45,93,&quot;Minneapolis&quot;; latitute longtitude name 38 122 Berkeley 100 71 Cambridge 45 93 Minneapolis select 语句只是拿来显示而不是存储, 存储用 create table [name] as [select statement] 12345create table cities as select 38 as latitude, 122 as longitude, &quot;Berkeley&quot; as name union select 42, 61, &quot;Cambridge&quot; union select 45, 93, &quot;Minneapolis&quot;;SELECT * FROM cities; -- 验证表结构和数据 select 语句可以通过列出单行的值或更常见的通过在 from 子句中使用现有表来定义一个新表 (现有表投影) select [column name] from [existing table name] 更复杂一点的, 有: select [column name] from [existing table name] where [condition] order by [order] eg. select name, latitude, temp from cities, temps where name = city; 还可以写一些算数式子, 以课程中的图举例: 合并表信息: 如果直接连接, select * from A, B; , 若 A 表有 m 行, B 表有 n 行, 则新表有 m*n 行 一般来讲我们只 select 我们需要的行, 还要加上一些 where [condition] 来限制它 有时候表的某个列名我们需要导入多次分别使用, 我们可以给相同的列名取一些别名(aliases) 1234select a.child as first, b.child as second from parents as a, parents as b where a.parent = b.parent and a.child &lt; b.child-- 一段找兄弟姐妹的程序 string 表达式: 12select &quot;hello, &quot; || &quot;world&quot;;hello, world 有一些函数可以用, 比如说 substr 和 instr, 用到再查 12345678910111213141516create table nouns as select &quot;dog&quot; as phrase union select &quot;cat&quot; union select &quot;bird&quot;;select subject.phrase || &quot; chased &quot; || object.phrase as phrase from nouns as subject, nouns as object where subject.phrase &lt;&gt; object.phrase; -- &lt;&gt; 即为不等于号phrasebird chased catbird chased dogcat chased birdcat chased dogdog chased birddog chased cat limit [number] 可以限制输出的条目数目, 一般放在最后 聚合函数(aggregate functions) max() / min() / sum() / avg() / count()(返回行数) / ...... 聚合函数会选择对应的行, 当写出 select max(A), B from All 这样的代码时, 只会返回 max(A) 所在的那一行 但是当你写一些更抽象的东西, 比如 select avg(A), B from All, 返回什么只有天知道了 select 语句默认都在一个大的聚合(group)里, 我们可以给这些 select 语句加上分类, 这样聚合函数会对每个聚合进行运算, 并输出多行结果, 格式: select [columns] from [table] group by [expression] having [expression] eg. select legs, max(weight) from animals group by legs 之后课本上还有一些内容, 比如分布式计算和并行式计算, 可惜没有课程了, 相关内容可能以后会补充到笔记中","link":"/2025/02/06/CS61A-notes/"},{"title":"Codeforces exercises 2024","text":"记录了24/7/8-24/12/28的十八场 codeforces 补题, \\(rating\\;1646\\rightarrow1785\\) Undone: 982 的 E 还没写完, 后补 codeforces round 956 (#Div2) 24/7/8 \\((1646 \\rightarrow 1692, rk1180)\\) \\(next\\_permutation\\) 不会用导致卡了40mins C, 认了 E 题意: Alice 和 Bob 轮流随机取球, 一共 \\(n \\leq 4 \\times 10^5\\) 个带有一定分值 (\\(v_i\\)) 的球, 前 \\(k\\) 个特殊球取了之后还能继续取球, 效果可叠加, 问 Alice 先手情况下两人得分的期望 (模 \\(10^9+7\\) 输出)。 思路: 小巧精妙的题。分 \\(n-k\\) 个普通球和 \\(k\\) 个特殊球来思考, 先求出各自种球分值的和。对于普通球, Alice 一定会取 \\(\\lceil \\frac {n-k}{2}\\rceil\\) 个。 对于特殊球取的个数, 因为其与普通球有关, 一个直观的想法是 D。设 \\(dp_{i,j}\\) 表示此时有 \\(i\\) 个普通球, \\(j\\) 个特殊球时取走的特殊球数量的期望, 显然其可以通过 \\(dp_{i-1,j}, dp_{i,j-1}\\) 转移而来, 很可惜这个想法是 \\(O(n^2)\\) 的, 我们可以以此辅助先打表试试 (当然前提是你C++有一个分数类的模板或你略懂 python) 。 我们发现, 除最后一次取球外, 一个人的完整的一次取球必定以取普通球结束 (n个连续的特殊球加上一个普通球), 我们可以采用配凑的手段在取球序列的最后加上一个普通球, 这 \\(n-k+1\\) 个普通球将整个取球序列分为 \\(n-k+1\\) 段, 每一个特殊球出现在每一段中的概率都是相等的, 而 Alice 会取走 \\(\\lceil \\frac{n-k+1}{2} \\rceil\\) 段, 所以会取走 \\(k \\times \\frac{\\lceil \\frac{n-k+1}{2} \\rceil}{n-k+1}\\) 个特殊球, 剩下的就是一个逆元板子了。 代码: link codeforces round 958 (#Div2) 24/7/15 \\((1692 \\rightarrow 1642, rk3656)\\) 乱用 define int long long 加上D题思路错误, 认了 D 题意: 树, 点有权值, 每回合先收到树上权值和(\\(\\sum {a_{i}}\\))的伤害, 然后可以删去任意多不在一条边上的点, 问受到的最小总伤害。(\\(n \\leq 3 \\times 10^5, a_i \\leq 10^{12}\\)) 思路: 一开始认为总轮数最多为 3 , 否则会在第一次删点后在树上形成长度大于等于 3 的长链, 那么中间的点就应该第一轮被删去, 这个思路是错误的, 反例很好举: 先删5 6 7 8, 再删 1 4, 再删 2, 再删 3。 所以思考最多要几轮删完: 设第 \\(i\\) 个点第 \\(b_i\\) 轮被删, 题目限制等价于不存在相邻的两个点 \\(b_i\\) 相同, 在最优情况下, \\(b_i = mex_{(j,i) \\in E} b_j\\) , 设最大的 \\(b_i = u\\) , 使其为根节点, 其子节点的 \\(b_j\\) 遍历 (1~ u-1) , 这样递归推下去, 可推得 \\(u= \\lfloor log_2n \\rfloor +1\\) 。 在真实比赛中, 我们可以根据最大权值和 \\(3 \\times 10^5 \\times 10^{12} \\leq 10^{18}\\), 以及 \\(2^{60} \\; &gt;\\; 10^{18}\\) , 构造出一种最多删 60 轮的删法, 即每次将树黑白染色后删较大权值和的部分, 直接根据 \\(u \\leq 60\\) 来 DP。 设 \\(f_{i,k}\\) 表示以 \\(i\\) 为根的子树全部删完且 \\(i\\) 节点在第 \\(k\\) 轮删去的受伤害最小值, 即有 \\(f_{i,k}=k \\times a_{i} + \\sum\\limits_{(i,j)\\in E} min\\{f_{j,1},f_{j,2},...,f_{j,k-1},f_{j,k+1},...f_{j,60}\\}\\) 前缀和后缀和做一下即可快速更新, 复杂度 \\(O(60n)\\) 代码: link codeforces round 959 (#Div1+2) 24/7/18 \\((1642 \\rightarrow 1631, rk3146)\\)​ D 题意: 给你 \\(a_i\\) 数组, \\(n-1\\) 次操作, 第 \\(x\\) 次操作时若 \\(x\\) 整除\\(|a_u-a_v|\\) 则可连边, 问能不能连成一棵树, \\(n \\leq 2000, a_i \\leq 10^9\\) 思路: 鸽巢原理, 反着操作, 第 \\(x\\) 次操作时有 \\(x+1\\) 个连通块, 则一定有两个连通块中的两个数关于 \\(x\\)​ 同余, 加个并查集即可通过本题 代码: link E 幽默贪心, 个人 D &gt; E, 题解区有人说应该把 E 放在 C 位置上的 codeforces round 960 (#Div2) 24/7/20 \\((1631 \\rightarrow 1695, rk950)\\) E = 2500, 先咕着, 等我上 2000 就回来补。 ---24.9.29 Pinely round 4 (#Div1+2) 24/7/28 \\((1695 \\rightarrow 1709, rk2176)\\) E 题意： 交互题, 给定无向图和三种颜色 \\(1, 2, 3\\), Alice 和 Bob 进行游戏, 每一轮: Alice 选择两种不同颜色 Bob 选择其中一种颜色并选择一个未染色点染色 如果 \\(n\\) 轮后, 有相邻节点颜色相同, 则 Alice 胜利, 否则 Bob 胜利, 请输出胜者并完成交互 思路: 一个观察: 只要有奇环 Alice 就获胜, 否则 Bob 获胜。 一开始想要把图的奇环取出来, 比较困难 一个更进一步的观察: 若选择 Alice 胜利来交互, 只要重复选择颜色 \\(1,2\\) 就行了, 所以只用黑白染色确认奇环的存在就行。对于 Bob 胜利的情况, 我们刚刚染好色的图就派上用场了, 如果 Alice 给了对应染色的颜色就选那种颜色的点染色, 否则直接选择颜色 3, 此时 \\(1, 2\\) 种肯定有一种颜色染完了, 就染剩下颜色对应的点。 代码: link F 题意： 给你 \\(n\\) 根棍子, 长度 \\(a_i\\) , \\(q\\) 次询问, 每次问 \\([l,r]\\;(r-l+1\\geq 6)\\)​ 的范围中能不能取六根棍子组成两个三角形。 \\(n, q \\leq 10^5, a_i \\leq 10^9\\) 思路: 纯纯诈骗题, 一个最优的思想肯定是将所给区间排好序后尝试选择相邻的棍子构成三角形, 这样暴力做一次的代价是 \\(O(nlogn)\\) 的。我们注意到 \\(a_i \\leq 10^9\\), 尝试构造一个构造不出来三角形的最长数列, 即 \\(1,1,2,3,5,8,13,...\\) 事实上四十多项就涨破 \\(10^9\\) 了, 于是我们对于那些过宽的区间 \\([l,r]\\) 缩到最多 \\(50\\) 的长度, 这样单次代价 \\(O(50log(50))\\)​​, 可以通过本题。 如果这题的样例没有精心构造过, 这题会从诈骗题晋升为钓鱼题。对于 \\(\\{2,2,4,5,10,10\\}\\) 这个样例, 里面 \\(\\{2,4,5\\}\\) 是连续的, \\(\\{2,10,10\\}\\) 不是连续的, 这告诉我们答案可能会从连续的 \\(6\\) 个数字中不连续的出来, 设 \\(6\\) 个数字从小到大为 \\(a,b,c,d,e,f\\) ,其中 \\(a\\) 一定为短边, \\(f\\) 一定为长边, 要检查一共七种可能情形, 分别是: \\(a+b&gt;d,\\;c+e&gt;f\\) \\(a+c&gt;d,\\;b+e&gt;f\\) \\(b+c&gt;d,\\;a+e&gt;f\\) \\(a+b&gt;e,\\;c+d&gt;f\\) \\(a+c&gt;e,\\;b+d&gt;f\\) \\(a+d&gt;e,\\;b+c&gt;f\\) \\(b+c&gt;e,\\;a+d&gt;f\\) 对于 \\(\\{1,2,2,4,6,10,10\\}\\) 这个样例, 里面的 \\(\\{1,2,2\\} ,\\{6,10,10\\}\\) 并没有接到一起去, 但是是各自连续的, 这启示我们答案可能由两个不在一起的 \"三连续子数组\" 构成, 可证明只存在这两种构成方式。 代码: link codeforces round 963 (#Div2) 24/8/4 \\((1709 \\rightarrow 1661, rk3739)\\) D 绝妙思维题, 告诉我们题目的每个条件都应尽量贴切用上, 可能还是我见的太少了...... 题意: 给你长度为 \\(n\\) 的数组 \\(a_i\\) 和常数 \\(k\\) , 每次在数组中删掉长度为 \\(k\\) 的连续段直到不能删为止, 问剩下数列的中位数最大是多少？ \\(n,k \\leq 5*10^5, a_i \\leq 10^9\\) 思路: 首先套一个二分, 中位数根据与 mid 的关系映射为 \\(-1/1\\) , 再套一个前缀和。 接下来我的想法是贪心暴力, 先把目前能删的连续段都放进优先队列里, 若删去一个段再考虑把它两边接起来的段继续扔进去, 需要一个双向列表来维护, 难写得很, 复杂度大抵是对的, 但是过不了题。 题解思路利用了一个条件, 每次删除的连续段长度不变, 都是 \\(k\\), 假设删到最后剩下 \\(x\\) 个数, 它们下标 \\(mod\\;k\\) 的结果一定是 \\(1,2,3,...,x\\) ,这样就可以通过下标判断选了几个数从而 \\(dp\\) , 设 \\(f_u\\) 表示最后一个数字 \\(mod\\; k=u\\) 的答案, 转移一下就行了。 代码: link codeforces round 965 (#Div2) 24/8/10 \\((1661 \\rightarrow 1672, rk2013)\\) EPIC Institute of Technology Round August 2024 (#Div1+2) 24/8/11 \\((1672 \\rightarrow 1797, rk773)\\) D2 是 \\(\\color{black}{\\tt{B}}\\color{red}{\\tt{rothercall}}\\) 的思路, 不是我的, 谢罪, \\(\\color{black}{\\tt{B}}\\color{red}{\\tt{rothercall}}\\)​ 真乃直觉杀手。 简写一下, 这题问给定排列是不是给定树的 DFS 序, 你手玩一颗树后可以猜想其充必条件为: 对于排列上所有相邻的两点, 它们的 lca 要么为前点(正在下探), 要么为后点的父节点(探完当前子树去新子树) 其必要性显然, 但充分性我真不会证 educational codeforces round 169 (#Div2) 24/8/15 \\((1797 \\rightarrow 1827, rk928)\\)​ E 题目名称 Not A Nim, 实则是 SG 函数的板子, 把这个知识漏洞补了下 codeforces round 973 (#Div2) 24/9/20 \\((1827 \\rightarrow 1800, rk1595)\\) D 分别做两次二分, 可证明两次互不干扰, 我想到二分套二分上去了…… E 小清新数论, 注意到 gcd 的减少至少也要缩一半 (/2), 故缩到 0 最多十次, 贪心解决, \\(O(10n)\\)​ Educational Codeforces Round 170 (#Div2) 24/10/14 卡了三十分钟 C, 因为双指针不会写加上多测不清空, 寄之。 E 题意: 有 \\(n\\) 组牌, 每组牌有 \\([1,m]\\) 数字的牌各一张, 一人拿 \\(n\\times m/2\\) 张牌, 然后要找这样的拿牌方案数, 规定对于一方出的任意一张牌, 另一方都有牌比它大, 每张牌只能用一次。牌的大小是这样比较的: 第一组的牌可以打败任意非同组的牌 若牌的组数相同，则比较牌的数字大小 规定 \\(n,m \\leq 500\\) 且 \\(m\\) 为偶数, 方案数模 \\(998244353\\) 输出。 思路: 假设 \\(n=1\\) , 这本质上就是个卡特兰数 若 \\(n \\neq 1\\) , 第一组牌的取法就成了卡特兰数的变形, 我们可以取 \\(c\\geq m/2\\) 张牌, 在和其他第一组的牌比较完后还能剩 \\(c-m/2\\) 张牌给后面用(相当于万能牌了), 对于 2~n 组, 可以看出每组只能取最多 \\(m/2\\) 张牌, 否则多取的花不出去无法跨组比较，我们可以想到这样一个 DP: \\[ f_{i,j}=f_{i-1,j-1}+f_{i-1,j+1}\\;\\;\\;\\;f_{i,j}\\;表示第1组目前取了i张牌并剩余j张万能牌 的方案数\\] \\[g_i=f_{m,m-i}\\;\\;\\;\\;g_i\\;表示m张卡片取i张并获得i/2场胜利的方案数(剩下m-i张用万能牌取胜)\\] \\[h_{i,j}= \\sum\\limits_{k=0}^j h_{i-1,k}*g_{m+k-j}\\;\\;\\;\\;h_{i,j}\\;表示对于2\\sim i 组,用过了j张万能牌的方案数\\] 答案即为 \\(\\sum\\limits_{i=0}^{m}f_{m,i}*h_{n,i}\\) 复杂度 \\(O(nm^2)\\) 主要代码: 1234567891011121314151617f[0][0]=1; //f[i][j]: choose i cards and rest j cards on handfor(int i=1;i&lt;=M;i++) for(int j=0;j&lt;=M;j++) { if(j&gt;0) f[i][j]=(f[i-1][j-1]+f[i-1][j+1])%p; else f[i][j]=f[i-1][j+1]; } for(int i=0;i&lt;=M;i++) g[i]=f[M][M-i]; //g[i]:choose i cards (i/2 each) and just win h[1][0]=1; //h[i][j]: i-th line, use j extra cards nowfor(int i=2;i&lt;=N;i++) for(int j=0;j&lt;=M;j++) for(int k=0;k&lt;=j;k++) h[i][j]=(h[i][j]+h[i-1][k]*g[M+k-j])%p;int ans=0;for(int i=0;i&lt;=M;i++) ans=(ans+f[M][i]*h[N][i])%p;cout &lt;&lt; ans &lt;&lt; '\\n'; codeforces round 980 (#Div2) 24/10/20 \\((1800 \\rightarrow 1688, rk4628)\\) 寄中寄, 做完 AB 后 C 陷入了先入为主猜结论的死胡同, D 又想假了, 一场就送掉了…… D 题意: 有 \\(n\\) 个问题编号从 \\(1\\) 到 \\(n\\) , 每个问题有得分 \\(a_i\\) 和参数 \\(b_i\\) 从第一个问题开始决策, 每次决策有两种选择: 提交问题获得 \\(a_i\\) 分, 然后问题作废, 下一题的下标 \\(j\\) 是满足 \\(j&lt;i\\) 且问题未作废的最大下标 跳过问题, 然后问题作废, 下一题的下标 \\(j\\)​ 是满足 \\(j \\leq b_i\\)​ 且问题未作废的最大下标 最大化得分, 数据范围 \\(n \\leq 4 \\times 10^5,a_i \\leq 10^9, 1 \\leq b_i \\leq n\\) 思路： 题目本身不是很难, 有优先队列(dp)解法, 大体思路是设 \\(f_i\\) 为到 \\(i\\) 点的最小花费, 更新 \\(i\\) 时, 找队列中 \\(b_j \\geq i\\) 且代价最小的作为 \\(f_i\\) , 更新完后把 \\(\\{a_i+f_i,b_i\\}\\) 压入优先队列中, 记得开小根堆。 然而这题真正有趣的在于其图论刻画, \"下一题\" 类似于一种有向边, 最大化得分则是最长路, 考虑建图: \\(i \\stackrel{-a_i}{\\longrightarrow} b_i\\) , 表示跳过问题 \\(i \\stackrel{0}{\\longrightarrow} i-1\\) 表示提交问题 \\(i \\stackrel{\\sum_{j=1}^i a_j}{\\longrightarrow} 0\\) 表示不再跳过问题, 按顺序回答前面的每个问题直至结束 只有跳过问题会导致分数的减少, 只有不再跳过才进行分数的结算, 故提交问题的边权为 \\(0\\) 代码(来源知乎): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;bits/stdc++.h&gt;using namespace std;using ll = long long;constexpr ll inf = 0x3f3f3f3f3f3f3f3f;int main() { int t; cin &gt;&gt; t; while (t--) { int n; cin &gt;&gt; n; n++; vector&lt;int&gt; a(n); vector&lt;ll&gt; pre(n); for (int i = 1; i &lt; n; i++) { cin &gt;&gt; a[i]; pre[i] = pre[i - 1] + a[i]; } vector&lt;vector&lt;pair&lt;ll, int&gt;&gt;&gt; E(n); for (int i = 1; i &lt; n; i++) { E[i].push_back({ 0, i - 1 }); E[i].push_back({ pre[i], 0 }); } for (int i = 1; i &lt; n; i++) { int x; cin &gt;&gt; x; E[i].push_back({ -a[i], x }); } auto Dijkstra = [&amp;](int s) { vector&lt;int&gt; vis(n); vector&lt;ll&gt; dis(n, -inf); dis[s] = 0; priority_queue&lt;pair&lt;ll, int&gt;&gt; Q; Q.push({ 0, s }); while (!Q.empty()) { auto [d, u] = Q.top(); Q.pop(); if (vis[u]) continue; vis[u] = 1; for (auto&amp; [w, v] : E[u]) if (dis[v] &lt; w + d) { dis[v] = w + d; Q.push({ dis[v], v }); } } return dis; }; auto d = Dijkstra(1); cout &lt;&lt; d[0] &lt;&lt; &quot;\\n&quot;; } return 0;} codeforces round 982 (#Div2) 24/10/26 \\((1688 \\rightarrow 1793, rk301)\\) 手速场, D2 不会, E 是我见过最难的 SG 函数了 E1 题意: \\(n\\) 堆石子, 每堆 \\(x_i\\) 个, 有一个限制上限 \\(a_i\\) , 两人先后手在一堆石子中取, 每次可以取 \\(d\\) 个, 满足 \\(1 \\leq d \\leq a_i\\) 且 \\(x \\;\\&amp;\\;d=d\\) 其中 \\(x\\) 是当且这堆石子的数量, 问理想状态下谁赢 ? \\(n \\leq 10^4; a_i,x_i \\leq 2^{30}\\) 思路: 一眼 SG 函数, 但是不能只靠打表 当 \\(a_i \\geq x_i\\) 时相当于 \\(a_i\\) 对 \\(x_i\\) 没有影响, 此时 \\(sg(x_i) = popcount(x_i)\\) 即 \\(x_i\\) 在二进制下 \\(1\\) 的个数。 当 \\(a_i &lt; x_i\\) 时, 我们尝试归并到上面情况: 首先忽略 x 比 a 高的位数, 因为肯定取不上去, 完成 a, x 位数的对齐, 例如 a=100, x=1101 等价于 a=100, x=101 对于 a=100010, x=110000 , \\(x\\) 中第一个比 \\(\\underline{a}\\) 大的位置后的 1 的数量为 0, 所以 \\(x\\) 不管怎么取之后 \\(x' &lt; a\\) 且 \\(x' \\neq 0\\), 所以 \\(sg(x)=0\\) 对于 a=100010, x=110001, 其等价于 a=100001, x=110001, (对于 x=0, a=1的第一位, a 后面的 0 都翻成 1), 这又等价于 a=101, x=111(去除两边都为 0 的部分), 这下子 x 被我们消成了 \\(2^k-1\\) 的形式 codeforces global round 27 24/10/27 \\((1793 \\rightarrow 1788, rk1780)\\) 每次都天真的往 D 糊一个 1400 难度的假做法, 还恰巧能过样例, 于是上头狂调, 一小时有余方才发现错误, 亡羊补牢为时已晚, 掉了小分 E 题意: 初始攻击力为 \\(0\\) ，进行以下操作，以最少的代价击败生命值为 \\(z\\) 的 Boss： 1.攻击力提升 \\(1\\) ，花费 \\(x\\) 的代价。最多连续进行 \\(k\\) 次攻击力提升。 2.进行一次攻击，造成相当于攻击力的伤害，花费 \\(y\\) 的代价。 \\(1 \\leq x,y,z,k \\leq 10^8\\), 100 组数据 思路: 一开始蒙了个三分, 认为先提升攻击最后进行攻击的击败代价函数是个二次函数, 后来发现不对劲, 因为整数的离散性需要上取整, 函数不是平滑的。 然而总体的思路不变, 先升级再打, 注意到不是每次升级攻击力都会减少回合数, 例如对于血量为 \\(20\\) 的 boss 而言, 只有攻击力提升到 \\(1,2,3,4,5,7,10,20\\) 时才会减少回合数, 可以观察到临界值个数的数量级为 \\(\\sqrt z\\) 于是我们每次尝试拿 \\(min(\\sqrt z, k)\\) 个临界值更新临界答案 (即加到临界值的攻击后停止提升并一直攻击的答案) , 然后进行一次攻击, 若 boss 仍有血量并继续循环, 循环次数为 \\(O(\\sqrt{z/k})\\) 次, 一次循环有 \\(min(\\sqrt z, k)\\) 个临界值, 当 \\(k \\sim O(\\sqrt z)\\) 时, 复杂度为 \\(O(z^{3/4})\\) 代码:link educational codeforces round 171 (#Div2) 24/10/28 \\((1788 \\rightarrow 1810, rk986)\\)​ E 题意: 给定大小为 \\(n\\) 的数组 \\(a\\), 现在让你从 \\(a\\) 中选取一些数, 记为数组 \\(b\\) , 求 \\(|b|-popcount(\\bigvee b_i )\\) 最大值 \\(T, n \\leq 100, a_i \\leq 2^{60}\\) 思路: 十分的 educational , 最大权闭合图裸题, 设 1 ~ N 为数组各位, N+1~N+60 为 60 个二进制位, 连边: \\(S\\stackrel{1}{\\longrightarrow} i (i \\in [1,n])\\) \\(i (i \\in [1,n]) \\stackrel{inf}{\\longrightarrow} n+j \\;\\; ((a_i &gt;&gt; j) \\&amp; 1==1)\\) \\(n+j\\stackrel{1}{\\longrightarrow} T\\) 答案即为 \\(n-dinic()\\) 代码: link codeTON round 9 (#Div1 + 2) 24/11/23 \\((1810 \\rightarrow 1852, rk1139)\\) ​ C2 过了还是很懵, 分类讨论容易把人写浑, 不要追求讨论不重​, 追求不漏就可以了 E 题意: 初始 \\(a=\\{0,1\\}\\), 每次操作将 \\(a\\) 的逆序对个数选择插入 \\(a\\) 的任意位置, 求长度为 \\(n\\) 的 \\(a\\) 数组的方案数, \\(n \\leq 10^6\\) 思路: 就不抄一遍 Tutorial 了, 思想就是分类, 递推。因为注意到当 \\(a\\) 的逆序对个数大于 \\(a\\) 的最大值时, 所有插法都是新的, 记这样的 \\(a\\) 为 \"好数组\"。于是可以先考虑一个长度为 \\(i\\) 的 \"好数组\" 可以构成多少个长度为 \\(n\\) 的 \"好数组\", 再看有多少个 \"好数组\" 可以由 \"坏数组\" 一步构造成, 答案为 \"坏数组\" 数量加上一个 $ a*b $ 形式的东西 代码: link codeforces round 989 (#Div1 + 2) 24/11/30 \\((1852 \\rightarrow 1795, rk2503)\\) D 把一个 set 的 rbegin() 改成 begin() 就过了, 告诉我们题目要想清楚一点再写, 因为以后有你急的 codeforces round 992 (#Div2) 24/12/08 \\((1795 \\rightarrow 1835, rk554)\\) E 的实现丑陋，期望还在拿数列极限推，其实题目思想比较经典，可惜了 E 题意: 初始在树上选择一个非根点后开始操作, 奇数步往根跳, 偶数步可以支付一元往根跳, 否则随机跳, 给你树的形态, 进行 \\(Q\\) 次询问, 每次询问提供初始点和钱的数量 \\(k\\) , 问跳到树根所需次数的最小期望 \\(k \\leq N \\leq 2000, Q\\leq 2000\\) 思路: 考场上把一条链上的需要花钱的点抽出来按期望 sort 后毙掉花钱最多的 \\(k\\) 个, 但是期望算错了并且实现很丑, 实际上可以树型 DP 解决 设点 \\(u\\) 往上跳的步数期望为 \\(P(u)\\) , 有 \\(P(u) = \\frac{1}{son_u+1} + \\frac{son_u}{son_u+1} \\times (P(u)+2)\\), 解得 \\(P(u) = 2 \\times son_u+1\\) 设 $f_{i,j} $ 表示点 \\(i\\) 花费 \\(j\\) 元跳到根节点的最小期望, 有: \\[f_{root,i} = 0,\\; f_{son_{root},i} = 1,\\; f_{v,0}=f_{fa_u,0}+P(u)+1 (v \\notin son_{root})\\] \\[f_{v,i} = min(f_{fa_u,i-1}+2,f_{fa_u,i}+P(u)+1)\\] 然后 DP 即可, 复杂度 \\(O(n^2)\\) 代码:link Good Bye 2024: 2025 is NEAR 24/12/28 \\((1835 \\rightarrow 1785, rk3180)\\) 纯纯小丑，D 一眼题却实现丑陋，WA 的要道心破碎了，果然以后有我急的，应该以后还有我急的。 E 看上去挺漂亮 E 题意: 给定一棵大小为 \\(N\\) 的树, 定义 \\((p,q)\\) 为树上 \\(p,q\\) 之间的路径, 在每一回合, Alice 可以选择一个连接 \\(p\\) 且不在路径上的点 \\(u\\) , 并将整个路径向 \\(u\\) 方向移动, Bob 可以选择一个连接 \\(q\\) 且不在路径上的点 \\(v\\) , 并将整个路径向 \\(v\\) 方向移动。当任何时刻 \\(p\\) 为度数为一的节点时, Alice 获胜, 当任何时刻 \\(q\\) 为度数为一的节点时, Bob 获胜, 否则平局。问有多少个有序对 \\((p,q)\\) 能让 Bob 获胜？\\(N \\leq 200000\\) ​ 思路: 考场上想出来了 80%, 场下找到了实现方式, 觉得自己思路比较自然(抽象), 所以分享一下。 让我们定义度数为一的点为叶子节点(一类点), 连接其的点为二类点, 若都不是即为三类点。 有一种明显对答案的贡献方式, 即 \\(p\\) 为非一类点, \\(q\\) 为一类点, 所以 Bob 上来就赢了 让我们想想还有什么方式能对答案贡献, 若 \\(p\\) 为一类点或二类点肯定不行, 当 \\(p\\) 为三类点时, 在 Alice 走完一步后, 若 \\(q\\) 变成二类点, 则 Bob 可以向 \\(q\\) 连接的一类点走一步来获得胜利。 CF2053E 如样例四的图: 绿色为一类点, 黄色为二类点, 青色为三类点, 答案为 \\(5 \\times 5 + (3,5) + (5,3) = 27\\) 我们要想办法统计每个作为三类点的 \\(p\\) 对答案的贡献, 若 \\(q\\) 在 \\(p\\) 的子树中, 不难发现 \\(p\\) 会使 \\(q\\) 上移为其父节点; 若 \\(q\\) 不在根到 \\(p\\) 的路径上, \\(q\\) 也会上移为其父节点; 若 \\(q\\) 在根到 \\(p\\) 的路径上, 则 \\(q\\) 会朝 \\(p\\) 方向下移。 上移的方向是确定的, 而下移的方向因 \\(p\\) 而异, 这启示我们先统计每颗子树的上移贡献情况, 再在 dfs 时处理根到 \\(p\\) 的路径。 代码的实现用了三次 dfs (尽显幽默): 第一次处理一、二、三类点; 第二次处理 \\(num[x]\\) , 即以 \\(x\\) 为根的子树中有多少非一类点的父节点为二类点; 第三次处理统计贡献。 同时, 可以注意到我一直在用一类点的定义而非 “叶节点”, 因为如果从随意一点开始 dfs 的话根节点可能也为一类点, 在代码中我寻找第一个度数 \\(\\geq 2\\) 的点开始 dfs, 特判一下 \\(N=2\\) 的情况。 代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#include&lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;typedef long long ll;const int MX=200020;int T, N, leaf=0;ll ans=0;int head[MX], cnt=0, in[MX], son[MX], num[MX];bool isl[MX], isu[MX], to[MX];int read(){ int r=0, f=1; char ch=getchar(); while(ch&lt;'0'||ch&gt;'9') {if(ch=='-') f=-1; ch=getchar();} while(ch&gt;='0'&amp;&amp;ch&lt;='9') {r=r*10+ch-'0'; ch=getchar();} return r*f;}struct edge{int nxt, to;}e[2*MX];inline void ae(int u, int v){e[++cnt].to=v; e[cnt].nxt=head[u]; head[u]=cnt; in[u]++;}void dfs(int x, int f){ bool tag=0; //son tag for(int i=head[x];i;i=e[i].nxt) { int y=e[i].to; if(y==f) continue; dfs(y,x); tag=1; if(isl[y]) isu[x]=1; //type 2 } if(!tag) isl[x]=1, leaf++; //type 1}void dfs2(int x, int f){ if(isu[f]&amp;&amp;!isl[x]) to[x]=num[x]=1; for(int i=head[x];i;i=e[i].nxt) { int y=e[i].to; if(y==f) continue; dfs2(y,x); num[x]+=num[y]; }}void dfs3(int x, int f, int cur){ for(int i=head[x];i;i=e[i].nxt) { int y=e[i].to; if(y==f) continue; dfs3(y,x,cur+num[x]-to[x]-num[y]+isu[y]); } if(!isl[x]&amp;&amp;!isu[x]) //type 3 ans=ans+1ll*(cur+num[x]-to[x]); //contribute 2}int main(){ T=read(); while(T--) { N=read(); for(int i=1;i&lt;N;i++) { int u=read(), v=read(); ae(u,v); ae(v,u); } if(N==2) cout &lt;&lt; &quot;0\\n&quot;; else { for(int i=1;i&lt;=N;i++) if(in[i]&gt;=2) {dfs(i,0); break;} ans+=1ll*leaf*(N-leaf); //contribute 1 for(int i=1;i&lt;=N;i++) if(in[i]&gt;=2) {dfs2(i,0); break;} for(int i=1;i&lt;=N;i++) if(in[i]&gt;=2) {dfs3(i,0,0); break;} cout &lt;&lt; ans &lt;&lt; '\\n'; } for(int i=1;i&lt;=N;i++) head[i]=in[i]=isl[i]=isu[i]=to[i]=num[i]=0; cnt=leaf=ans=0; } return (0-0);} 其中 isl[x] 代表一类点, isu[x] 代表二类点, in[x] 为度数, to[x] 代表非一类点的 \\(x\\) 上探是否为二类点, num[x] 为以 \\(x\\) 为根的子树中有多少非一类点的父节点为二类点。 第三次 dfs3 中的 cur 变量代表当前子树外的统计情况, 贡献答案的时候还要加入当前子树, 并记得把 \\(x\\) 的情况扣掉, 因为 \\(p \\neq q\\) 在理解上文的情况下我们可以看见官方题解更简单的做法, 我们能注意到一个更强的事实, 假设三类点总数为 \\(c\\), 一个二类点 \\(x\\) 相邻的非一类点数量为 \\(m\\) , 则这个二类点会对答案产生 \\(c \\times (m-1)\\)​​ 的贡献, 因为对于任意三类点 \\(p\\) , 考虑 \\(q\\) 在移动后到达该二类点 \\(x\\) , 总存在唯一的 \\(q\\) 在 \\((p,x)\\) 路径上不合法, 而其他点都是合法的且移动方向正确, 这样我们也不用弄什么 dfs 了, 直接统计即可。 我把官方题解代码也放在了下面并加上一点我的注释: 1234567891011121314151617181920212223242526272829303132#include &lt;bits/stdc++.h&gt;#define MAXN 200001std::vector&lt;int&gt; g[MAXN];inline int deg(int u) { return g[u].size(); }int d[MAXN]; // d[x]: x 点连接的 2,3 类点数量void solve() { int n; std::cin &gt;&gt; n; long long ans = 0; for (int i = 1, u, v; i &lt; n; ++i) { std::cin &gt;&gt; u &gt;&gt; v; g[u].push_back(v), g[v].push_back(u); } int c1 = 0, c2 = 0; //type 1 num, type 3 num for (int i = 1; i &lt;= n; ++i) c1 += (deg(i) == 1); ans += 1ll * c1 * (n - c1); //contribute 1 for (int i = 1; i &lt;= n; ++i) if (deg(i) &gt; 1) { for (int v : g[i]) d[i] += (deg(v) &gt; 1); c2 += (d[i] == deg(i)); } for (int m = 1; m &lt;= n; ++m) if (deg(m) &gt; 1 &amp;&amp; d[m] != deg(m)) //type 2 point m ans += 1ll * c2 * (d[m] - 1); //contribute 2 std::cout &lt;&lt; ans &lt;&lt; '\\n'; for (int i = 1; i &lt;= n; ++i) (std::vector&lt;int&gt;()).swap(g[i]), d[i] = 0;}int main() { std::ios::sync_with_stdio(false); std::cin.tie(nullptr), std::cout.tie(nullptr); int t; std::cin &gt;&gt; t; while (t--) solve(); return 0;}","link":"/2025/01/05/Codeforces-exercises-2024/"},{"title":"Codeforces exercises 2022","text":"记录了22/5/31-22/11/20的三十七场 codeforces 补题, \\(rating \\;1568\\rightarrow1906\\)​ 文章因为过于陈旧已不再受维护 codeforces round 795 (#Div2) 22/5/31 \\((1568\\rightarrow1532, rk3394)\\) 这几场都打的很寄, 例行只会ABC, stl不会用,场上一共寄了六发,都是各种边界条件…… D 题意： 给定长度为 \\(n\\) 的数组 \\(a (-10^9\\leq a_{i} \\leq 10^9)\\), 问是否能找到一段区间 \\([L,R]\\) 使得 \\(max([L,R])&lt;\\sum\\limits_{i=L}^{R}a_{i}\\) 思路： 区间的选择有 \\(n^2\\) 种, 区间最大值的选择只有 \\(n\\) 种, 因此要枚举所有的 区间最大值, 然后让包含它的区间最大。 流程： \\(1.\\) 单调栈找到每个 \\(a_{i}\\) 第一个左边比它大的数 \\(L_{i}\\) 与右边比它大的数\\(R_{i}\\) \\(2.\\) 对 \\(a\\) 做前缀和, 对 \\(pre\\) 建线段树, 找到 \\([i,R_{i}-1]\\) 中的最大值和\\([L_{i},i-1]\\)中的最小值, 两个相减得到包含 \\(a_{i}\\) 的最大区间。 代码注意：线段树下标从 \\(0\\) 开始, 记得开 \\(long\\) \\(long\\) 代码： 1234567891011121314151617181920212223242526272829for(int i=1;i&lt;=N;i++) l[i]=r[i]=0; top=0;N=read();for(int i=1;i&lt;=N;i++) ar[i]=read();ar[N+1]=inf;for(int i=1;i&lt;=N+1;i++){ while(top&amp;&amp;ar[stac[top]]&lt;ar[i]){ //单调栈, 看每个 a[i] 对左边的影响 r[stac[top]]=i; // 左边的 a[stac[top]] 找到右边第一个比他大的数 a[i] top--; } stac[++top]=i;}top=0; ar[0]=inf;for(int i=N;i&gt;=0;i--){ while(top&amp;&amp;ar[stac[top]]&lt;ar[i]){ l[stac[top]]=i; // 右边的 a[stac[top]] 找到左边第一个比他大的数 a[i] top--; } stac[++top]=i;}for(int i=1;i&lt;=N;i++) pre[i]=pre[i-1]+ar[i]; bt(1,0,N);int ck=0;for(int i=1;i&lt;=N;i++){ int t1=ask(1,0,N,i,r[i]-1,1), t2=ask(1,0,N,l[i],i-1,2); //线段树求最值, 略 if(ar[i]&lt;t1-t2) {ck=1; break;}}if(ck==1) cout &lt;&lt; &quot;NO\\n&quot;;else cout &lt;&lt; &quot;YES\\n&quot;; 是个好题, 告诉我们做题要找简单切入点, 不要盯着 \\(n^2\\) 的区间选择看…… E 题意： \\(N\\) 条线段, 两种不同的颜色, 若不同颜色的线段有交集则说明是联通的(一个交点也算), 问有多少个连通块。 思路： 好想，难写(对于我这个不会用STL的菜鸡)。 暴力连边 \\(O(N^2)\\) 不可取, 尝试排序线段端点，带删连边。 我们可以把一条线段拆成左右两端点, 到左端点时加入, 右端点时删除, 存入两个 \\(set\\) 中(代表两个不同颜色), 加入左端点时, 若另一个 \\(set\\) 中还有元素(另一个 \\(set\\) 中的线段都包含这个左端点), 则全部合并, 因为全部合并, 可以将另一个 \\(set\\) 中的元素删除到仅剩一个右端点最大的元素为止, 最后统计连通块数目。 代码： 123456789101112131415161718192021222324252627282930313233343536373839struct seg {int l, r, col, pos;}; seg ar[MX];int main(){ T=read(); while(T--){ N=read(); for(int i=1;i&lt;=N;i++) fa[i]=i; vector&lt;PII&gt; op; for(int i=1;i&lt;=N;i++){ int col=read(), l=read(), r=read(); ar[i]=(seg){l,r,col,i}; op.push_back({l,-i}); //use &quot;i&quot; and &quot;-i&quot;, we can easily discern the left or right endpoint op.push_back({r,i}); // STL pair will compare the second keyword if the first keyword is the same } // so we MUST set the left endpoint -i, because we can connect segments with the same endpoint (not the same side) too. sort(op.begin(),op.end()); //By the default order, the first element of the pair. set&lt;PII&gt; s[2]; //Store two colors. for(auto it=op.begin();it!=op.end();it++) //The nobel auto guides us. { //&quot;auto [pos,id] : op&quot; is an alternative option by tourist, only works on c++17 (Structured binding declaration) PII tmp=*it; int pos=tmp.first; int id=tmp.second; if(id&lt;0){ //left endpoint, add it id=-id; s[ar[id].col].insert({ar[id].r,id}); int t=ar[id].col^1; //another color while(s[t].size()&gt;1){ //find in the another set merge(id,s[t].begin()-&gt;second); s[t].erase(s[t].begin()); } if(!s[t].empty()) merge(id,s[t].begin()-&gt;second); } else s[ar[id].col].erase({ar[id].r,id}); //right endpoint, delete it } int res=0; for(int i=1;i&lt;=N;i++) res += (find(i)==i); cout &lt;&lt; res &lt;&lt; endl; } return (0-0);} 有几点想说： 1、\\(auto\\) 真好用, \\(set\\) 真香, 这里 \\(pair\\) 换 \\(struct\\) 也可以, 记得 \\(set\\) 里存的右端点。 2、代码是改 \\(tourist\\) 的, 在拆端点时记得给左边赋 \\(-i\\) ,因为有一个交点也算连通块。 3、这题拖了一个月, 因为一开始 STL 蛋都不懂。 codeforces round 796 (#Div2) 22/6/3 \\((1532\\rightarrow1639, rk438)\\) 全场贪心(指我会的ABCD), 猜了一个结论, E最小生成树全忘了, 到时候复健一下, F很有趣, 但不会。场上只寄了一发F, 但A题写了19mins, 下次思考完再动手…… E (Div1 B) 复健了一下…… 交互题 题意: \\(N\\) 个点, \\(M\\) 条边的图(不一定联通), \\(2M\\) 次询问, 每次自由加一些边, 告诉你最大生成树林的边权和, 你要最后回答最小生成树林的边权和。 思路： 先用 \\(M\\) 个类似 \\(0000...0010...0000\\) 的字符串求出每一条边的边长, 从小到大排序一下, 再贪心 \\(M\\) 次尝试一条条加回去, 如果加回去后返回的最大森林边权和等于目前的答案加上加入的那条边的边权, 则说明新加入的这条边联通了两个块, 应该保留, (类似 \\(kruskal\\) 的思想), 最后输出答案。 代码： 123456789101112131415161718192021222324N=read(); M=read();string str;for(int i=1;i&lt;=M;i++) str+='0';for(int i=0;i&lt;M;i++){ str[i]='1'; cout &lt;&lt; &quot;? &quot; &lt;&lt; str &lt;&lt; endl; fflush(stdout); cin &gt;&gt; e[++cnt].d; e[cnt].pos=cnt; str[i]='0';}sort(e+1,e+1+M);for(int i=1;i&lt;=M;i++){ int tmp=0; str[e[i].pos-1]='1'; cout &lt;&lt; &quot;? &quot; &lt;&lt; str &lt;&lt; endl; fflush(stdout); cin &gt;&gt; tmp; if(tmp==ans+e[i].d) ans=tmp; else if(tmp!=ans+e[i].d) str[e[i].pos-1]='0';}cout &lt;&lt; &quot;! &quot; &lt;&lt; ans &lt;&lt; endl; F (Div1 C) 题意： 给定两个长为 \\(n\\) 的数组 \\(a\\), \\(b\\), 再给 \\(m\\) 个区间 \\([l_{i}, r_{i}]\\) , 每次操作可以选择一个 \\(\\sum\\limits_{j=l_{i}}^{r_{i}}a_{i}=\\sum\\limits_{j=l_{i}}^{r_{i}}b_{i}\\) 的区间, 把 \\(b\\) 的这部分换到 \\(a\\) 上, 问能否通过若干次操作将 \\(a\\) 换成 \\(b\\)。 思路： 先是套路部分的题意转换, 区间相等\\(\\implies\\)做前缀和. 令 \\(c_{i}=a_{i}-b_{i}\\) , \\(pre_{i}=pre_{i-1}+c_{i}\\) , 题意转化为当某个 \\(pre_{l_{i}-1}=pre_{r_{i}}\\) 时将 \\([pre_{l_{i}},pre_{r_{i}}]\\)全部赋为 \\(pre_{l_{i}-1}\\)(因为此时\\(c_{i}=a_{i}-b_{i}=0\\), 前缀和不再变化), 我们要使得最后 \\(pre\\) 数组清零。 一个 \\(key\\;observation\\) 是我们要贪心的选取满足 \\(pre_{l_{i}-1}=pre_{r_{i}}=0\\)的可操作区间, 注意到这样的操作会使得\\([pre_{l_{i}},pre_{r_{i}}]=0\\)， 然后我们继续寻找可操作区间。 为什么这样做是对的？我们最终的目标是全部置 \\(0\\), 如果一个位置 \\(pre_{i}\\neq0\\), 要想把这个位置置 \\(0\\), 一定存在一个 \\([l_{j}, r_{j}]\\)包含位置 \\(i\\) 且 \\(pre_{l_{j}-1}=pre_{r_{j}}=0\\) 流程： 贺的题解代码, 现学现卖用 \\(set\\) 维护尚不能更新的区间 \\(1.\\) 读入, 前缀和, 不能用的点 ( \\(pre_{i}\\neq0\\) ) 放 \\(set\\) 里, 能用的点压入 \\(queue\\), \\(vector\\) 存一下每个点作为起点或终点时对应的起点终点。 \\(2.\\) \\(iterator\\;lower\\_bound\\) 维护一下, 找到可更新的区间就把 \\(set\\) 里区间里的点压进 \\(queue\\) 中， 若最终 \\(set\\) 为空则说明可行。 代码： 123456789101112131415161718192021222324252627282930313233N=read(); M=read();for(int i=1;i&lt;=N;i++) ar[i]=read();for(int i=1;i&lt;=N;i++) br[i]=read();for(int i=1;i&lt;=N;i++) pre[i]=pre[i-1]+ar[i]-br[i]; // if pre[l-1]=pre[r], that this sub-segment is &quot;Operate-able&quot;.queue&lt;int&gt; q; set&lt;int&gt; S;S.clear(); while(!q.empty()) q.pop();for(int i=0;i&lt;=N;i++) { if(pre[i]!=0) S.insert(i); if(pre[i]==0) q.push(i);}for(int i=1;i&lt;=M;i++){ int l=read(), r=read(); ed[l-1].push_back(r); //starting stores endings ed[r].push_back(l-1); //ending stores startings}while(!q.empty()){ int u=q.front(); q.pop(); for(int i=0;i&lt;ed[u].size();i++) //for every point, enumerate the startings or endings. { int v=ed[u][i]; if(pre[v]) continue; //not accept the condition: pre[]=0; int l=min(u,v), r=max(u,v); IT it=S.lower_bound(l); //typedef set&lt;int&gt;::iterator IT; while(it!=S.end()&amp;&amp;(*it)&lt;=r) { int p=*it; pre[p]=0; q.push(p); IT f=it; it++; S.erase(f); } }}if(S.empty()) cout &lt;&lt; &quot;YES\\n&quot;;else cout &lt;&lt; &quot;NO\\n&quot;;for(int i=0;i&lt;=N;i++) ed[i].clear(); 注意 \\(iterator\\) 不要原地删除后再跳…… codeforces round 800 (#Div2) 22/6/16 $(1639, rk574) $ C, D玄学猜结论过题, E 题真是人类智慧好题, 但我不会. E (Div1 C) 题意： 给出一个有向图起点为 \\(1\\), 终点为 \\(N\\), 每次操作可以删去一条边或者随机选择一条边移动,问最优的操作下从起点移动到终点至多要多少步。 思路： 定义 \\(dis_{u}\\) 为 \\(u\\) 到 \\(N\\) 所需的最小边数, 对于一个 \\(u\\) 的子节点 \\(v_{1}, v_{2},\\cdots, v_{m}\\) 有 \\(dis_{v_{1}}, dis_{v_{2}},\\cdots, dis_{v_{m}}\\), 设 \\(dis_{v_{1}},dis_{v_{2}},\\cdots, dis_{v_{m}}\\) 升序排列, 保证选到 \\(v_{j}\\) 或更优就要删掉 \\(dis_{v_{j+1}},dis_{v_{j+2}},\\cdots, dis_{v_{m}}\\) 这 \\(m-j\\) 条边. 我们反向建图, 可以保证每次更新的节点都是正向图中未被更新的 \\(dis\\) 最小的, 具体如例图： 最短路算法会先跑到 \\(v1\\) , 再到 \\(v2\\) , 再到 \\(v3\\) , 我们把边权设为入点 \\(u\\) 还没走过的子节点数 \\(+1\\) 跑 \\(dijkstra\\) 就是最后的答案. 代码( \\(dijkstra\\) 记录 \\(in\\) 数组)： 123456789//dijkstra part......for(int i=head[x];i;i=e[i].nxt){ int y=e[i].to; if(dis[y]&gt;dis[x]+in[y]){ dis[y]=dis[x]+in[y]; q.push((node){dis[y],y}); } in[y]--;} codeforces round 801 (#Div2) 22/6/18 $(1697, rk1030) $ D 题真是人类智慧好题(下次我怕不是要夸 C 题了), 但我还是不会 想 rush 出 D 的 easy version, 失败了, 上小分 这次ABC做得都不顺, 属于真实水平暴露了 D1 题意： 无根树上一个隐藏的顶点 \\(x\\) , 每次问询你可以知道被问询点与隐藏点 \\(x\\) 在树上的距离, 给你树的结构, 让你找出最小的问询次数 \\(k\\) 使得一定存在一系列问询点 \\(v_{1}, v_{2}, ... , v_{k}\\) 能寻找到 \\(x\\) 点. 树上节点数量小于2000 思路： 没有思路, 题解是贺的. \\(key\\;observation:\\) 对于任意一个根节点 \\(u\\) , 其所有子树中, 只能有一颗子树不含查询点, 否则不含查询点的子树的高度相等的点无法排除，只剩一颗子树不含查询点则可以通过容斥确认 \\(x\\) 点所在子树。 考虑 \\(O(n^2)\\) 暴力树型 \\(DP\\) , 枚举每个根节点并保证选取, 接下来要做的就是确定唯一的子树节点。状态转移 \\(dp_{u}=\\sum\\limits_{v\\in son_{u}}dp_{v}+max(cnt_{u}-1,0)\\) , 其中 \\(cnt_{u}\\) 代表子树个数为 1 的 \\(u\\) 的子节点数量, 最终答案 \\(dp_{root}+1\\) , \\(dp_{root}\\) 记得取 \\(min\\) , 答案最后再取根节点 \\(+1\\) 。 代码：link D2 题意同上。 树上节点数量小于2e5 思路： 做完 \\(D1\\) 再来看, 一眼 换根DP , 但其实有更优雅的解法(毕竟我也不会换根DP)。 考虑到枚举根节点复杂度过大, 尝试压缩，有两个结论： 1、除了根是否要询问，其余询问的总次数固定。 2、若根节点的的子节点数量 \\(&gt;2\\) ， 则这个根节点不需要询问。 这两个观察是“信仰之跃”式的思维跳跃, 关键是注意到 D1 的子树上查询点的选择其实等价于对于每个以 \\(v\\) 为根的子树, 要么查找到 \\(v\\) 节点, 要么在 \\(v\\) 节点的子树外有查询以确定 \\(x\\) 点所在的子树, 当根节点的的子节点数量 \\(&gt;2\\) 时几个节点互相确定, 不用选根节点, 性质二又能推出性质一(换根)。 如果没有根节点的子节点的数量 \\(&gt;2\\) 原树则为一条链, 答案为 1。 抄完题解还是较难理解…… 代码：link Educational codeforces round 131 (#Div2) 22/7/8 $(1715, rk2820) $ 需要复健，怕是废了。 \\(ABC\\) 都做得很慢, 还发现简单套路题做得真的不多, 慢慢刷吧…… D 题意：由排列 \\(a\\) 构造出数组 \\(b\\) , 其中 \\(b_{i}= \\lfloor\\frac{i}{a_{i}}\\rfloor\\) ,现在已知 \\(b\\) , 求一个可行的 \\(a\\) . 思路： 1、要确定每一个 \\(b_{i}\\) 对应的 \\(a_{i}\\) 的范围, 我自己用数论分块 + 二分弄出 \\(a_{i}\\) 的范围, 大佬题解的搞法是找等价条件， 注意不到 \\(b_{i}= \\lfloor\\frac{i}{a_{i}}\\rfloor\\) 等价于： \\[b_{i}\\leq\\frac{i}{a_{i}}&lt;b_{i}+1\\] 又等价于： \\[ \\lfloor\\frac{i}{b_{i}+1}\\rfloor+1 \\leq a_{i} \\leq\\lfloor\\frac{i}{b_{i}}\\rfloor\\] 注意这里的左界的符号变化。 2、此时问题转换为区间选数构成排列，现场我就寄在这里…… 简单贪心一下：从 \\(1\\) 到 \\(N\\) ,每次加入这个左端点对应的所有区间, 选择右端点最小的先处理, 这样可以给后来的多一点选择空间。 实现： 对每一个左端点建一个 \\(vector\\) , 存 \\(pair\\), 第一键值右端点, 第二键值存下标 , 再建一个 \\(pair\\) 的 \\(set\\), 循环时把对应 \\(vector\\) 全压进 \\(set\\) 里, \\(set\\) 每次处理一个。 代码： 123456789101112131415for(int i=1;i&lt;=N;i++) v[i].clear();for(int i=1;i&lt;=N;i++) { int l, r; b[i]=read(); if(b[i]==0) l=i+1, r=N; else l=i/(b[i]+1)+1, r=i/b[i]; v[l].push_back({r,i});}for(int i=1;i&lt;=N;i++){ for(auto x:v[i]) s.insert(x); //that's really useful PII it=*s.begin(); ar[it.second]=i; s.erase(it);}for(int i=1;i&lt;=N;i++) cout &lt;&lt; ar[i] &lt;&lt; &quot; &quot;; cout &lt;&lt; endl; 注意： 不要在函数内定义你该死的 vector[MAXN]. E 开始赛后补题…… 题意：给你两个字符串 \\(s, t\\)，长度为 \\(N, M\\) , 均小于 5000, 保证 \\(N\\geq M\\) , 问至少多少个操作能将 \\(s\\) 转换为 \\(t\\) , 不能则输出 \\(-1\\) , 一开始有一个光标在 \\(s\\) 末尾, 操作有五种：前移一位光标; 后移一位光标; 光标置于最前; 光标置于最后; 删除光标前的字母。 思路：很明显光标肯定只会回到最前端一次或不回去, 不会反复横跳, 并可能在中间存在不需修改的部分。 我们可以把 \\(s\\) 串分为三部分 DP , 前修改串, 中保留串(不修改), 后修改串。然后开始贺题解： 设 \\(f_{i,j,0/1/2}\\) 表示处理到 \\(s\\) 的第 \\(i\\) 个字母, 并已经和 \\(t\\) 的前 \\(j\\) 个字母匹配完成, 0/1/2 分别代表 前/中/后 三部分。 1、在任意时刻前缀与后缀部分都可以删除后/前字母, 其中删除后字母的代价为两次操作(右移+删除), 而删除前字母的代价为一次操作(删除), 转移如下： \\[f_{i,j,0}=min(f_{i,j,0},f_{i-1,j,0}+2)\\] \\[f_{i,j,2}=min(f_{i,j,2},f_{i-1,j,2}+1)\\] 2、在 \\(s_{i}=t_{j}\\) 时刻, 可仅仅移动光标完成匹配, 中间部分则直接沿用： \\[f_{i,j,0}=min(f_{i,j,0},f_{i-1,j-1,0}+1)\\] \\[f_{i,j,1}=min(f_{i,j,1},f_{i-1,j-1,1})\\] \\[f_{i,j,2}=min(f_{i,j,2},f_{i-1,j-1,2}+1)\\] 3、任意时刻的匹配都能从处于前面的部分转入处于后面的部分： \\[f_{i,j,2}=min(\\{f_{i,j,0},f_{i,j,1},f_{i,j,2}\\})\\] \\[f_{i,j,1}=min(f_{i,j,0},f_{i,j,1})\\] 你可以理解为虽然真实顺序是从后到前跳转, 但我们从前到后正常处理匹配, 最终汇总到 \\(f_{i,j,2}\\) 中, 这个记得最后更新 4、若存在不回到开头的方案, 其操作数为 \\(N-len\\) , 其中 \\(len\\) 为 \\(s,t\\) 串最长公共前缀的长度, 它要在与回到开头的答案 \\((f_{N,M,2}+1)\\) 取 \\(min\\) 。 代码：link 原题卡空间, 还要开滚动数组, 在循环中再设初始值吧…… F 题意： 数轴上三元组 \\((i,j,k)\\) , 满足 \\(i&lt;j&lt;k\\;\\&amp;\\&amp;\\;k-i\\leq D\\) 的为好三元组, 一开始数轴为空, \\(D\\) 为定值, \\(Q\\) 个操作, 每次给你一个数 \\(\\in [1,2e5]\\) , 若未加入过就加入, 已加入过就删除, 在线求当前数轴中好三元组的数量。 思路： 简化问题, 考虑加入一个点时它分别作为三元组的 \\(i,j,k\\) 时的贡献。每个点 \\(x\\) 的加入会影响 \\([max(0,x-D),min(2e5,x+D)]\\) 的范围, 我们对每个点 \\(p\\) 受到来自左边或右边的影响单独计数, 为 \\(l_{p},r_{p}\\) , 这样当 \\(p\\) 作为 \\(i\\) 时的贡献为 \\(\\frac{1}{2}*r_{p}*(r_{p}-1)\\) , 同理 \\(p\\) 作为 \\(k\\) 时的贡献为 \\(\\frac{1}{2}*l_{p}*(l_{p}-1)\\) 。 现在考虑当 \\(p\\) 处于三元组中间时的贡献, 看上去难以下手(我到这儿就想不出来了), 一个切入口是暴力统计 \\[\\sum\\limits_{i=p-D}^{p-1}{[i_{exist}]}*\\sum\\limits_{k=p+1}^{i+D}{[k_{exist}]}\\] (\\(i_{exist}\\)表示 \\(i\\) 点已经在数轴上)这个暴力重复过多, 可以再拆一下贡献, \\(\\sum\\limits_{k=p+1}^{i+D}{[k_{exist}]}\\) 就可以拆为 \\[\\sum\\limits_{k=1}^{i+D}{[k_{exist}]}-\\sum\\limits_{k=1}^{p}{[k_{exist}]}\\] 来做一个简单前缀和, 记 \\(c[l,r]\\) 为 \\([l,r]\\) 内点的个数, 所以贡献为： \\[\\sum\\limits_{i=p-D}^{p-1}{([i_{exist}]*c[1,i+D])}-c[p-D,p-1]*c[1,p]\\] 前面是动态的所以还是得展开, 我们可以再记 \\(v_{i}=c[1,i+D]\\) , 最终贡献： \\[\\sum\\limits_{i=p-D}^{p-1}{([i_{exist}]*v_{i})}-c[p-D,p-1]*c[1,p]\\] 剩下的简单了, 线段树维护 \\(v, c\\) 就行, 之前的 \\(l_{p}, r_{p}\\) 也可以用 \\(c[x-D,x-1],c[x+1,x+D]\\) 换掉了, 注意有关 \\(0\\) 的边界情况还是要特判的……删除同理…… 代码： link 其实小细节挺多, 我的实现略繁琐， 用了 \\(v1, v2\\) 两个数组来处理 \\([i_{exist}]\\) , \\(v1\\) 为 \\(v\\) , \\(v\\) 有两次更新, 在单点更新 \\(c\\) 时把之前 \\(v1\\) 积累的更新给 \\(v2\\) 一并加上，向上更新；第二次区间更新考虑单点更新 \\(c\\) 时对 \\([max(1,i-d),2e5]\\) 的 \\(v1,v2\\) 都有影响, 向下更新, 打懒标记什么的, 最后实际询问的是 \\(v2\\) 贡献。删除的更新顺序与上面相反。此外还要注意数轴长度固定为 \\(200000\\)，与 \\(Q,D\\) 无关, 记得开 \\(long\\;long\\) , 这题算一个不错的线段树题。 codeforces round 808 (#Div2) 22/7/16 \\((1661\\rightarrow1621, rk2785)\\) 两个月来最差的一把。 十九分钟过了\\(A,B\\) , 然后罚坐到比赛结束都没能把1600的 \\(C\\) 写出来, 明明只是一个贪心, 现在是7/21的凌晨1：48, 喝了咖啡睡不着, 起来补题, 最近效率很低, 作息也不稳定, 今天能补到哪里算哪吧……现在学OI只有一个目标……cf上1900, 人生的意义就是这点薯条了, 其他我都不想管…… C 题意： 对于数组 \\(a\\) 与初始值 \\(q\\), 按顺序遍历 \\(a_{1}\\) 到 \\(a_{n}\\) ,若 \\(q \\geq a_{i}\\) , 则可以取走 \\(a_{i}\\) 而不付代价, 否则若选择取走 \\(a_{i}\\) 则会付出 1 的代价, 问最多能取走几个物品。 思路： 越晚取越好是肯定的, 但太晚取多余的 \\(q\\) 又会被浪费。 可以推断出不取 \\(a_{i}\\) 的条件：\\(q&lt;a_{i}\\) 且 \\(q&lt;\\sum\\limits_{i}^{N}[q&lt;a_{i}]\\), 这样保证即使现在不花 \\(q\\) 来取, 后面总是能花掉的。 代码：link 这题二分也行, 搞笑的是我考场上两个都想过却都认为自己想假了, 试都没试一下。 D 题意： 求一个 \\(N\\) 位有序数组(每项值域为 \\([1,5e5]\\) , 从小到大排序)的 \\(N-1\\) 阶差分数组(最后的一个数字), 不同的是每次求完差分数组都要对其从小到大排序。 思路： 以为是推式子题, 啥也没推出来。 很有趣的一道题, 看到差分数组想到痛苦的回忆, 再看一眼题解 \"对于值域为 \\(N\\) 的有序数组差分后最多产生 \\(\\sqrt{N}\\) 个不同的数\" , 逐渐痛苦起来了, 又注意到 \\(0\\) 对差分无贡献, 这样本题就暴力可解了, 复杂度 \\(O(N\\sqrt{N})\\)。 代码： 上 \\(map\\) 和 \\(auto\\) 1234567891011N=read(); mp.clear();for(int i=1;i&lt;=N;i++) ar[i]=read(), mp[ar[i]]++;for(int i=1;i&lt;N;i++){ for(auto it=mp.begin();it!=mp.end();it++){ if(it-&gt;second&gt;1) mp2[0]+=(it-&gt;second-1); auto nxt=it; nxt++; if(nxt!=mp.end()) mp2[nxt-&gt;first - it-&gt;first]++; } mp=mp2; mp2.clear();}cout &lt;&lt; mp.begin()-&gt;first &lt;&lt; endl; 注意调用说明存在, 因此 \\(mp2[0]\\) 仅仅会在 it-&gt;second 大于 1 时被调用, 不能写成 max(0,it-&gt;second-1) 的形式。 这题在 codechef 上有一个困难版本, 应该只能用 \\(n log n\\) 做法了, 给个link自己看…… E 题意： 给你错误的求最小生成树的 \\(Prim\\) 代码(贪心部分只寻找了当前点连接的最小边, 生成从每个点出发的 DFS 搜索树), 给你图, 保证有唯一正确的最小生成树, 问从每个节点运行错误代码得到的是不是最小生成树。 思路： 先把正确的最小生成树求出来, 观察图像： 随手做的……红线为 MST 边, 在这个例子中, 从 1, 2, 8 节点开始的错误算法会选择 4-5 边导致最后生成错误的树。 我们设没选的边类型分为两种, 返祖边与横叉边。让我们从 8 号节点开始举例, 按照错误的算法, 先连 8-1, 再连 1-2, 此时 2-8 为一条返祖边, 连接选择过的 8 号节点, 所以返祖边不会被选择。继续模拟, 连 2-5, 此时按照错误算法应连接 5-4，5-4 便为一条横叉边, 其两个端点位于 2 号节点的不同子树中, 因为 4 号节点未被选择过, 按照 DFS 的顺序, 正确的 2-4 边不可能被选择, 要么选 5-4 边, 要么选择另一条(如果存在) x-4 的横叉边, 所以横叉边不能存在。 问题转化为计算以每个点为根时是否存在横叉边, 考虑每条非 MST 上的边 u-v, 若 lca(u,v)!=u/v, 则 u, v 子树外的点全部不合法(先到 u/v, 再根据 DFS 走 u-v 边), 若 lca(u,v)=u/v, 则从 u 到 v 链上的点的子树全部不合法(除u, v两点), 如图： 从 3 号节点开始走不合法。 最后用树上差分的方式给不合法部分打上标记, 统计输出。 具体讲一下这里的差分策略, 对于 lca!=u/v 情况, 在 u, v, lca 上打上 +1, +1, -1 标记; 对于 lca=u/v 情况, 在 \\(u\\;(dep_{u} \\geq dep_{v})\\) 上及满足 \\(fa[t][0]=v\\) 的 t 点上打上 +1, -1 标记, 最后 dfs 统计的时候每个点的值等于该点到根节点链上的差分值之和, 若值为 0 则说明该点合法。 代码：link 事实上我可以边读边边跑 \\(kruskal\\) 的……就不用 \\(f1\\), \\(f2\\) 了。 codeforces round 809 (#Div2) 22/7/18 \\((1621\\rightarrow1694, rk733)\\) D1 糊过的, 现在自己代码看不懂了...... D2 题意： 给定数组 \\(a\\) , 长度为 \\(N\\), 定义 \\(a\\) 的价值为 \\[\\underset{1\\leq i \\leq N}{max}(\\lfloor\\frac{a_{i}}{p_{i}}\\rfloor)-\\underset{1\\leq i \\leq N}{min}(\\lfloor\\frac{a_{i}}{p_{i}}\\rfloor)\\] 其中 \\(1 \\leq p_{i} \\leq k\\) , 给定 \\(k\\) , 求最小的价值, \\(1 \\leq N, a_{i}, k \\leq 1e5\\), 空间 \\(64M\\) 。 思路： 有一点整除分块思想。 D1还可以简单枚举 \\(\\lfloor \\frac{a_{i}}{p_{i}}\\rfloor\\) , 但现在不行了, 算一下当 \\(k\\) 足够大时, 设 \\(b_{i}=\\lfloor \\frac{a_{i}}{p_{i}}\\rfloor\\) 最多有 \\(2 \\sqrt a_{i}-1\\) 种取值, \\(b_{i}\\) 不可能全部存下来, 尝试在双指针扫的同时动态求 \\(b_{i}\\), 用 \\(cnt_{i}\\) 表示区间内一个 \\(i(a_{i})\\) 对应的合法的 \\(b_{i}\\) 数量, 当 \\(tot=\\sum[cnt_{i}\\geq 1]=N\\) 时更新最小区间长度。 在具体实现上, 由于 \\(p_{i}\\) 从小到大, \\(b_{i}\\) 从大到小, 从可能的最大值开始双指针, 分开给左右指针开 \\(vector\\) , 处理一个位置后清空它的内存。 代码： 12345678910111213141516171819202122232425262728N=read(); K=read(); M=0;for(int i=1;i&lt;=N;i++) ar[i]=read(), cnt[i]=0, M=max(M,ar[i]);N=unique(ar+1,ar+1+N)-ar-1; //去重for(int i=0;i&lt;=M;i++) vector&lt;int&gt;().swap(qr[i]);for(int i=1;i&lt;=N;i++){ pl[i]=1; ql[ar[i]].push_back(i); //在对应的值上挂上 i , 这里初始 ai/1 = ai, 在 ai 上挂 i, 后面也好分辨 pr[i]=1; qr[ar[i]].push_back(i); //右端点同理 }tot=0; ans=inf;for(int r=M,l=M;l&gt;=0;l--){ for(auto i=ql[l].begin();i!=ql[l].end();i++){ if(!cnt[*i]++) tot++; //cnt, tot一起加, 注意顺序 pl[*i]=ar[*i]/(ar[*i]/pl[*i])+1; //根据整除分块, 去往下一段的左端点注意 +1 if(pl[*i]&lt;=min(ar[*i],K)) ql[ar[*i]/pl[*i]].push_back(*i); //为甚么 pl[i] &lt;= ar[i], 因为 ar[i]/(ar[i]/pl[i]) 需要定义 ? } vector&lt;int&gt;().swap(ql[l]); //处理完了满足 ai/pi=l 的几个端点 while(tot==N &amp;&amp; r&gt;=l) {//若出现满足条件的区间, 尝试缩小它 ans=min(r-l,ans); for(auto i=qr[r].begin();i!=qr[r].end();i++){ //考虑右端点同理 if(!--cnt[*i]) tot--; pr[*i]=ar[*i]/(ar[*i]/pr[*i])+1; if(pr[*i]&lt;=min(ar[*i],K)) qr[ar[*i]/pr[*i]].push_back(*i); } vector&lt;int&gt;().swap(qr[r--]); //先清空, 再缩 } }cout &lt;&lt; ans &lt;&lt; endl; 有一些要注意的点： 1、在这里 \\(p_{i} \\leq a_{i}\\) , 因为我们不需要 \\(p_{i} &gt; a_{i}\\) 时 \\(b_{i}=0\\) 的情况, 这样会在整除分块时越界, 而且所有 \\(b_{i}=0\\) 的情况都可以用 \\(b_{i}=1\\) 替代。 2、循环改成 $ for(auto i:ql[l])$ 会简单不少……现在只是让你看一下挂 \\(i\\) 的好处。 3、\\(vector\\) 释放内存用 vector().swap(xxx), 开头的释放不可避免, 因为右指针没有缩到底。 E 题意： 无向图,给 \\(n\\) 个点, \\(m\\) 条边并标号, \\(q\\) 个询问, 每次询问一个最小的 \\(k\\) , 满足标号在 \\([l,r]\\) 之间的点仅使用前 \\(k\\) 条边便两两联通, \\(n \\leq 1e5, \\;m, q \\leq 2e5\\)。 思路： 看到 \\(q \\leq 2e5\\) ,思考可复用性, 考虑到记 \\(c_{i}\\) 为 \\(i\\) 点到 \\(i+1\\) 点经过的边的最大编号, 建线段树, 每次静态查询 \\(c_{l}\\) 到 \\(c_{r}\\) 之间的最大值, 问题转化为求出 \\(c_{i}\\)。 如果你知道 \\(kruskal\\)重构树, 那么你赢麻了,因为你知道： 原图中两个点之间的所有简单路径上最大边权的最小值 = 最小生成树上两个点之间的简单路径上的最大值 = \\(Kruskal\\)重构树上两点之间的 LCA 的权值。 但我输麻了, 我只会最小生成树, 所以我证明一下结论的前半部分： 按照 \\(kruskal\\) 算法的加边顺序, 如果有一条边 u-v 不在 MST 上, 说明先前的贪心已经使用了边权更小的边连接上了 u-v, 所以结论成立。 代码：link 简单线段树 + LCA + kruskal...... Educational codeforces round 132 (#Div2) 22/7/21 $(1694, rk3029) $ 又寄回来了, 我都不想写这个, 搞得很尴尬, 这场是最寄的, C &gt;&gt; D, 而我 D 又因为一个 max/min 没加写挂了, 就下了大分, 又成为 1600 守门员...... 因为博客太长, 后台的高亮显示已经出问题了, 决定每 10 场开一篇博客, 然后总体记录总结。 C 题意：给你括号串, 问号为通配符, 保证存在合法, 问是否唯一。 思路：注意到这是个贪心就好写了, 因为保证存在合法, 可以先把所有的 \\((\\) 填掉, 再填 \\()\\), 除此之外最有可能合法的便是交换最后一个 \\((\\) 与第一个 \\()\\) , 只要检查一下这个就可以了。 代码：link E D是裸的线段树就不放了…… 题意：给一颗树, 顶点有值($ 1a_{i} ^{30}$), 为了满足树上所有简单路径的异或和不为零, 你需要至少修改几个顶点(可改为任意正整数)？顶点数 \\(\\leq 2e5\\)。 思路： 没有思路, 贺题解, 启发式合并。 一开始没有注意到可修改为任意正整数, 想了个寂寞。 注意到如果一条简单路径不合法, 改变 lca 处一定不劣于改变其他处, 因为改变的值可以任意构造, 改变 lca 可以保证切断所有子树对其他部分的影响。这样, 记 \\(dis_{i}\\) 为 \\(i\\) 点到根节点的异或和, 所以 \\(u\\rightarrow v\\) 的简单路径和为 $ dis_{u}dis_{v} a_{lca}$ , 根据异或的性质, 对于每个满足 \\(x=lca(u,v)\\) 的 \\(x\\) 点, 若 \\(dis_{u} \\bigoplus dis_{v}=a_{x}\\) 则必须要修改 \\(x\\) 点。 具体实现方面：dsu on tree, 每一个节点开一个 \\(set\\) 记录这个节点子树的(含自身) \\(dis\\), 若一个节点被改变, 则其 \\(set\\) 为空(切断连接), \\(dfs\\) 一个节点的时候把它的重儿子直接换上来, 对于轻儿子, 先查询再尝试合并。 代码：(省略了求 \\(dis\\) 的过程) 12345678910111213141516void dfs2(int x, int f){ if(son[x]) dfs2(son[x],x), swap(s[x],s[son[x]]); int tag=0; //choose(delete) or not if(s[x].find(dis[x]^ar[x])!=s[x].end()) tag=1; // detect dis[x] illegal to insert s[x].insert(dis[x]); //insert for(int i=head[x];i;i=e[i].nxt) { int y=e[i].to; if(y==f||y==son[x]) continue; dfs2(y,x); for(auto i:s[y]) if(s[x].find(ar[x]^i)!=s[x].end()) tag=1; //find s[son[x]] ^ s[y] = ar[x] for(auto i:s[y]) s[x].insert(i); //combine } if(tag) s[x].clear(), ans++;} s.find() 可以找元素, 找不到返回 end() 的迭代器。 再分享一个 \\(\\color{red}{wsyear}\\) 的代码, 惊为天人的简洁。 codeforces round 810 (#Div2) 22/7/24 \\((1639\\rightarrow1629, rk1814)\\) 已经无处可掉了, 手速场没手速, 罚时多 D 又不会…… D 题意： 下 \\(N\\) 天雨, 每天下雨在数轴上 \\(x_{i}\\) 处, 强度为 \\(p_{i}\\), 位置 \\(x_{j}\\) 会受到 \\(max(0,p_{i}-|x_{i}-x_{j}|)\\) 的降雨, 当任意时间任意位置降雨量大于 \\(M\\) 时发生洪水, 问对于每一天, 取消当日降雨量会不会导致洪水。 思路：见等差数列想差分, 下面是一个 \\(x_{i}=5\\) , \\(p_{i}=4\\)​ 的二阶差分表格 0 1 2 3 4 3 2 1 0 0 0 1 1 1 1 -1 -1 -1 -1 0 0 1 0 0 0 -2 0 0 0 1 可以做二阶前缀和回到原数组, 现在看去掉一段 \\((x_{i},p_{i})\\) 的降雨对在总降雨量为 \\(val\\) 的 \\(x_{j}\\) 点有什么影响: 1、设 \\(x_{j}\\geq x_{i}\\) ,此时 \\(x_{j}\\) 点降雨量为: \\[val-(p_{i}-(x_{j}-x_{i})) \\iff val-p_{i}+x_{j}-x_{i}\\] 所以若不发生洪水需要满足: \\[val+x_{j}-M \\leq p_{i}+x_{i}\\] 2、设 \\(x_{j} &lt; x_{i}\\) , 同理有 \\(val-M-x_{j} \\leq p_{i}-x_{i}\\) 接下来我们只需求解最大的 \\(val+x_{j}-M\\) 与 \\(val-M-x_{j}\\) 即可。 具体来说, 我们不用关心 \\(x_{j}\\) 和 \\(x_{i}\\) 的大小关系, 因为原式计算是带绝对值的, 不影响结果。又因为若 \\(val \\leq M\\) , 则当点去掉降雨无影响, 需要省去。 代码： link 本题用 \\(vector\\) 就行了, 当然若你用扫描线/差分+线段树写 div2D, 也是可以的…… E 数位DP, 咕 codeTON Round 2 (#Div1+2) 22/7/31 \\((1629\\rightarrow1651, rk2075)\\) 应该是打的第一场 div1+2，因为 D 的一个构造想不出来罚坐近 2h, 又成了 ABC 战神…… D 题意： 给你一个数组 \\(b\\) , 长度为 \\(m\\) , 在此基础上生成 \\(n\\) 个数组, \\(c_{1}...c_{n}\\) , 一开始的 \\(c_{i}=b\\) , 有一个特殊数组 \\(c_{k}\\) ; 生成数组的规则是: 1、选择 \\(i, j\\), 满足 \\(2 \\leq i &lt; j \\leq m-1\\) \\[t \\neq k, c_{t}[i-1]+1, c_{t}[i]-1, c_{t}[j]-1, c_{t}[j+1]+1\\] 2、选择 \\(i, j\\), 满足 \\(2 \\leq i &lt; j \\leq m-2\\) \\[t = k, c_{t}[i-1]+1, c_{t}[i]-1, c_{t}[j]-1, c_{t}[j+2]+1\\] 每个 \\(c_{i}\\) 至少完成一次上述操作, 给你最后的 \\(c_{1}...c_{n}\\) , 问你 \\(k\\) , 以及 \\(c_{k}\\) 完成了多少次操作？ 思路： 本题看似唬人, 实则唬人, 设 \\(pre\\) 为一个 \\(c_{i}\\) 的前缀和数组： 对于操作一, \\(pre[i-1]+1, pre[j]-1\\) 对于操作二, \\(pre[i-1]+1, pre[j]-1, pre[j+1]-1\\) 于是做完了。 代码：link E 题意： 有向无环图(\\(n,m\\leq1000\\)), 每个节点有正值, 恰好有一个没有出边的节点, 每秒钟每个节点的值 -1 , 并将所通向的节点的值 +1 , 找到所有的节点的值为 0 的第一个时刻, 答案模 998244353 输出。 思路： 类似拓扑排序, 但要注意在节点变为 0 后是有重新获值的可能性的, 比如说本身为 0 , 但几代前的父节点有值, 考虑起来十分复杂, 于是注意到 \\(n \\leq 1000\\) , 只要暴力完成前 \\(n\\) 轮, 再拓扑最后一轮累加, 就可以了。 代码：link F SG函数, 咕咕咕咕咕 Educational codeforces round 133 (#Div2) 22/8/4 \\((1651\\rightarrow1749, rk476)\\) 坚持不看榜人 70 mins 才出 C, 以为又要寄了, 但这把 CD 都难……, ABC 战神反而能上分…… D 题意： 给你 \\(n, k\\), 从数轴原点开始, 第 \\(i\\) 次可以跳 \\((i+k-1)\\) 的倍数步(不可以跳 0 步), 问你跳到 \\([1,n]\\) 格各有多少种方案。 \\(n,k \\leq 2e5\\), 方案数模 998244353。 思路： 完全背包问题稍微改了改, 最多有 \\(m\\) 种物品(步数与其倍数), 满足 \\((2* k+m-1)* m \\leq 2* n\\) (等差数列求和), 注意每种\"物品\"至少要取一种, 可以从之前的转移……， 复杂度 \\(O(n\\sqrt n)\\)。 代码： 12345678910f[0]=1;for(int i=1;(2*K+i-1)*i&lt;=2*N;i++){ int t=i+K-1; for(int j=t;j&lt;=N;j++) f[j]=(f[j]+f[j-t])%p; for(int j=N;j&gt;=t;j--) f[j]=f[j-t]; for(int j=t-1;j&gt;=0;j--) f[j]=0; for(int j=t;j&lt;=N;j++) ans[j]=(ans[j]+f[j])%p;}for(int i=1;i&lt;=N;i++) cout &lt;&lt; ans[i] &lt;&lt; &quot; &quot;; cout &lt;&lt; endl; 好像不难啊……上赛场就摆烂…… E 题意： 给你 \\(2^n\\) 长的数组 \\(a\\) , \\(q\\) 组操作, 每次操作给一个 \\(k\\) , 数组从 \\(1\\) 遍历到 \\(2^n-2^k\\) , 在没有交换过的前提下交换 \\(a_{i}\\) 与 \\(a_{i+2^k}\\) , 每次操作结束后问你最大字串和(包括空字串)。 \\(n \\leq 18, q \\leq 2e5\\)。 思路： 手玩一组样例, 设 \\(k=1\\) , \\(a_{1}\\) 换 \\(a_{3}\\), \\(a_{2}\\) 换 \\(a_{4}\\), \\(a_{5}\\) 换 \\(a_{7}\\), \\(a_{6}\\) 换 \\(a_{8}\\dots\\)。 我们想到了要找寻一种性质来总体描绘一次操作中的所有交换。因为一次操作内部以 \\(2^k\\) 为单位, 想到下标的二进制表示, 若下标从 \\(0\\) 开始, \\(0, 1, 10, 11, 100, 101, 110, 111\\) , 不难看出每次交换其实是让下标异或 \\(2k\\)。 如何维护最大字串和？ 考虑线段树, \\(2^n\\) 的长度, 一定是漂亮的满二叉树, 每次给的 \\(k\\) 相当于在线段树的倒数 \\(k+1\\) 层交换两个子树。若暴力做这个交换, 每次自底向上维护, 则有 \\(O(4^n)\\) 的复杂度, 肯定是寄的。 题解的方法优雅又暴力, 把节点的所有版本在建树时就保存下来, 最后操作的时候直接异或输出异或输出就可以了。 对于一个长度为 \\(2^m\\) 的线段树节点 \\(v\\) , 有 \\(2^m\\) 种状态, 对应当前 \\(\\sum2^k=[1,2^m]\\) , 其左右儿子同理便有 \\(2^{m-1}\\) 种状态, 我们从 \\(0\\) 到 \\(2^m-1\\) 枚举 \\(i\\) ( \\(0\\) 为初始态, \\(2^{m}-1\\) 为 \\(k\\) 取遍 \\(0,1,2,\\dots, m-1\\) 的“最乱状态”), 有如下转移(设 \\(t(v,x)\\) 为 \\(v\\) 节点的第 \\(x\\) 种状态)： 1、 \\(i &lt; 2^{m-1}, \\;t(v,i)=combine(\\;t(ln,i),\\;t(rn,i)\\;)\\) 2、 \\(i \\geq 2^{m-1}, \\;t(v,i)=combine(\\;t(rn,i-(2^{m-1})), \\;t(ln,i-(2^{m-1}))\\;)\\) 这个 \\(combine\\) 就写成结构体构造函数, 维护一些最大前缀和, 后缀和, 字串和, 总值这样的东西…… 因为 \\(i \\in [0,2^{m}-1]\\) , 所以 \\(i \\geq 2^{m-1}\\) 就代表出现了 \\(k=m-1\\) 换掉了 \\(v\\) 的左右儿子。 这种做法复杂度全在建版本上, 版本有 \\(n2^n\\) 个, 查询为 \\(O(1)\\) , 复杂度为 \\(O(n2^n)\\) . 代码： 12345678910111213141516171819202122232425262728293031323334353637struct node{ int v, pre, suf, mx; node(node a, node b){ v=a.v+b.v; pre=max(a.pre,a.v+b.pre); suf=max(b.suf,a.suf+b.v); mx=max(max(a.mx,b.mx),a.suf+b.pre); } node(int x){ v=x; pre=suf=mx=max(ll(0),x); } node() {}};vector&lt;node&gt; t[4*MX];void bt(int n, int s, int e){ t[n].resize(e-s+3); if(s==e) t[n][0]=node(ar[s]); else{ int mid=(s+e)&gt;&gt;1, ln=2*n, rn=2*n+1; //length = 2^k= (e-s+1) = 2*(mid-l+1) bt(ln,s,mid); bt(rn,mid+1,e); for(int i=0;i&lt;=mid-s;i++) {// i ~[0, 2^(k-1)] t[n][i] = node(t[ln][i],t[rn][i]); // &lt; 2^(k-1) t[n][i+mid-s+1] = node(t[rn][i],t[ln][i]); // &gt;= 2^(k-1), elegant, now ln &amp; rn are swaping } }}signed main(){ N=read(); M=(1&lt;&lt;N); for(int i=1;i&lt;=M;i++) ar[i]=read(); bt(1,1,M); Q=read(); int cur=0; for(int i=1;i&lt;=Q;i++){ int K=read(); cur^=(1&lt;&lt;K); cout &lt;&lt; t[1][cur].mx &lt;&lt; '\\n'; } return 0;} 别忘了你的 resize ! F 斯特林数与下降幂……咕咕 Codeforces round 812 (#Div2) 22/8/6 \\((1749\\rightarrow1735, rk1538)\\) ABC 战神, 勇于跳 D 开 E 不跟榜还观察不出来 D 性质的傻子, 罚时与罚坐大师今天又掉分了……(不是三个人, 是一个人) D(交互) 题意： \\(2^n\\)个人打比赛, 赢者晋级输者淘汰, 最多 \\(\\lceil\\frac{1}{3}* 2^{n+1}\\rceil\\) 次询问任意两个人的胜利次数大小关系, 问谁是最后的赢家。 思路： 数学很好的你一眼就注意到 \\(\\lceil\\frac{1}{3}* 2^{n+1}\\rceil=\\lceil\\frac{2}{3}* 2^{n}\\rceil\\) (废话), 又注意到了 \\(2*\\sum\\limits_{i=1}^{\\infty}{\\frac{1}{4^n}}=\\frac{2}{3}\\), 一下子就发现了只要每四人一组, 用两次询问得到四人中的赢家, 向上递归便能轻松解题。 代码：link E 题意： 给一个矩阵, 每次可以选一个 \\(k\\) 并交换所有的 \\(a[i][k], a[k][i]\\), 问可能得到的最小字典序的矩阵。 思路： 显然对于矩阵上的一格 \\(a[i][j]\\), 有唯一对应的 \\(a[j][i]\\) , 选择 \\(i, j\\) 均可完成交换, 若交换, 则两者选其一; 若不交换, 则两者均选或均不选。 考虑种类并查集, 开两倍的并查集, 注意当两者关系已确定后不再更新。 代码：link 不会卡常……警示后人, 用 ‘' 换掉你的 'endl'。 F FMT……, 要咕一辈子了 Codeforces round 813 (#Div2) 22/8/13 \\((1735\\rightarrow1729, rk1599)\\) 这场 135 mins, ABC 做完只用了 20mins 左右, 我以为 D 稳了, 先罚坐了一个小时, 静下心来想和之前的 809E 很像, 都有一个点两两联通的无向图, 也可以按照顺序拆成链, 遂码之, 最后样例测过了, 我交上去, 以为还有一分钟, 但我电脑卡延迟了……实际上比赛已经结束了, 现在是 8/14, 1:28am, 我紧张的等待 system test 结束后想再交一发……我既期冀着打破 ABC 的魔咒, 又有一点不希望自己能成功……看别人写的都是二分, 而我用上了线段树…… 交了一发, D 错了, 去睡觉了…… D 题意： 给你一个数组 \\(a\\) , 可以更改其中的 \\(k\\) 个数, 图上两点 \\(i, j(i&lt;j)\\) 的边权为 \\(min([a_{i}\\dots a_{j}])\\) , 问你能得到的最大树的直径为多少。 思路： 注意到树的直径一定从相邻两点的距离中选出, 假设非相邻两点的距离是直径, 两点之间的最短路只会从 \\(min(a_{i},a_{j}), 2* min([a_{1}\\dots a_{n}])\\) 中取得, 可以观察到走三步以上没有意义, 走两步最小值的构造方法是：看 \\(a\\) 中最小值在起点( \\(a_{i}\\) )的左边或右边, 然后先去左边的 \\(a_{1}\\)(或右边的 \\(a_{n}\\) )再回到 \\(a_{j}\\) 即可。这样看来, 最长的最短路一定在相邻两点间取得。 从 \\(a_{i}\\) 到 \\(a_{i+1}\\) 有三种走法：直接走 i--j, 走 i--1, 1--i+1, 走 i--n, n--i+1, 第二、三种走法要求我们预处理出前缀, 后缀最小值。 现在便有了一个二分做法：二分出 mid , 将前缀后缀 \\(2* a_{i} \\leq mid\\) 的值修改, 看修改的数目与 \\(k\\) 的关系, 复杂度 \\(O(n log n)\\) 代码：link E1 题意： 给你 \\(l, r\\) , 找出满足 \\(lcm(i,j,k) \\geq i+j+k\\) 的三元组 \\((i,j,k)\\) 数目, 其中 \\(l \\leq i&lt;j&lt;k \\leq r\\), \\(l,r \\leq 2e5\\) 最多五组数据。 思路： 正难则反, 思考计算 \\(lcm(i,j,k) &lt; i+j+k\\) 的三元组数目, 注意到 \\(lcm(i,j,k)\\) 为 \\(k\\) 的倍数, 因为 \\(2* k \\leq i+j+k &lt; 3* k\\) , 所以所有不合法的三元组必须满足以下条件： \\[lcm(i,j,k)=k\\;\\;or\\;\\;lcm(i,j,k)=2k, i+j&gt;k\\] 只要枚举 \\(k\\) 的约数, \\(2k\\) 的约数即可。 代码： 1234567891011121314151617ll sum=(ll)(r-l+1)*(r-l)*(r-l-1)/6;int num=0;for(int i=l+2;i&lt;=r;i++){ for(auto x: fac[2*i]){ //k = i if(x&lt;=l) continue; //j &gt; l if(i%x&amp;&amp;2*x&lt;=i) continue; // continue if x is the factor of 2k but x+y&lt;k (2k part) if(x&gt;=i) break; //x = j &lt; k for(auto y: fac[2*i]){ //y = i &lt; j = x &lt;k if(y&lt;l) continue;// i &gt;= l if(y&gt;=x) break; //i &lt; j if(i%x||i%y) // x/y is factor of 2k (2k part) num+= (x+y&gt;i); //can they? else num++; //x/y is factor of k } }}cout &lt;&lt; sum-num &lt;&lt; endl; 用 \\(2k\\) 的约数做 \\(k\\) 的约数, 在其中再特判 \\(2k\\) 的约数。 E2 题意同上, 最多可能有 2e5 组数据。 思路：E1都是贺的我 E2 有屁思路 考虑离线下来做, 同上, 从小到大枚举 \\(k\\) , 将 \\((i,j,k)\\) 中所有有贡献的 \\(i\\) 及其贡献(固定 \\(i,k\\) 后的三元组数目)用树状数组单点修改, 每次枚举完 \\(k\\) 后查看是否有 \\(r=k\\) 的询问, 其答案便为树状数组上 \\(i\\) 到 \\(k\\) 的区间求和。 代码：link F 长链剖分, 咕。 Codeforces round 814 (#Div2) 22/8/16 \\((1729\\rightarrow1774, rk501)\\) ABC 战神糊出来了 D1 , 上了小分。 D2 题意： 给你大小为 \\(N\\) 的数组 \\(a\\) , 每次可以选择 \\(l, r, x\\) 使得 \\([l,r]\\) 范围内的数全部异或 \\(x\\) , 并耗费 \\(\\lceil \\frac{r-l+1}{2}\\rceil\\) 的代价, 问将 \\(a\\) 数组全部置零的最小代价。\\(N \\leq 2e5\\) 。 思路： 因为有上取整的要求, 一次操作范围为 2 的数字是最赚的, 范围再扩大则没有必要, 原先的 D1, \\(N\\) 的范围在 \\(5000\\) 以内, 可以暴力二维 \\(DP\\) , 原先有一个简单的置零方法就是一个一个置零, 代价为 \\(N\\), 我们每两个两个操作就可能省下一个代价, 我的 D1 做法是设 \\(f_{i,j}\\) 为考虑省下 \\(i\\) 处代价, 从头一直处理到 \\(j\\) 处所省下的代价。 \\[f_{i,j}=max(f_{1,i-1},f_{2,i-1},...,f_{i-1,i-1})+[a_{i} \\oplus a_{i+1} \\oplus ... \\oplus a_{j} == 0]\\] 最后取 \\(N-max(f_{1,N},f_{2,N},...,f_{N,N})\\) 为答案, 有点玄学, 我也不知道怎么过的题, 复杂度 \\(O(n^2)\\) 。 事实上我们可以直接设 \\(f_{i}\\) 为将 \\(1...i\\) 所有数置零付出的最小代价, 要么我们操作长度为 1 的数组一次, 要么我们一直操作长度为 2 的数组直到某段区间的异或和为零。 \\[f_{i}=min(f_{i-1}+1, \\underset {a_{j+1}\\oplus a_{j+2} \\oplus...\\oplus a_{i}=0}{min}(f_{j}+i-j-1))\\] 复杂度也是 \\(O(n^2)\\) , 但应该有了些优化空间, 比如把最小的 \\(f_{j}-j\\) 存入 \\(map\\) 中, 事实上, 我们还可以设 \\(xors_{i}=a_{1} \\oplus a_{2} \\oplus ...\\oplus a_{i}\\) , 那么有 \\(a_{j+1}\\oplus a_{j+2} \\oplus...\\oplus a_{i}=0 \\Longleftrightarrow xors_{j}=xors_{i}\\) 这样一个小性质。 代码： 123456789101112131415N=read();for(int i=1;i&lt;=N;i++){ ar[i]=read(); f[i]=inf; xors[i]=xors[i-1]^ar[i];}MII mp;mp[0]=0; //记录前缀和i所对应的最小 f[j]-jfor(int i=1;i&lt;=N;i++){ if(!ar[i]) f[i]=min(f[i], f[i-1]); else f[i]=min(f[i],f[i-1]+1); if(mp.find(xors[i])!=mp.end()) f[i]=min(f[i],mp[xors[i]]+i-1); mp[xors[i]]=min(mp[xors[i]],f[i]-i);}cout &lt;&lt; f[N] &lt;&lt; endl; 弄一下就 \\(O(n\\;log\\;n)\\) 了。 E 题意： \\(k\\) 种字符, 每种 \\(a_{1}, a_{2},...,a_{k}\\) 个, 问是否能用它组成一个字符串 \\(s\\) , 使得：字符串中的每一段连续相同字母个数重新组合后是斐波那契数列的前几项。 思路： 显然若 \\(\\sum a_{i}\\) 不等于斐波那契数列的某个前缀和时可以直接判定不存在。先处理出 \\(fib[i]\\) , 在用 \\(map\\) 处理出 \\(fib[i]\\) 的前缀和, 键值设为 \\(i\\) , 到 90 项就够了。 然后我们知道了最终要构建的斐波那契数列的项数 \\(M\\) , 考虑贪心的从高项往低项填充, 可以拉个优先队列实现, 又因为相邻两项字符不能相同, 每次填完后剩下的字符数量隔一轮再加回去。 代码：link F 22/10/23, 应 \\(\\color{black}{\\tt{c}}\\color{red}{\\tt{hengchunhao}}\\) 推荐来补题, \\(\\color{black}{\\tt{c}}\\color{red}{\\tt{hengchunhao}}\\) 大队长直言道, 1600的诈骗题不会补, \\(\\color{blue}{\\tt{frankly6}}\\) 怕是连面子都不要了。 题意： 一个循环数组(\\(a_{n}\\) 后面接 \\(a_{1}\\)), 自选一个位置 \\(s\\) 开始跳 \\(n\\) 步, 步长 \\(k &lt; n\\) , 每次把那个位置上的数加入到 \"有用值\" 中去。 有 \\(Q\\) 次单点修改, \\(a_{p}=v\\) , 修改不独立, 问这 \\(Q+1\\) 次的 \\(Q+1\\) 个最大有用值。 \\(n,Q \\leq 2e5, a_{i} \\leq 1e9\\) 思路： 只会朴素做法……对 \\(n\\) 的所有不等于 \\(n\\) 的约数 \\(d\\) 作为步长, 枚举 \\(j \\in [0,d-1]\\) 作为起始点, 求 \\(f(d,j)=d* \\sum\\limits_{i \\equiv j\\;mod\\;d}{a_{i}}\\) , 再求出最大的 \\(f\\) , 单次的复杂度应该是 \\(O(n* d(n))\\) 的。 可以手玩小样例来优化, 设 \\(n=12\\), 步长 \\(k=2,3,6\\) : 1.\\(k=2, ans=2 * max(a_{1}+a_{3}+a_{5}+a_{7}+a_{9}+a_{11},a_{2}+a_{4}+a_{6}+a_{8}+a_{10}+a_{12})\\) 2.\\(k=3, ans=3 * max(a_{1}+a_{4}+a_{7}+a_{10},a_{2}+a_{5}+a_{8}+a_{11},a_{3}+a_{6}+a_{9}+a_{12})\\) 3.\\(k=6, ans=6 * max(a_{1}+a_{7},a_{2}+a_{8},a_{3}+a_{9},a_{4}+a_{10},a_{5}+a_{11},a_{6}+a_{12})\\) 可以看出来 \\(k=6\\) 一定优于 \\(k=2, k=3\\) 的答案, 因此若 \\(k_{1}|k_{2}\\) , 则 \\(k_{1}\\) 不需要额外求解, 只有当 \\(k\\) 满足 \\(\\frac{n}{k}\\) 为质数时, 才要求解 \\(k\\) , 否则 \\(\\frac{n}{k}\\) 可以拆出一个因子安在 \\(k\\) 身上, 我们只要考虑所有环长(\\(\\frac{n}{k}\\))为质数的环即可。 代码： 1234567891011121314151617181920212223242526272829T=read();while(T--){ N=read(); Q=read(); for(int i=1;i&lt;=N;i++) ar[i]=read(); int tmp=N; cnt=0; for(int i=2;i&lt;=tmp;i++){ if(tmp%i==0){ d[++cnt]=N/i; // N/d = N/(N/i) = i = PRIME! while(tmp%i==0) tmp/=i; } } multiset&lt;int&gt; s; for(int i=1;i&lt;=cnt;i++){ for(int j=0;j&lt;=d[i]-1;j++) f[j][i]=0; // f[j][i] = f(cnt[i],j), cnt[i] is the length of a step. for(int j=1;j&lt;=N;j++) f[j%d[i]][i]+=ar[j]; //congruence modulo and solve for(int j=0;j&lt;=d[i]-1;j++) s.insert(d[i]*f[j][i]); } cout &lt;&lt; *s.rbegin() &lt;&lt; '\\n'; //rbegin() = end()-1 while(Q--){ int p=read(), x=read(); for(int i=1;i&lt;=cnt;i++){ s.erase(s.find(d[i]*f[p%d[i]][i])); // must use s.find() or it will delete all elements f[p%d[i]][i]+=x-ar[p]; s.insert(d[i]*f[p%d[i]][i]); } cout &lt;&lt; *s.rbegin() &lt;&lt; '\\n'; ar[p]=x; }} Codeforces round 816 (#Div2) 22/8/20 \\((1774\\rightarrow1764, rk1256)\\) 可能会 D , 但 WA on pretest 4 三发, 遂摆之。 D 题意： 给你长度为 \\(N\\) 的数组 \\(a\\) ， \\(Q\\) 个约束条件, 每个约束条件 \\((i,j,x)\\) 表示 \\(a_{i}|a_{j}=x\\) , 要你构造一个满足 \\(Q\\) 个约束条件的字典序最小的数组, \\(a_{i} &lt; 2^{30}\\)。 思路： 按位把询问的 \\(x\\) 拆开来, 若 \\(x\\) 的某一位为零则 \\(a, b\\) 的对应位全为零, 若 \\(x\\) 的某一位为一则 \\(a, b\\) 的对应位至少有一个一。若 \\(a=b\\) 则那个数字对应 \\(x\\) 。我们可以先初始化 \\(a\\) 的每一位为 \\(2^{30}-1\\) (所有位都为 1 ), 用 vector 把询问都挂上去, 从前往后, 从高位到低位尝试置零。 代码： 1234567891011121314151617181920212223242526N=read(); Q=read();for(int i=1;i&lt;=N;i++) ar[i]=(1&lt;&lt;30)-1;for(int i=1;i&lt;=Q;i++){ int a=read(), b=read(), val=read(); if(a==b) {ar[a]=val; vis[a]=1; continue;} for(int k=29;k&gt;=0;k--){ if((val&gt;&gt;k)&amp;1) continue; if((ar[a]&gt;&gt;k)&amp;1) ar[a]^=(1&lt;&lt;k); if((ar[b]&gt;&gt;k)&amp;1) ar[b]^=(1&lt;&lt;k); } v[a].push_back({b,val}); v[b].push_back({a,val});}for(int i=1;i&lt;=N;i++){ if(vis[i]) continue; for(int j=29;j&gt;=0;j--){ if(!((ar[i]&gt;&gt;j)&amp;1)) continue; bool tag=1; for(auto k:v[i]){ int b=k.first, val=k.second; if(((val&gt;&gt;j)&amp;1)&amp;&amp;!((ar[b]&gt;&gt;j)&amp;1)) {tag=0; break;} } if(tag) ar[i]^=(1&lt;&lt;j); }}for(int i=1;i&lt;=N;i++) cout &lt;&lt; ar[i] &lt;&lt; &quot; &quot;; cout &lt;&lt; endl; E (于22/11/2日补题……) 斜率优化DP 题意： 有 \\(n\\) 座城市, 城市间有 \\(m\\) 条双向道路, 通过第 \\(i\\) 条道路需要花费 \\(w_{i}\\) 时间, 两两城市(u,v)之间还可以坐飞机来通行, 花费为 \\((u-v)^2\\) , 你最多可以做 \\(k\\) 次飞机, 问 \\(1\\) 号城市到所有城市的最短时间为多少。 \\(n, m \\leq 1e5, k \\leq 20, w_{i} \\leq 1e9\\) 思路： \\(dijkstra\\) 是肯定要做的, 看到 \\(k\\) 这么小, 可以想到分层图。 设 \\(f[i][k]\\) 表示乘坐 \\(\\leq k\\) 次航班从 \\(1\\) 点到 \\(i\\) 点且最后一次航班直接到达 \\(i\\) 点的最短时间, \\(dis[i][k]\\) 表示乘坐 \\(\\leq k\\) 次航班从 \\(1\\) 点到 \\(i\\) 点的最短时间。我们先跑一遍 \\(dijkstra\\) 求出 \\(dis[i][0]\\) , 接下来考虑转移, 有： \\[f[i][k]=min\\{dis[j][k-1]+(i-j)^2\\}, j\\in [1,n]\\] 这个复杂度是 \\(O(n^2)\\) 的, 我们拆一下式子： \\[f[i][k]=dis[j][k-1]+i^2+j^2-2*i*j\\] \\[\\underline{dis[ j ][ k-1 ] + j^2}_y = \\underline{2*i}_k * \\underline{j}_x + \\underline{f[ i ][ k ] - i^2}_b\\] 一开始看到平方贡献你就能想到斜率优化了, 现在是个典型的斜率优化式子, 控制枚举顺序, 我们可以让 \\(k,x\\) 单增, 因为 \\(b\\) 要最小, 维护下凸壳, 于是我们可以 \\(O(n)\\) 更新所有的 \\(f[i][k]\\) , 然后我们再拿着 \\(f[i][k]\\) 跑 \\(dijkstra\\) 更新 \\(dis[i][k]\\) , 重复 \\(k\\) 次就行了, 复杂度 \\(O(kn \\;log\\;n)\\)。 代码：link F 咕咕, 咕咕咕！！ Educational codeforces round 134 (#Div2) 22/8/27 \\((1764\\rightarrow1650, rk4729)\\) 大寄特寄, 只会 AB。一个暑假, 除了心态进步, 其余都不曾改变, 上个 edu 场涨的分全掉回来了, 现在凌晨整个人非常清醒......清醒着沉沦......差不多吧。 D 题意： 给你两个数组 \\(a, b\\) , 你要调整 \\(b\\) 数组的顺序, 使得构造出来的 \\(c\\) 数组(\\(c_{i}=a_{i}\\oplus b_{i}\\)) 与起来的和最大 (\\(d=c_{1} \\&amp;c_{2} \\&amp; ...\\&amp;c_{N}\\) 最大) 思路： 显然要从高往低做, 要想 \\(d\\) 的二进制中某一位为 \\(1\\) , 就要判断 \\(a\\) 中 \\(1\\) 的个数是否等于 \\(b\\) 中 \\(0\\) 的个数, 这样我们能把数组拆成 \"a0b1\" 和 \"a1b0\" 两部分, 接下来在两部分内分别递归向下检查就行了。 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041T=read();while(T--){ vector&lt;int&gt; a, b, ord; vector&lt;pair&lt;vector&lt;int&gt;, vector&lt;int&gt;&gt;&gt; t; N=read(); for(int i=1;i&lt;=N;i++) a.push_back(read()); for(int i=1;i&lt;=N;i++) b.push_back(read()); for(int i=0;i&lt;N;i++) ord.push_back(i); //equals to &quot;iota(ord.begin(), ord.end(), 0);&quot; (c++11) t.emplace_back(ord,ord); int ans=0; for(int i=29;i&gt;=0;i--){ bool tag=0; for(auto tmp:t){ auto va=tmp.first, vb=tmp.second; int cnta=0, cntb=0; for(auto x:va) if(a[x]&gt;&gt;i&amp;1) cnta++; for(auto y:vb) if(b[y]&gt;&gt;i&amp;1) cntb++; if(cnta+cntb!=va.size()) {tag=1; break;} } if(tag) continue; ans |= 1&lt;&lt;i; vector&lt;pair&lt;vector&lt;int&gt;, vector&lt;int&gt;&gt;&gt; t2; for(auto tmp:t){ auto va=tmp.first, vb=tmp.second; vector&lt;int&gt; a1, a0, b1, b0; for(auto x:va){ if(a[x]&gt;&gt;i&amp;1) a1.push_back(x); else a0.push_back(x); } for(auto y:vb){ if(b[y]&gt;&gt;i&amp;1) b1.push_back(y); else b0.push_back(y); } if(a0.size()) t2.emplace_back(a0,b1); if(a1.size()) t2.emplace_back(a1,b0); } swap(t,t2); } cout &lt;&lt; ans &lt;&lt; endl;}return (0-0); 写得很精妙……是的, auto 拆开来写的……那玩意好像是 c++17 的结构化绑定……devc++不支持……以及我也不知道代码中的 \\(push\\_back\\) 为什么会报错, 换成 \\(emplace\\_back\\) 竟然就没事了…… E 持久化 KMP , 咕 F 题意：太长机翻 思路： 二分图中, 最大匹配 = 最小点覆盖 = 总点数 - 最大独立集。 于是跑完 Dinic 后求最大独立集的补集, 在残留网络上跑 dfs , 选择左边点集没取到的和右边点集取到的(这一步是选出补集), 再存一下边就做完了。口胡一下正确性, 左边点集取到了说明没有被匹配过, 一定在最大独立集里, 而为了 \"独立\" , 右边就要取取到过的点了, 因为我们还要取补集, 便得到了上述的结论。 代码：link Codeforces round 818 (#Div2) 22/9/2 \\((1650\\rightarrow1701, rk773)\\) 八发罚时可谓是人麻了, 甚至交上去一发带 freopen 的, C 糊过的, D 抄的自己的逆元板子, 我对这场的评价是有思维性, 因为 A 就卡了十分钟…… E 题意： 给你 \\(N\\) , 让你计算 \\(\\sum lcm(c,gcd(a,b))\\) , 对于所有满足 \\(a+b+c=N\\) 的三元组, 所有数都是正整数。 思路： 小清新数论题, 找一个刁钻的角度枚举, 一眼瞪出枚举 \\(gcd(a,b)\\) , 然后不会力。 设 \\(gcd(a,b)=t\\) , 原式转化为 \\(\\sum lcm(c,t) \\sum_{a+b=N-c} [gcd(a,b)=t]\\) 看上去是无用的转换, 问题是如何计算 \\([gcd(a,b)=t]\\) 的个数, 但聪明的你两眼瞪出 \\(gcd(a,b)=t \\Leftrightarrow gcd(\\frac{a}{t}, \\frac{b}{t})=1\\) , 又想到了此时 \\(\\frac{a}{t}+\\frac{b}{t}=\\frac{N-c}{t}\\) , 三眼又瞪出一个重要结论： 若正整数 a+b = c , 则 a 与 b 互质的充要条件是 a 与 c 互质 证明很简单, 若 \\(gcd(a,b)=u&gt;1\\) , 则 \\(u|c\\) , \\(gcd(a,c)&gt;0\\) ; 若 \\(gcd(a,c)=u&gt;0\\) , 则因为 \\(b=c-a\\) , \\(u|b\\) , 矛盾。 所以我们最后的结论是： 满足 \\(gcd(a,b)=t\\) 的式子个数, 即为与 \\(\\frac{N-c}{t}\\) 互质的正整数个数, 即 \\(\\varphi(\\frac{N-c}{t})\\) 我们要求的转化为 \\(\\sum lcm(c,t) \\varphi(\\frac{N-c}{t})\\) , 注意 \\(\\frac{N-c}{t} \\neq 1\\) , 否则找不到正整数 \\(a, b\\)。 代码：link F 题意： 给定长度为 \\(N\\) , 初始数组全为 \\(0\\) 的数组 \\(b\\) , 以及 \\(m\\) 对 \\([u,v]\\), 对于每对 \\([u,v]\\) , 你可以且必须选择 \\(b[u]--, b[v]++\\) 或 \\(b[v]--, b[u]++\\) 一次, 又给定一个 \\(01\\) 数组 \\(s\\) , 一个数组 \\(a\\) , 问是否存在一种选择方案, 使得： \\[\\forall s[i] \\neq 0, b[i]=a[i]\\] 思路： 点对就是点对, 即使它叫做 \\([u,v]\\) 也是点对, 你要把它看成一条边来搞网络流的话, 怎么保证这条边一定被选中呢?(我一开始还想再套个费用流, 寄!) 考虑原问题的等价条件, 一个 +1, 一个 -1, 等价于两个 \\(b_{i}, b_{j}\\) 都 -1, 再选一个 +2, 等价于两个 \\(a_{i}, a_{j}\\) 都 +1, 再选一个 -2, 问题转化为图的匹配, 无向边就没了, 把点对看成点, 往 u, v 各连一条容量为 1 的边, S(源点) 连点对, a 数组(对应的 \\(s[i] \\neq 0\\) 的部分)连 T(汇点), 容量为 \\(a_{i}/2\\) ; 如何保证所有的点对都被用掉？ 我们可以让那些点对在 \\(s[i]=0\\) 的 \\(a[i]\\) 上 \"宣泄\" 流量, 让 \\(s[i]=0\\) 的点连一条 inf 流量的边到中转点, 再从中转点连一条容量为 \\(M-\\sum a_{i}[s[i]==1]\\) 的边到汇点。 然后贺你的 Dinic 模版, 再检查一下流量大小与方向, 就行了。 代码：link 有很多小细节, 总的流量要为 M, \\(s[i]=1\\) 的 \\(a[i]\\) 流量要跑满, \\(a[i]\\) 更新后不为正偶数直接退出。 Codeforces round 819 (#Div1+2) 22/9/6 UNRATED 早上到机房一看, 出题人抄了 F 题(本场 A-H, 135mins)被发现, unrated 了, 乐。 反正我也做不到 F......, 补题还是要补的, 昨晚打的很奇妙, 个人体验 B &gt;&gt; C , 反正 B 我还猜了一个小结论, 用时 30 mins(寄定了), C 只用了 6 mins, 剩下 90 mins 在 DEF 之间来回跳, 0:30 熬不下去了, 3 题含恨离场。 D 题意： \\(N\\) 个点, \\(M\\) 条边, 联通的无向图, 无自环, 重边, \\(M \\leq N+2\\) , 给你图的结构, 对边进行红蓝染色, 设只考虑红色边时的联通分量数为 \\(c_{1}\\) , 只考虑蓝色边时的联通分量数为 \\(c_{2}\\) , 问一种染色方法使得 \\(c_{1}+c_{2}\\) 最小。 思路： 生成树肯定是要求的, dfs 一遍即可, 剩下最多 3 个点(N+2-(N-1)=3) 全部染成另外一种颜色(这里指蓝色吧), 若最小化 \\(c_{1}+c_{2}\\) , 则剩下的 3 个点不能成环, 我们要选择一条环上的边, 染成红色, 环转移到了生成树上, 我们再把这条边的一个端点连接的其他所有边染成蓝色, 这样我们保留了生成树的同时也拆掉了环。 代码：link E 题意： 对于一个长度为 \\(n\\) 的排列 \\(p\\) , 若当 \\(1 \\leq i \\leq n\\) 时都有 \\(|p_{i}-p_{i}^{-1}| \\leq 1\\) 时我们称它是 \"几乎完美\" 的。(\\(p_{k1}^{-1}=k2\\) 当且仅当 \\(p_{k2}=k1\\)) 给你排列的长度 \\(n\\) (\\(n \\leq 3e5\\)), 求出所有 \"几乎完美\" 的排列, 对 998244353 取模。 思路： 咕, 等我学学排列组合, 不会推这个东西。 Educational codeforces round 136 (#Div2) 22/9/8 \\((1701\\rightarrow1675, rk1841)\\) 只会 ABC, 事实证明, 在 edu round , 所有你不会的 STL 都会攻击你……, 写 C 还被迫学了 multiset , 一看 \\(\\color{black}{j}\\color{red}{iangly}\\) 的代码才发现, 原来只要 priority_queue 啊……, 不过这和我是 ABC 战神没有半点关系, 因为我 D 的 DP 又寄了。 D 题意： 两人对弈(Alice &amp; Bob), 轮流在给定字符串的头或尾处取走一个字符(Alice 先取), 加到自己字符串的开头, 保证字符串长度 \\(N\\) 为偶数, 问在最优情况下, 谁的字符串字典序最小(或是平局)。 思路： 要么 Alice 胜利, 要么平局。 因为最后剩两个字母时 Alice 总可以选取字典序最小的在最前面, 若两字母相同, 则 Alice 之前还是可能有选取更小字典序的机会, 对 Alice 而言最坏是平局。 让我们用 \\(f_{i,j}\\) 表示当字符串为给定字符串 \\([i,j]\\) 的子串时游戏的赢家, \\(f_{i,j}=1\\) 为 Alice 获胜, \\(f_{i,j}=0\\) 时为平局, \\(f_{1,N}\\) 即为答案。 我们从小区间向大区间递推, 先初始化所有的 \\(f_{i,i+1}\\) 。 \\[s[i]=s[i+1] \\;?\\;f_{i,i+1}=0\\;:\\;f_{i,i+1}=1\\] 然后我们向上递推到长度为 \\(4, 6, 8, ......\\) 的区间。每个上级区间可以看出实际上是由三个重复的下级区间构成的, 我们可以设为 \\(a,b,c\\) , 目前我们要递推的区间为 \\(f_{i,j}\\) 1. \\(f_{a}+f_{b}+f_{c}=3, \\; f_{i,j}=1\\) 显然。 2. \\(f_{a}+f_{b}+f_{c}=2\\) 2.1. \\(f_{a}=0 \\; || \\; f_{c}=0, \\; f_{i,j}=1\\) 最左边 / 最右边是平局态, Alice 可以破坏。 2.2. \\(f_{b}=0 \\; \\&amp;\\&amp; \\; s[i] \\neq s[j] , \\; f_{i,j}=1\\) 中间是平衡态, Alice 可以抢两边字典序小的字符的先手。 2.3. \\(f_{b}=0 \\; \\&amp;\\&amp; \\; s[i] = s[j] , \\; f_{i,j}=0\\) Bob 跟 Alice 对偶着下就平局了。 3. \\(f_{a}+f_{b}+f_{c}=1\\) 3.1. \\(f_{a}=0 \\; \\&amp;\\&amp; \\; f_{c}=0, \\; f_{i,j}=0\\) Bob 可以破坏掉中间 Alice 的胜利状态并留下一个平局状态。 3.2. \\(f_{b}=0 \\; \\&amp;\\&amp; \\; s[i]=s[j], \\; f_{i,j}=0\\) 中间平局, 两边无法抢先手, 最终平局。 3.3. \\(f_{b}=0 \\; \\&amp;\\&amp; \\; s[i] \\neq s[j], \\; f_{i,j}=1\\) 中间平局, 两边可以抢先手, 最终 Alice 胜。 4. \\(f_{a}+f_{b}+f_{c}=0\\) 4.1. \\(s[i]=s[j], \\; f_{i,j}=0\\) 中间平局, 两边无法抢先手, 最终平局。 4.2. \\(s[i] \\neq s[j], \\; f_{i,j}=1\\) 中间平局, 两边可以抢先手, 最终 Alice 胜。 看着很难, 事实上写的时候很自然, 评分顶多2000, 至于如何不重不漏的转移, 你补题的时候 WA 6发就知道了, 话说应该我转移重了很多......, 不管了。 代码： link E 题意： 有 \\(N\\) 盘菜, 每盘加入一个红胡椒或黑胡椒获得 \\(a_{i}\\) 或 \\(b_{i}\\) 的美味值, 不能都加/都不加/加入超过一个胡椒。 有 \\(M\\) 家店, 每家卖出一份红胡椒或黑胡椒包含 \\(x_{j}\\) 或 \\(y_{j}\\) 的对应胡椒, 你需要输出分别仅在第 \\(j\\) 家店买得所有胡椒并恰好用完所获得的最大美味值, 若不存在方案输出 \\(-1\\)。 思路： 小清新数论题, \\(exgcd\\), 三分, 还考察了二元一次不定方程的通解, 好题。 对 \\(a_{i}-b_{i}\\) 排序, 获得用 \\(i\\) 个红胡椒与 \\(N-i\\) 个黑胡椒的最大美味值, 记为 \\(v_{i}\\)。 接下来对每对 \\(x_{j}, y_{j}\\) 做 \\(exgcd\\) , 先判断是否有解, 在记录有解情况下红胡椒/黑胡椒最少需要的份数 , 再判断一次是否有解, 然后去做三分, 因为答案 \\(v_{ans}\\) 一定单峰。 代码：link , 注释都在里面了, 不贴了 F 题意： (来自官方题解) 给定 \\(a\\) 数组, 长度为 \\(n\\) , 确定每个 \\(b_{i}\\) 的值, 使得 \\(a_{i}|b_{i}\\) , 所有 \\(b_{i}\\) 不同, 且 \\(\\sum b_{i}\\) 最小。\\(n \\leq 1000, 1 \\leq a_{i} \\leq 10^6\\) 。 思路： (来自官方题解) 根据鸽笼原理, 对于每个 \\(a_{i}\\) , 只要考虑 \\(1*a_{i}, 2*a_{i}, 3*a_{i}, ..., n*a_{i}\\) 这几个 \\(b_{i}\\) 值, 问题可以转化为二分图最优匹配, 跑费用流的复杂度为 \\(O(n^4)\\) , 寄定了, 注意到全职在点上而不在边上, 我们就可以跑传统二分图。 具体来说, 我们的左侧点是从小到大排列的 \\(b_{i}\\) 集合, 右侧点是对应的 \\(a_{i}\\) , 这样我们便可以保证答案的最小性, 然而复杂度还是 \\(O(n^4)\\) (左侧点决定主要复杂度, 太大了)。 一个优化是：不重置 \\(vis\\) 数组, 如果没有找到增广路, 它可以把复杂度优化到 \\(O(M(E+V))\\) , M 为最大匹配的大小, 这样复杂度便为 \\(O(n^3)\\) 。 代码：(来自本人) link G 状压 DP + 数学, 爷不会！ Codeforces round 821 (#Div2) 22/9/19 \\((1675\\rightarrow1735, rk592)\\) 糊出来 D1, 但糊不出 D2, 算了, 能上小分就不错了…… D2 题意： 给定三个数 \\(n, x, y\\) , 其中 \\(5 \\leq n \\leq 5000\\) , 再给定两个长度为 \\(n\\) 的 01数组 \\(a, b\\) 。 你每次可以选择 \\(a\\) 中的两个位置, 让它们都异或 \\(1\\) , 若选择的位置相邻, 代价为 \\(x\\) , 否则代价为 \\(y\\) , 输出 \\(a\\) 数组变为 \\(b\\) 数组的最小代价, 若不存在则输出 \\(-1\\) 。 思路： DP 是显然的, 注意到仅当 \\(a, b\\) 不同的位数是偶数时有解, 两个位置 \\(i, j\\) 操作的代价为 \\(min(y,(j-i)*x)\\) , 当 \\(j-i=1\\) 时代价为 \\(min(2*y,x)\\) , 因为我们可以找一个远一点的中转点进行两次操作来改变 \\(i,j\\) 位置的值 。 当 \\(x \\geq y\\) 时, 优先使用 \\(y\\) 代价的操作, 若不可避免的要使用 \\(x\\) 代价操作, 则取 \\(min(2*y,x)\\)。 当 \\(x &lt; y\\) 时, 两个位置 \\(i, j\\) 操作的代价为 \\(min(y,(j-i)*x)\\) , 此时没有上文的特例了。若在不相邻的一对位置上使用 \\(x\\) 操作一定更劣, 因为拆开来就能覆盖更多位置。所以我们从前往后枚举最后一个位置是否用 \\(x\\) 操作与前一个位置消除。 设 \\(f_{i}\\) 为前 \\(i\\) 个位置最小代价的两倍, 对于 \\(i &gt; 2\\) ： \\[f_{i}=min(f_{i-1}+y,f_{i-2}+2*x*(pos_{i}-pos_{i-1}))\\] 其中 \\(f_{1}=y, f_{2}=min(2*y,2*x*(pos_{2}-pos_{1}))\\) , 可以看出我们巧妙分拆了贡献, 对于奇数项, 不管与不与前一项消除, 身上总是带着一个 \\(y\\) 的, 帮助了偶数项的转移。 代码：link E 题意： 有一个 \\(120*120\\) 大小的棋盘, 左上角位置为 \\((0,0)\\) , 每格上有一个传送带, 初始方向全部朝右, 从 \\(0\\) 时刻开始会发生： 1、每一秒在 \\(0,0\\) 放下一个球。 2、如果 \\((i,j)\\) 存在一个球, 则它会移动到 \\((i,j)\\) 传送带的下一个位置; 如果某个位置存在多个球, 则合并为一个; 如果下一个位置超出棋盘, 则丢弃。 3、每传送一次球, 传送带的方向就会变化; 原来向右变向下, 原来向下变向右。 1e4 次询问, 问第 \\(t(t \\leq 10^{18})\\) 秒时, \\((i,j)\\) 是否有球, codeforces 原题链接里还有个图片流程示例, 就不贴了。 思路： 找规律/递推, 一个重要的性质是, 假设经过某个位置有 \\(n\\) 个球, 则被传到右边的球数为 \\(\\lceil \\frac{n}{2} \\rceil\\) , 被传送到左边的球数为 \\(\\lfloor \\frac{n}{2} \\rfloor\\) , 考虑通过差分前缀和算出第 \\(t\\) 秒是否有球经过, 设 \\(f_{t,x,y}\\) 为到第 \\(t\\) 秒为止时经过 \\((x,y)\\) 点的球的总数, 若 \\(f_{t,x,y} \\neq f_{t-1,x,y}\\) 说明第 \\(t\\) 秒有球经过。 在具体操作中, 去除 \\(t\\) 维, \\(f_{0,0}=max(0,t-x-y+1)\\), \\((f_{i,j}+1)/2 \\rightarrow f_{i,j+1},\\;f_{i,j}/2 \\rightarrow f_{i+1,j}\\) 推一下就行了。 代码：link 不是很理解这个 \\(memset\\) 复杂度 * 1e4 询问量 怎么过的掉…… Codeforces Round 822 (#Div2) 22/9/23 \\((1735\\rightarrow1792, rk475)\\) 构造, 一生之敌……构造 + E题 = 含恨离场。 但还是上分了, 好耶。 E 题意： 初始给定一个 \\(b\\) 数组, 构造一个 \\(n * n\\) 矩阵 \\(a\\) , 保证 \\(n\\) 为质数, 满足以下条件： 1、\\(a_{i,i}=b_{i}, \\;\\forall \\;0 \\leq b_{i} &lt;n\\) 2、\\(a_{r1,c1}+a_{r2,c2}\\not\\equiv a_{r1,c2}+a_{r2,c1} \\; (mod\\;n), \\;\\forall \\;1\\leq r_{1}&lt;r_{2}\\leq n, 1 \\leq c_{1} &lt; c_{2} \\leq n, 0 \\leq a_{i,j} &lt; n\\) 思路： 转化式子, 第二个约束条件等价于： \\[a_{r1,c1}-a_{r1,c2}\\not\\equiv a_{r2,c1}-a_{r2,c2} \\;(mod\\;n)\\] 于是我们只要让每一行为公差不同的等差数列即可, 公差为 \\([0,n-1]\\)。 现在考虑第一个性质, 要满足角标元素, 可以这么构造： \\[a_{i,j}=(i*(j-i+n)+b_{i})\\] 这样子 \\(i\\) 承担了公差的使命, \\(i=j\\) 时 \\(a_{i,j} \\; mod \\; n =b_{i}\\) , 后面加个 \\(n\\) 防止取模取出负数。 代码：link 官方题解更厉害, 直接给出了构造的通用二次多项式……, 这题构造方法极多, 场上一个没想出来有点离谱。 F 题意： 给你一个无限长度的 \\(\\tt{Thue-Morse\\;sequence}\\), 通过如下方式构造, 初始字符串 \\(s = 0\\) , 接下来无限步中, \\(s=s\\;+\\; inv(s)\\) , 其中 \\(inv(s)\\) 为 \\(s\\) 按位取反得到的字符串, \\(\\bf Example: 0 \\;01\\;0110\\;01101001\\;0110100110010110\\;...\\)。 给定两个数 \\(n, m \\leq 1e18\\) , 问 \\(s\\) 长度为 \\(m\\) 的两个子串 \\(s[0,m-1], s[n,n+m-1]\\) 有多少处不同？ 思路： 二进制考虑手玩一些性质, 我将每个数对应它们的二进制下标可以发现：下标中 \\(1\\) 的数目为奇数, 对应数位为 \\(1\\) , 下标中 \\(1\\) 的数目为偶数, 对应数位为 \\(0\\) 。换言之, \\(s[i]=\\_\\_builtin\\_parity(i)\\) , 效率是 \\(O(n)\\) 的, 显然不够, 我们要找一种只带 \\(log\\) 的做法 , 想到这里我就寄了去看题解。 考虑递归推式子, 我们简记 \\(\\_\\_builtin\\_parity(i)=par(i)\\) , 问题是求 \\(f(n,m)=\\sum\\limits_{i=0}^{m-1}{[par(i) \\neq par(n+i)]}\\) , 当 \\(m=0\\) 时, \\(f(n,m)=0\\) , 当 \\(m=1\\) 时, \\(f(n,m)=[par(0) \\neq par(n)]=par(n)\\)。 为了方便讨论, 我们可以固定 \\(m\\) 的奇偶性, 让我们固定 \\(m\\) 为偶数, 因为这样可以方便递归, 在 \\(m\\) 为奇数时拆出最后一项, \\(f(n,m)=f(n,m-1)+[par(m-1) \\neq par(n+m-1)]\\) 就可以了; 接下来我们分类讨论 \\(n\\) 的奇偶性, 若 \\(n\\) 为偶数, 观察 \\(par()\\) 的特性, 因为 \\(par(2k) \\neq par(2k+1)\\) , 同理 \\(par(2k+n) \\neq par(2k+n+1)\\) , 所以有恒等式： \\[[par(2k) \\neq par(n+2k)]=[par(2k+1) \\neq par(n+2k+1)]\\] 成立, 设 \\(2k=i\\) , 发现又变为了 \\([par(i) \\neq par(n+i)]\\) 的形式, 所以有： \\[ \\begin{aligned} f(n,m) &amp;= \\sum\\limits_{i=0}^{m-1}{[par(i) \\neq par(n+i)]}\\\\ &amp;=2 \\sum\\limits_{k=0}^{m/2-1}{[par(2k) \\neq par(n+2k)]}\\\\ &amp;=2 \\sum\\limits_{i=0}^{m/2-1}{[par(i) \\neq par(n/2+i)]}\\\\ &amp;=2* f(n/2,m/2) \\end{aligned} \\] 若 \\(n\\) 为奇数, 同理有恒等式： \\[[par(2k) \\neq par(n+2k)]=[par(2k) = par(n+2k-1)]\\] 注意这里是 \\(-1\\) 而不是 \\(+1\\) , 奇数保证能与它前面的偶数二进制位相反, 与后面的偶数的二进制位关系则是不确定的, 同理还有： \\[[par(2k+1) \\neq par(n+2k+1)]=[par(2k)=par(n+2k+1)]\\] 所以我们可以把 \\(2k, 2k+1\\) 都转为 \\(2k\\) 然后向下递归。 \\[ \\begin{aligned} f(n,m) &amp;= \\sum\\limits_{i=0}^{m-1}{[par(i) \\neq par(n+i)]}\\\\ &amp;= \\sum\\limits_{k=0}^{m/2-1}{\\Big([par(2k) \\neq par(n+2k)]\\;+\\;[par(2k+1) \\neq par(n+2k+1)]}\\Big)\\\\ &amp;= \\sum\\limits_{k=0}^{m/2-1}{\\Big([par(2k) = par(n+2k-1)]\\;+\\;[par(2k) = par(n+2k+1)]}\\Big)\\\\ &amp;= \\sum\\limits_{i=0}^{m/2-1}{\\Big([par(i) = par((n-1)/2+i)]\\;+\\;[par(i) = par((n+1)/2+i)]}\\Big)\\\\ &amp;=m\\;-\\;f((n-1)/2,m/2)\\;-\\;f((n+1)/2,m/2) \\end{aligned} \\] 最后一行因为不等号变等号进行了容斥。 状态数为 \\(O(log\\;n\\;log\\;m)\\) , 进行记忆化搜索即可。 代码： 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;#include&lt;map&gt;#define int long longusing namespace std;typedef pair&lt;int,int&gt; PII;int T, N, M;map&lt;PII,int&gt; mp;int par(int x) {return __builtin_parityll(x);}int f(int n, int m){ if(m==0) return 0; if(m==1) return par(n); if(m%2==1) return f(n,m-1)+(par(m-1)!=par(n+m-1)); if(mp.count(make_pair(n,m))) return mp[make_pair(n,m)]; int ans=0; if(n%2==0) ans=2*f(n/2,m/2); else ans=m-f((n-1)/2,m/2)-f((n+1)/2,m/2); return mp[make_pair(n,m)]=ans;}signed main(){ cin &gt;&gt; T; while(T--){ cin &gt;&gt; N &gt;&gt; M; mp.clear(); cout &lt;&lt; f(N,M) &lt;&lt; '\\n'; } return (0-0);} \\(\\mathcal{ELEGANT}\\) 有好想但难写的正统数位 DP 做法, 可能以后更新。 Codeforces Round 823 (#Div2) 22/9/25 \\((1792\\rightarrow1672, rk4583)\\) 只会 AB, 一把掉干, 但是心态又比上次进步了许多......, B 题就糊了个三分上去就离谱, C 题简单字符串上贪心没调出来, 我想这也是算法竞赛的一部分, 不爽不要玩…… 然而我还得玩, 在度过糟糕的一晚后过来补题了…… C 题意： \\(0\\) 到 \\(9\\) 组成的字符串 \\(s\\) , 你可以随时取出一位数字 \\(d\\) , 再插入 \\(min(d+1,9)\\) 到任意位置, 问你能得到的字典序最小字符串。 思路： 从后往前更新, 因为要把字典序小的甩到前面, 大的放到后面去。动态更新最小值, 将大于最小值的全部甩到后面就行了。 代码： 12345678910111213string s; cin &gt;&gt; s; int cnt[10]={};int x=9;for(int i=s.size()-1;i&gt;=0;i--){ if(s[i]-'0'&lt;=x){ x=s[i]-'0'; cnt[s[i]-'0']++; } else cnt[min(s[i]-'0'+1,9)]++;}for(int i=0;i&lt;=9;i++) cout &lt;&lt; string(cnt[i],'0'+i); // cnt[i] 个 '0'+i 字符 cout &lt;&lt; '\\n'; D 题意： 两个只包含小写英文字母且长度为 \\(n(n \\leq 1e5)\\) 的字符串 \\(s_{1}, s_{2}\\) , 你每次可以交换长度为 \\(k(1 \\leq k \\leq n)\\) 的 \\(s_{1}\\) 的前缀与 \\(s_{2}\\) 的后缀, 问是否能使得 \\(s_{1}=s_{2}\\) ? 思路： 观察每次我们所做的：交换 \\(s_{1}[1...i]\\) 与 \\(s_{2}[n+1-i...n]\\) , \\(s_{1}[i]\\) 与 \\(s_{2}[n+1-i]\\) 不可能处于同一个串中, 且始终对称。 接下来一个可以手玩出的结论是：对于一对 \\(s_{1}[i],s_{2}[n+1-i]\\) 可以在保证它们对称的前提下随意调整它们的位置。一种构造是, 设 \\(op(k)\\) 为一次操作, 交换长度为 \\(k\\) 的 \\(s_{1}\\) 的前缀与 \\(s_{2}\\) 的后缀, 一次 \\(op(k)\\) , 一次 \\(op(k+1)\\) 可以视为一组操作, 在 \\(s_{1}, s_{2}\\) 的前 \\(k+1\\) 位上轮换, 最终我们都能调整字符对到我们想要的位置, 只要从大到小轮换就可以了。 Windows画图好难受 所以, 能使得 \\(s_{1}=s_{2}\\) , 当且仅当对于所有对称字符对\\((a,b)(a \\leq b)\\) ： 1、\\(a \\neq b\\) , \\((a,b)\\) 的出现次数是偶数(\\(s_{1}[i]=s_{2}[n+1-i], \\; s_{1}[n+1-i]=s_{2}[i]\\)) 2、\\(a = b\\) , 仅在 \\(n\\) 为奇数时最多出现一对出现次数为奇数的 \\((a,b)\\) , 可以安插在中间(事实上…… \\(n\\) 不用判奇偶……想想为什么)。 代码：link E 题意： 给你长度为 \\(n(n \\leq 5e5)\\) 的数组 \\(a(a_{i} \\leq 1e6)\\) , 问你有多少段区间 \\([l,r]\\) 满足 \\(min\\{a_{l...r}\\}|max\\{a_{l...r}\\}\\) 。 思路： 值域这么小能不能用一用啊……好像不行……是用来分解质因数的…… 区间的选择有 \\(n^2\\) 种, 区间最大值的选择只有 \\(n\\) 种, 这句话来自 codeforces exercises 1 的第一场比赛的第一篇题解, 现在又用上了。 让我们同理处理 \\(a_{i}\\) 左边/右边 第一位比它 大/小 的数字的下标, 记为 \\(rmax_{i}, rmin_{i}, lmax_{i}, lmin_{i}\\), 其中 \\(lmax_{i}\\) 特殊定义为左边第一位大于等于 \\(a_{i}\\) 的数的下标, 当： \\[lmax_{i} &lt; l \\leq i \\;\\;\\&amp;\\&amp;\\;\\; i \\leq r &lt; rmax_{i} \\;\\;(1)\\] 时, \\(a_{i}=max\\{a_{l...r}\\}\\) , 在 \\((1)\\) 的前提下, 我们枚举 \\(a_{i}\\) 的除数 \\(j\\) 作为最小值, 设左边离 \\(i\\) 最近的最小值为 \\(j_{1}\\) , 要满足： \\[lmax_{i} &lt; l \\leq pos_{j_{1}} \\;\\;\\&amp;\\&amp;\\;\\; pos_{j_{1}} &lt; i \\;\\;\\&amp;\\&amp;\\;\\; i &lt; rmin_{pos_{j_{1}}}\\] 设右边离 \\(i\\) 最近的为 \\(j_{2}\\) , 也同理, \\[pos_{j_{2}} \\leq r &lt; rmax_{i} \\;\\;\\&amp;\\&amp;\\;\\; pos_{j_{2}} \\geq i \\;\\;\\&amp;\\&amp;\\;\\; i &gt; lmin_{pos_{j_{2}}}\\] 最难理解的部分是去除重复统计的区间, 官方题解简直是谜语人附体, 思路是这样的, 枚举 \\(j_{1}\\) 的区间可以包含 \\(j_{2}\\) , 枚举 \\(j_{2}\\) 的区间时不能包含 \\(j_{1}\\) , 如果只有 \\(j_{1}\\) 或只有 \\(j_{2}\\) 照常枚举, 如果 \\(ar_{i}=j_{1}\\) 那么实际上是 \\(ar_{i}=j_{2}, j_{1}=ar_{i}\\) 因为无法满足 \\(lmax_{i} &lt; l \\leq pos_{j_{1}}\\) , 这也是定义 \\(lmax_{i}\\) 时要取等号的原因, 你也可以不在 \\(lmax\\) 上取等号而在 \\(rmax\\) 上取等号, 效果是一样的...... 然后统计区间的个数, 加一加就行了, 复杂度 O(能过), 因为有 5s。 代码：link, 里面也有一些注释……具体实现要枚举因子, 储存 \\(pos\\) 与下一个下标 \\(idx\\) , 单调栈处理 \\(lmax,lmin,rmax,rmin\\) 等等...... F 咕~ Educational Codeforces Round 136 (#Div2) 22/9/29 \\((1672\\rightarrow1578, rk4496)\\) 继续下分, 好耶, specialist again! 早上起来发现自己 B 被 hack 了……讨厌 edu round, 只算题数不算分数导致我掉到了 4000 多名……D 是个二分还做不出来, 因此我们应该 \"Stop learning useless algorithms......\" D 题意： 给定一颗以 \\(1\\) 为根节点的树, 每次操作可以移植一颗子树到根的下部, 问 \\(k\\) 次操作后最大深度的最小值。 思路： 二分这个最大深度, 从树的底部向上更新, 若一段长度大于等于这个最大深度就接到根节点上去, 注意根节点与根节点的子节点不更新。 代码：link E 题意： 给你一个 \\(2* n\\) 的矩阵, 一些格子是 \"脏\" 的, 另一些是干净的, 分别用 \\(1,0\\) 表示。机器人从 \\((1,1)\\) 开始清理脏格子, 每次朝离它最近的脏格子移动, 若在任意时刻离它最近的脏格子有多个就会停机, 问存在机器人不停机且清理完脏格子所需要预先移除的最少的脏格子数量, \\(n \\leq 2e5\\) 。 思路： 显然这题没有这么显然。 后面的格子会隐隐对前面的格子形成控制之势, 如图： 考虑倒推, 设 \\(f_{i,j}\\) 表示机器人从 \\((i,j)\\) 出发需要预先移除的最少的垃圾数量, 进行分类讨论, 设当前在处理 \\(f_{i,j}\\)。 下文的转移建立在它本身条件以及上文条件的基础上： 1、\\(f_{i \\oplus 1, j}=0\\) , 此时最近的就是下一列的脏格子, 本列已经处理完毕, 转移为 \\(f_{i,j}=f_{i+1,j}\\) 。 2、\\(f_{i+1,j}=0\\) , 此时下一列对应行没有脏格子, 我们可以直接过去并移除本列的脏格子, 也可以从另一行绕过去, 转移为 \\(f_{i,j}=min(f_{i+1,j}+1,f_{i+1,j \\oplus 1})\\) 3、\\(f_{i+1,j \\oplus 1}=0\\) , 此时状态是这样的： 可以看到小机器人已经被包围了！我们必须去掉一个脏格子, 同时我们转移到两列后而不是一列后, 因为我们已经规定了下一列的状态, 转移为：\\(f_{i,j}=min(f_{i+2,j}+1,f_{i+2,j \\oplus 1}+1)\\) 。 4、\\(f_{i+1,j \\oplus 1}=1\\) , 此时状态是这样的： 要么移除下面两个, 从上面走, 要么移除上面一个, 从下面走, 转移为：\\(f_{i,j}=min(f_{i+2,j}+2,f_{i+2,j \\oplus 1}+1)\\) 。 代码：link 很短很好懂。 F 于 22.11.18 7:30~15:00 补题, 因为处理字符串的失误调的上头, 中间听完了四五遍 toe 的专, 很好的平复了情绪, 进行一个无关的推荐。 题意： (字符集大小为 12, 即前 12 个英文字母) 有 \\(n \\leq 1000\\) 个单词, 每个单词的相邻字符不相同, 每个单词有一个权值 \\(c \\leq 1e5\\), 每个单词长为 \\(s \\geq 2\\) , 保证 \\(\\sum s \\leq 2000\\)。 你要设计一个长度为十二的数组作为键盘, 使每一位对应字符集的一个字符, 不重不漏。若一个单词所有相邻的两个字母在键盘上也相邻, 你可以得到单词所对应的权值(称为单词能在键盘上被简单输出), 请输出一种获得最大权值和的数组构造方案。 思路： 我们可以把每个单词想象成最多只有十二个节点的无向图, 单词中每两个相邻的字符代表无向图中的一条边。 显然有一些 “图” 的权值我们不可能得到, 例如在图上有环的时候或者这个图有一些点的度 \\(\\geq\\) 3 的时候。 如果一个单词能在键盘上被简单输出, 例如 \"abcb\" , 它对应的无向图为 a-b-c , 实际上是一条链, 这个链在键盘上可能会有两种排列：abc 与 cba。 我们把所有合法的链的两种排列对应的字符串拉去建立AC自动机, 在每个单词末尾加入权值 \\((t[u].v+=val)\\) , 每个节点顺便也加上它对应的 fail 子树上所有节点的权值, 这个在连 fail 边时可以顺便进行, 因为如果一个单词在某个节点完成匹配, 它的 fail 边连接的是这个节点所代表单词的最长后缀, 一定也完成了匹配。 我们的目标是找到一个 \\(a-l\\) 的排列, 我们可以用状压DP的思想来做这个, 设计 \\(f[s][i]\\) 表示当前选取的字符集和状态为 \\(s\\) , 当前处于 trie 图上的 \\(i\\) 号节点时能得到的最大权值和, 转移 \\(f[s][i]\\) 需要在最外层枚举状态 \\(s\\) , 内层枚举当前节点 \\(i\\) , 最内层再枚举当前节点的子节点, 复杂度为 \\(2^{12} * 2^{12} * 12\\) 但因为跑不满 + 4s 时限所以还是可行的。 对于每个状态为 \\(s\\) 的可行节点 \\(u\\), 当我们枚举到它的 \\(j\\) 号儿子时的转移方程为： \\[ f[s \\oplus 2^j][t[u].s[j]]=max(f[s \\oplus 2^j][t[u].s[j]], f[s][u]+t[s \\oplus 2^j].v)\\] 我们处理出来最大值, 然后随便选一个等于最大值的 \\(f[2^{12}-1][i]\\) , 再逆着它回溯就能找到一种可能的键盘布局了。 代码：link 细节很多, 你不妨想想怎么把一条链对应的两个字符串拉出来, 然而我写的这一切 \\(\\color{black}{j}\\color{red}{iangly}\\) 十九分钟就写完了, 而我得用 190 行加上五六个小时。 Codeforces Global Round 22 22/9/30 \\((1578\\rightarrow1531, rk3457)\\) …… 来看题…… C 题意： 一个数组, Alice 和 Bob 轮流拿数, Alice 先手, 最后 Alice 拿到的数之和为偶数则 Alice 赢, 问在双方最优情况下谁赢。 思路： 分类讨论, 设奇数个数为 \\(x\\) , 偶数个数为 \\(y\\) , 则： 1、\\(x \\; mod \\; 4=0\\) , Alice 必胜, 可以保证 Alice 拿到一半的奇数。 2、\\(x \\; mod \\; 4=1, y \\; mod \\; 2=0\\) , Alice 必败, Bob 跟着 Alice 拿, 可以保证 Alice 拿到奇数个奇数。 3、\\(x \\; mod \\; 4=1, y \\; mod \\; 2=1\\) , Alice 可以保证让 Bob 拿到第一个奇数, 从而拿到偶数个奇数。 4、\\(x \\; mod \\; 4=2\\) , Alice 必败, Bob 可以保证 Alice 拿到奇数个奇数。 5、\\(x \\; mod \\; 4=3\\) , Alice 必胜, Alice 先拿一个奇数, 此时转化为上一种情况, Alice 可以保证 Bob 拿到奇数个奇数, 从而自己拿到偶数个奇数。 代码：link D 题意： 对于一个排列 \\(a_{1},a_{2},...,a_{n}\\) , 以及一个整数 \\(k(0 \\leq k \\leq n)\\) , 以如下的方式计算 \\(b\\) 数组: $; i , x=a_{i}, $ 1、\\(x \\leq k\\) , 设置 \\(b_{x}\\) 为满足 \\(1 \\leq j &lt; i \\;\\;\\&amp;\\&amp;\\;\\; a_{j} &gt; k\\) 的最后一个 \\(a_{j} (b_{a_{i}}=a_{j})\\) , 若不存在这样的 \\(j\\), \\(b_{x}=n+1\\) 2、\\(x &gt; k\\) , 设置 \\(b_{x}\\) 为满足 \\(1 \\leq j &lt; i \\;\\;\\&amp;\\&amp;\\;\\; a_{j} \\leq k\\) 的最后一个 \\(a_{j} (b_{a_{i}}=a_{j})\\) , 若不存在这样的 \\(j\\) , \\(b_{x}=0\\) 给定 \\(b\\) 数组, 让你求一个满足条件的 \\(a\\) 数组与 \\(k\\) , 题目保证存在。 思路： 做 permutation 题, 关键是看出它的操作到底想干什么, 不要被下标套娃绕晕了, 例如这题如何求 \\(k\\) ? 观察到它把下标 \\(\\leq k\\) 的数变为 \\(&gt; k\\) 的数, 我们只要统计 \\(b_{i}&gt;i\\) 的数目便可求出 \\(k\\) , 不用跟我开始一样顺着排列黑白染色……。 接下来开始不断手玩样例, 可以发现一个结论: \\(b_{a_{l}}=b_{a_{r}}\\) , \\(\\forall t \\in [l,r], b_{a_{t}}=b_{a_{l}}\\) 。用人话来讲, \\(b\\) 数组中值相同的位置处于 \\(a\\) 数组中连续的一段。 对于样例 7 7 7 3 3 3 的答案是: k=3, a=1 2 3 4 5 6, 在这里 \\(a\\) 中的 1&amp;2 可以互换, 4&amp;5&amp;6 也可以互换, 在连续的一段中仅存在最后的 3 不能移动, 因为它推导了 \\(b\\) 数组剩下的部分。 我们每次显然可以先确定 \\(b\\) 数组中 \\(0\\) 和 \\(n+1\\) 所对应的位置, 我们可以将 \\(b\\) 数组放到一个 \\([0,n+1]\\) 的桶中, 寻找唯一的\"推导数\"再往后推导, 最后构造出整个 \\(a\\) 数组。 代码： 123456789101112131415161718192021N=read(); a.clear();for(int i=0;i&lt;=N+1;i++) v[i].clear();int n1=0, now=0;for(int i=1;i&lt;=N;i++){ br[i]=read(); if(br[i]&gt;i) n1++; v[br[i]].push_back(i);}if(v[N+1].size()) now=N+1;int cnt=1;while(cnt&lt;=N){ cnt+=v[now].size(); for(auto &amp;u:v[now]){ if(v[u].size()) swap(u,v[now].back()); //把“推导数”放到最后 } a.insert(a.end(),v[now].begin(),v[now].end()); now=v[now].back();}cout &lt;&lt; n1 &lt;&lt; '\\n';for(auto u:a) cout &lt;&lt; u &lt;&lt; &quot; &quot;; cout &lt;&lt; '\\n'; E 题意： 给定一个非负整数数组 \\(a\\) , 长度为 \\(n(n \\leq 1e5)\\) , 计算有多少种分割使得分割后的每一段的和构成的数组是一个回文数组。 思路： \\(0\\) 在里面搅屎。 没有 \\(0\\) 很好做, 双指针弄一下, 找出所有对称的分割点组, 每组有选或不选之分, 弄一下就行了。具体而言, 设 \\(f_{i,j}\\) 为子序列 \\([i,j]\\) 中切分的方案数, 假设 \\(x\\) 是最小的满足存在一个 \\([i,j]\\) 的后缀等于 \\(x\\) 的前缀的位置(值可以为 0), 设这个后缀的位置为 \\(y\\) , 则有 \\(f_{i,j}=2 * f_{x,y}\\)。 现在考虑中间有 \\(0\\) 的情况, 上文的假设不变, \\(x\\) 后面第一个非 0 位置为 \\(l\\) , \\(y\\) 前面 第一个非零位置为 \\(r\\) , 我们便可以调整分割的位置, 或者多分割几刀构成几个对称的 \\(0\\) 区间。 设 \\(len=min(l-x,y-r)\\) , 我们可以在中间切 \\([0,len]\\) 刀, 此时的转移为\\[f_{i,j}= \\sum\\limits_{k=0}^{len}{l-x \\choose k}{y-r \\choose k} f_{x,y}\\] 然后我们递归求解这个问题。 代码：link 注意 \\([x,y]\\) 中间全为 \\(0\\) 部分的特判。 F 题意： 交互题, 给你一个 \\(n(n \\leq 1000)\\) 的无向图, 告诉你每个点的度, 你要给每个点染色, 满足： 1、同一个颜色的所有点在同一个联通块内。 2、同一种颜色(\\(c\\))的所有点的度(\\(s_{c}\\))之和小于等于点数的平方(\\(s_{c} \\leq n_{c}^2\\))。 你可以做至多 \\(n\\) 次询问, 每次询问一个点, 若当前询问是针对该点的第 \\(k\\) 次询问, 则回答该点连接的第 \\(k\\) 条边所连接的另一个顶点。 要求你最终给出一种染色方案, 保证存在这样的方案。 思路： 糊了个贪心上去, 只与正解差一个 \\(\\tt{break}\\) , 不知是幸运还是不幸…… 直觉上讲肯定是从度数大的点开始询问, 设当前询问点为 \\(u\\) , 我们以这样的算法来 bfs： 1、询问到的节点 \\(v\\) 未被访问过, 我们把 \\(v\\) 合并进 \\(u\\) 的联通块。 2、询问到的节点 \\(v\\) 被访问过, 我们把 \\(u\\) 合并进 \\(v\\) 的联通块, 并break 下面我们证明这种贪心的正确性, 我们实际上在构建一个森林, 当 \\(u\\) 的所有节点都未被访问过时, 设 \\(u\\) 点的度为 \\(d_{u}\\) , 有 \\(s_{c} \\leq d_{u}* (d_{u}+1) &lt; (d_{u}+1)^2 = n_{c}^2\\) , 满足条件。 当 \\(u\\) 的某个连接点 \\(v\\) 访问过时, \\(v\\) 所在的块设为 \\(c'\\) , 一定满足条件(\\(s_{c'} \\leq n_{c'}^2\\)), 设 \\(v\\) 为 \\(u\\) 上第 \\(i\\) 个1遍历到的点, 将 \\(u\\) 与前 \\(i-1\\) 个点加入这个块, 此时总节点数为 \\(n_{c'}+i\\) , 度数和小于等于 \\(s_{c'}+i* d_{u}\\) , 此时 \\(d_{u} \\leq n_{c'}\\) , 因为我们是从度数大的节点到度数小的节点遍历的, 所以有： \\[s_{new_c}=s_{c'}+i* d_{u} \\leq n_{c'}^2+i* d_{u} \\leq n_{c'}^2+ i* n_{c'} &lt; (n_{c'}+ i)^2= n_{new}\\] 于是做完了, 怎么感觉这个上界松松的……？ 自己看起来与题解只差了一个 \\(\\tt{break}\\) , 事实上没有证明什么正确性, 差的有点远, 但这样都过了五个点总是会让人产生一点错觉…… 代码：link Codeforces Round 824 (#Div2) 22/10/2 \\((1537\\rightarrow1543, rk1898)\\) …… 来看题…… D 为什么考场上摆烂不写, 为什么…… 到学校补题, 不看题解 17mins 写+调就过了, 比我一些 B 题用的时间都短…… 题意： \\(n\\) 张牌(\\(n \\leq 1000\\)), 每张牌有 \\(k\\) 个维度 (\\(k \\leq 20\\)) , 每个维度有三种取值 0/1/2, 一组三张牌被称为\"好的\"当且仅当这三张牌的每一个维度的三个取值都相同或不同, 五张牌被称为一个\"meta-set\"当且仅当里面有两组及以上\"好的\"牌组, 给你 \\(n\\) 张牌, 保证互不相同, 问有多少个 meta-set ? 思路： 分别钦定每个数为 meta-set 中的重复数, 暴力枚举第二个数, \\(map\\) 查第三个数, 复杂度 \\(O(20n^2+n^2\\;log\\;n), n \\leq 1000\\) , 时限四秒只用 139ms, 然后没了。 代码：link E 题意： 在数轴上有 \\(n(n \\leq 1000)\\) 个非负整数点 \\(h_{i}\\) , 以及两个位置 \\(p_{1}, p_{2}\\) , 有对应的两个距离数组 \\(d_{1}, d_{2}\\) , 其中 \\(d_{i,j}=|h_{j}-p_{i}|\\) , \\(d_{1}, d_{2}\\) 内部的顺序被打乱了, 现在给你 \\(d_{1}, d_{2}\\) , 让你还原一个可能的 \\(h\\) 数组以及 \\(p_{1}, p_{2}\\) , 或报告不存在满足性质的 \\(h,p_{1},p_{2}\\) 。 思路： 1h 在机房憋了个假做法, 但可以对正解有部分启示。 我的想法是, 固定 \\(p_{1}=0\\) , 先把 \\(d_{1},d_{2}\\) 取反后的数组加入(\\(n \\to 2n\\)), 再 sort \\(d_{1},d_{2}\\) , 把 \\(d_{1}\\) 扔进 \\(map\\) 里, 因为 \\(\\exists \\; d_{2,i}\\) 满足 \\(d_{2,i}=d_{1,N}\\) 或 \\(d_{2,i}=d_{1,N+1}\\) , 我们枚举这个 \\(i\\) , 保存偏移量, 然后看能不能找到大小为 \\(n\\) 的匹配。 这个贪心的问题是, 由于 \\(d_{1}, d_{2}\\) 各取反复制了一份, 失去了单调性。正解仅复制 \\(d_{1}\\) 的正负存进 \\(map\\) 中, 然后枚举 \\(p_{2}=\\pm d_{1,i} \\pm d_{2,1}\\) 中的 \\(i\\) 和 \\(p_{2}\\) 的四种情况来保证 \\(d_{2,1}\\) 的匹配, 然后贪心, 因为对称性我们只考虑 \\(p_{2} \\leq 0\\) 的情况(一个负的和一个正的匹配, 对应一定有一个正的与负的匹配)：对于 \\(d_{2}\\) 中的一个元素 \\(x\\) , 可能存在的匹配为 \\(|p_{2}-x|\\) 与 \\(|p_{2}+x|\\) , \\(|p_{2}-x| \\geq |p_{2}+x|\\) , 所以我们可以将 \\(d_{2}\\) 从大到小排序并优先选择较大数匹配, 因为若当前不与较大数匹配, 则后续无法匹配。 代码： link 有点抽象, 调了一上午, 寄！ 另一个思路： 考虑对于一个点, 要么到 \\(p_{1}, p_{2}\\) 的距离之和为 \\(f\\) , 要么到 \\(p_{1}, p_{2}\\) 距离的绝对值之差为 \\(f\\) , 我们的目标转为找到这个 \\(f\\) , 因为 \\(\\exists \\; d_{2,i}\\) 满足 \\(d_{2,i}=d_{1,1}\\) , 这两个点在同一个位置上, 我们可以枚举 \\(f\\) 的 \\(2* n\\) 种可能性(\\(d_{1,1}+d_{2,i}\\) 或 \\(|d_{1,1}-d_{2,i}|\\)) , 判断在钦定边长 \\(=f\\) 的情况下存不存在大小为 \\(n\\) 的匹配, dinic 之, 复杂度为 \\(O(n^{2.5})\\) 。 另一个代码： 待补, dinic 之后不会维护捏 F Dytechlab Cup 2022 22/10/7 \\((1543\\rightarrow1549, rk2114)\\) B 题 WA on pretest 4 查了半天查不出来, 我是** 什么手速场, 赞助场是 div1+2 形式的, 一堆橙、红名还在 D 卡着呢…… D 题意： 无向图, \\(n(n \\leq 500)\\) 个点, \\(m(m \\leq 250000)\\) 条边, 有边权, 要从 \\(1\\) 点出发到 \\(n\\) 点, 你可以在出发前进行任意次如下操作： 选择 \\(i\\) 号边, 设其连接 \\(u_{i}, v_{i}\\) , 边权为 \\(w_{i}\\) , 选择 \\(v_{i}\\) 通过任意边连接的另一个顶点 \\(t_{i}\\) (\\(t_{i}\\) 可等于 \\(u_{i}\\)) , 断开 \\(u_{i}, v_{i}\\) 的连接, 连接 \\(u_{i}, t_{i}\\) , 此举将耗费 \\(w_{i}\\) 的时间, 边权不变。 问从 \\(1\\) 点出发到 \\(n\\) 点的最短时间。 思路： \\(n \\leq 500, floyd\\)。 为什么题干不简单的把边权翻倍? 因为一条边可能在图上\"转来转去\", 移到任何地方, 例如, 在 codeforces 上的这个样例中： 答案是 \\(154\\) , 2 &lt;-&gt; 5 之间边权为 22 的边移动了六次, 连接了起点和终点, 最后加上一次走边的代价, 22 * 7 = 154。 因为重边有用, 可以转出去, 存图时只要存图的连通性就好。 一个 \\(key\\; observation\\) 是, 最短路一定可以直接连接 \\(1\\) 与 \\(n\\) 号点, 假设最短路不直接连接首尾, 例如 1 &lt;-&gt; u &lt;-&gt; v &lt;-&gt; N , 我们可以直接选择最短路上的最短边让它两头扩展连接首尾, 现在我们只要处理每一条边是不是最短边就好。 1、若一条边本身就在起点到终点的路径里, 我们可以直接扩展它至首尾。 2、若一条边本身不在起点到终点的路径里, 我们让它探到路径上的一个点, 称为中转点, 再在这个点上收缩形成自环, 再伸展至首尾。 代码： 12345678910int ans=inf;for(int i=1;i&lt;=M;i++){ int u=op[i].u, v=op[i].v, w=op[i].w; ans=min(ans,w*min(d[1][u]+d[v][N]+1,d[1][v]+d[u][N]+1)); //on the final path for(int j=1;j&lt;=N;j++){ //enumerate the transfer point ans=min(ans,w*(d[1][j]+d[j][N]+d[v][j]+2)); // attach, shrink, and extend ans=min(ans,w*(d[1][j]+d[j][N]+d[u][j]+2)); }}cout &lt;&lt; ans &lt;&lt; '\\n'; 若有边 i &lt;-&gt; j, d[i][j]=d[j][i]=1, 然后跑 \\(floyd\\) 。 上面的 +1 是走边的代价, 下面的 +2 是走边的代价加上缩成自环的代价, 好题！ E Codeforces Round 825 (#Div2) 22/10/10 \\((1549\\rightarrow1711, rk224)\\) \\(+162\\) ! 只要你分数够低, 总有可能上大分！ D 的构造想出来了, C2 线段树不会, 在手速场(C1:5500通过 C2:102通过)小寄大赢！ B Wa 了两发, 下次想好再写题, 不然做不出 D 又掉到 1000 名左右去了…… C2 题意： 给你 \\(a\\) 数组, 长度为 \\(n\\) ,定义好数组 \\([a_{l},a_{l+1},a_{l+2},...,a_{r}]\\) 满足 \\(\\forall \\; i, a_{i} \\geq i-l+1, (a_{l} \\geq 1, a_{l+1} \\geq 2, ..., a_{r} \\geq r-l+1)\\)。 \\(Q\\) 次单点修改(\\(a_{p}=v\\))且相互独立, 要求输出在每次修改后整个 \\(a\\) 数组好的子数组的数目。 \\(a_{i},n,Q \\leq 2e5\\) 思路： C1 没有修改, 仅询问一次, 我就固定遍历左端点, 二分能达到的右端点, 提前递推一个辅助数组判断 \\(i\\) 端点开头的线段在哪儿结束即可, 网上更简单的做法是 DP 或直接双指针弄一下。 带修之后双指针不行了, 但还可以用来处理初始答案, 我们注意到每次更新会改变一段连续端点开头的线段, 因为它们的右端点一定是递增的。 设 \\(i\\) 端点开头的线段结束点为 \\(j\\) , 这里定义为线段右端点后面的一个点, 单点更新可能会造成结束点的点值增加, 增长线段长度, 我们还要处理若结束点增长到足够大时以 \\(i\\) 节点为开头的新的线段长度(或新的线段长度与原线段长度的差分), 至于结束点是否真的能增长到足够大我们在统计时再判断好了, 从前往后的起始节点若拥有同一个结束点, 那么它们对结束点的要求一定是越来越低的, 若已知 \\(p,v\\) , 这部分的约束为 \\(max(1,p-v+1)\\) , 再与其他的约束取个 \\(max\\) 即可, 这里求的是增加点值可能影响的线段区间的左端点最小值。 谈远了……要维护新的线段长度我们可以直接上线段树二分, 之前优先处理 \\(a_{i}-i\\) 较大的位置, 线段树二分出结束点 \\(j\\) , 更新 \\(j\\) 点, 求出新的终点(新长度与原长度的差分), 再把 \\(j\\) 点更新回来。对结束点, 长度查分大小都做要一个前缀和。长度查分大小的前缀和可以帮我们统计在 \\(v&gt;a_{p}\\) 时 \\(p\\) 点的增长对左边造成的影响。 至于 \\(v&lt;a_{p}\\) 的情况, 它会对左边以 \\(\\geq p+1\\) 为结束点的线段产生影响, 最多能影响到 \\(max(0,p-v)\\) 位置, 同样求出区间后我们拿结束点前缀和搞一搞就能统计了。 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960bool cmp(int a, int b) {return ar[a]-a&gt;ar[b]-b;}void upd(int n, int s, int e, int pos, int val){ if(s==e&amp;&amp;s==pos) {t[n]=val; return;} else{ int mid=(s+e)&gt;&gt;1, ln=2*n, rn=2*n+1; if(pos&lt;=mid) upd(ln,s,mid,pos,val); else upd(rn,mid+1,e,pos,val); t[n]=min(t[ln],t[rn]); }}int find(int n, int s, int e, int L, int R, int val){ if(s&gt;R||e&lt;L) return N+1; else if(s&gt;=L&amp;&amp;e&lt;=R) { if(t[n]&gt;val) return N+1; if(s==e) return s; } int mid=(s+e)&gt;&gt;1, ln=2*n, rn=2*n+1; if(t[ln]&lt;=val) return find(ln,s,mid,L,R,val); else return find(rn,mid+1,e,L,R,val); }signed main(){ N=read(); for(int i=1;i&lt;=N;i++) ar[i]=read(), id[i]=i; sort(id+1,id+1+N,cmp); //high delta (ar[i]-i) first int ans=0; for(int i=1,j=0,r=1;i&lt;=N;i++){ //pointer i, pointer r, j is for the addition of ar[id[j]] while(j+1&lt;=N &amp;&amp; ar[id[j+1]]-id[j+1]+i-1&gt;=0) {// Solved by delta order, so we can put two pointers on it instead of restart every time j++; //id[j] represent the things now we can reach. upd(1,1,N,id[j],1); } r=find(1,1,N,i,N,0); // r represent the things nearest we can not reach. ~\\Sacred Binary Search/~ R[i]=r; ans+=r-i; if(r&lt;=N) { upd(1,1,N,r,1); //remove the obstacle r nxt[i]=find(1,1,N,i,N,0)-R[i]; // if we remove the obstacle r, the extra distance we can extend is nxt[i]. upd(1,1,N,r,0); //regain the obstacle r } } for(int i=1;i&lt;=N;i++) prenxt[i]=prenxt[i-1]+nxt[i]; //prefix of nxt[] for(int i=1;i&lt;=N;i++) preR[i]=preR[i-1]+R[i]; //prefix of R[] Q=read(); while(Q--){ int pos=read(), val=read(); if(ar[pos]==val) cout &lt;&lt; ans &lt;&lt; '\\n'; else if(ar[pos]&lt;val){ //we will get higher score int RB=upper_bound(R+1,R+pos,pos)-R-1; // R+1 first - 1 = R last int LB=lower_bound(R+1,R+pos,pos)-R; //R first LB=max(LB,max(ll(1),pos-val+1)); // only increasing enough value can let others getover the obstacle. if(LB&gt;RB) cout &lt;&lt; ans &lt;&lt; '\\n'; else cout &lt;&lt; ans+prenxt[RB]-prenxt[LB-1] &lt;&lt; '\\n'; } else if(ar[pos]&gt;val){ //we will get smaller score int LB=upper_bound(R+1,R+pos,pos)-R; //R+1 first = first affected int RB=max(ll(0),pos-val); // last affected if(LB&gt;RB) cout &lt;&lt; ans &lt;&lt; '\\n'; else cout &lt;&lt; ans-(preR[RB]-preR[LB-1])+(RB-LB+1)*pos &lt;&lt; '\\n'; // mind the non-answer part adding and erasing } } return (0-0);} 锐评：肯定是我做过最难的 C 了 E Editorial 都拖着不出, 不想补…… Codeforces Global Round 23 22/10/15 \\((1711\\rightarrow1712, rk1691)\\) 被 D 诈骗力, 应该三十分钟离场去打游戏的, 看起来这场都是妙妙题, 我不会做, 但我大受震撼。 D 题意： 给定一颗 \\(1\\) 为根节点的 \\(n(n \\leq 2e5)\\) 个节点树, 每个节点有一个权值 \\(s_{i}(s_{i} \\leq 1e4)\\) , 一个 \\(k(k \\leq 1e9)\\) 条简单路径的多重集称为合法, 当且仅当满足下面的条件： - 每条路径从根节点开始 - 设 \\(c_{i}\\) 为经过 \\(i\\) 节点的路径数, 若 \\(u, v\\) 拥有共同的祖先, 则 \\(|c_{u}-c_{v}| \\leq 1\\) 给定 \\(k\\) , 问 \\(\\sum c_{i}* s_{i}\\) 的最大值。 思路： 显然能注意到的一点是, 为了让答案更大, 所有的路径都要延伸到叶节点, 即 $ {v son{u}}{c_{v}}= c_{u}$ 。 设节点 \\(u\\) 有 \\(x\\) 个儿子, 所以 \\(\\forall v \\in son_{u}, \\; c_{v}= \\lceil c_{u}/x \\rceil \\;or\\; \\lfloor c_{u}/x \\rfloor\\) , 我们可以将这两种状态设为 \\(f_{v,1}\\) 和 \\(f_{v,0}\\) , 我们求出 \\(u\\) 的每个儿子的两种状态, 让 \\(c_{u} \\; mod \\; x\\) 个取 \\(1\\) 状态, 剩下的取 \\(0\\) 状态, 这个可以通过排序解决, 具体来说, 我们按 \\(f_{v,1}-f_{v,0}\\) 排序, 增量越大越优先。总的思想是树形DP, 这个很好想, 但我不会实现…… 代码：link 我们默认 \\(\\lceil c_{u}/x \\rceil = \\lfloor c_{u}/x \\rfloor + 1\\) , 至于是不是其实无所谓…… E1 又到了奇妙的交互+构造题 题意： 给定一个 \\(n(n \\leq 1e5)\\) , 让你猜一个数 \\(x \\in [1,n]\\) , 至多 \\(82\\) 次猜测, 每次你可以询问 \\([1,n]\\) 中的一个子集, 回复你 \\(x\\) 在不在这个子集中, 但是系统可能说谎, 不过可以保证连续的两次询问中一定有一次系统没有说谎, \\(x\\) 的值可能随着询问而动态改变, 不过一定满足之前的约束。 你一共有两次猜测 \\(x\\) 的机会。 思路： 这个 \\(82\\) 实在是看不出什么名堂, 思考一下我们怎么保证得到正确且有用的信息, 来缩小范围。 我们可以将数字集合分为不相交的 \\(A, B, C, D\\) 四部分, 询问 \\(A \\cup B\\) 以及 \\(C\\) , 设 \\(True\\) 为回复在这个子集里, \\(False\\) 为回复不在这个子集里, 我们可以分类讨论： 1. \\(A \\cup B=True, \\; C=False\\) 1.1 前者说谎且后者未说谎, 实际为 \\(A \\cup B=False, \\; C=False\\) , 可排除 \\(A, B, C\\) 。 1.2 前者未说谎且后者未说谎, 实际为 \\(A \\cup B=True, \\; C=False\\) , 可排除 \\(C, D\\) 。 2. \\(A \\cup B=True, \\; C=True\\) 2.1 前者说谎且后者未说谎, 实际为 \\(A \\cup B=False, \\; C=True\\) , 可排除 \\(A, B, D\\) 。 2.2 前者未说谎且后者说谎, 实际为 \\(A \\cup B=True, \\; C=False\\) , 可排除 \\(C, D\\) 。 3. \\(A \\cup B=False\\) 等价于 \\(C \\cup D=True\\) 进行 \\(swap(A \\cup B), swap(C \\cup D)\\) 即可。 上述的分类讨论都省去了矛盾的部分, 我们固定了 \\(A \\cup B\\) , 加入了更多的约束, 现在我们可以根据 \\(C\\) 的 \\(True/False\\) 来排除部分数集, 取一下交集, 我们可以在 \\(C=False\\) 时排除 \\(C\\) 部分, 在 \\(C=True\\) 时排除 \\(D\\) 部分。 这样每两次操作我们可以把数据规模大小从 \\(n\\) 变为 \\(\\lceil \\frac{3}{4}* n \\rceil\\) 。 如果还剩余三个及以下的数怎么办？ 因为最后可以问两次, 我们只要考虑剩三个数时的策略即可。 设这三个数为 \\(A, B, C\\) , 询问 \\(A, B\\) 各一次, 则： 1. \\(A=True, \\; B=True\\) 因为 \\(A, B\\) 不可能全为假, 则 \\(C\\) 一定为假, 可以排除 \\(C\\) 。 2. \\(A=True, \\; B=False\\) 分类取交集可排除 \\(B\\) , 具体见下。 2.1 前者说谎且后者未说谎, 实际为 \\(A=False, B=False\\) , 可排除 \\(A, B\\) 2.2 前者未说谎且后者说谎, 实际为 \\(A=True, B=True\\) , 矛盾 2.3 前者未说谎且后者未说谎, 实际为 \\(A=True, B=False\\) , 可排除 \\(B, C\\) 3. \\(A=False, \\; B=True\\) 分类取交集可排除 \\(A\\) , 与 2. 同理。 4. \\(A=False, \\; B=False\\) , 我们可以再问一次 \\(B\\) , 然后再问一次 \\(A\\) 决定排除谁, 具体见下。 4.1 重问中存在 \\(True\\) 的回答, 一定可以转化为上面三种大状况之一。 4.2 重问也全为 \\(False\\) , 因为两次的 \\(B\\) 是连续询问的且全为 \\(False\\) , 所以我们可以排除 \\(B\\) 艹！好难想！ 代码： link Educational Codeforces Round 137 (#Div2) 22/10/17 \\((1712\\rightarrow1791, rk377)\\) 好! DP 不会! 但糊了D! 希望下次继续! E 题意： 有一个敌人, 血量为 \\(h\\) , 你有两把武器, 伤害和攻击冷却时间分别为 \\(p_{1}, t_{1}, p_{2}, t_{2}\\) 。当武器冷却完成时, 你可以使用一把, 或是等到两把武器都冷却完成时一起使用, 会造成 \\(P-s\\) 的伤害, 其中 \\(s\\) 是一个给定的值(护盾?), \\(P\\) 是使用的武器的伤害的总和。 问可以消灭敌人(使其血量 \\(\\leq 0\\))的最少时间(初始两把武器都需要完全冷却)。 \\(1 \\leq h,p_{1},p_{2} \\leq 5000, 1 \\leq s &lt; min(p_{1},p_{2}), t_{1}, t_{2} \\leq 1e12\\) 思路： \\(\\tt{dp}\\) 是显然的, 状态是不会列的。 考虑到每次同时使用两把武器时, 冷却时间都重置为 \\(0\\) , 转化为一个更低 \\(h\\) 的子问题。 我们直接设 \\(f[i]\\) 表示造成 \\(i\\) 点伤害所需的最短时间, \\(f[i]\\) 可以通过单发射击以及更复杂的齐射转移而来。 \\[f[i]=min(f[max(0,i-(p_{1}-s))]+t_{1},f[max(0,i-(p_{2}-s))]+t_{2})\\] 这代表造成 \\(i\\) 点伤害的最短时间可以由造成 \\(i-p_{1}\\) 或 \\(i-p_{2}\\) 伤害的最短时间加上 \\(t_{1}\\) 或 \\(t_{2}\\) 的冷却时间转移。 接下来考虑复杂转移：考虑进行 \\(j\\) 次射击某一把武器, 其中最后一次是齐射, 下文以 \\(j\\) 次使用第一把武器射击为例： 第一把的独立贡献为： \\((j-1)* (p_{1}-s)\\) 第二把的独立贡献为： \\((j* t_{1}-t_{2})/t_{2}* (p_{2}-s)\\) 齐射的贡献为： \\(p_{1}+p_{2}-s\\) 总贡献为： \\[D=(j-1)* (p_{1}-s)+(j* t_{1}-t_{2})/t_{2}* (p_{2}-s)+p_{1}+p_{2}-s\\] 转移方程为： \\(f[i]=min(f[i],f[max(0,i-D)]+j* t_{1}\\) 使用另一把武器同理, 总复杂度为 \\(O(n^2)\\) 代码：link F 矩阵 + 线段树, 不会矩阵…… G DP Educational Codeforces Round 138 (#Div2) 22/10/20 \\((1791\\rightarrow1780, rk1050)\\) 后排膜 \\(\\color{black}{\\tt{L}}\\color{red}{\\tt{STM\\_\\_}}\\) 三十场题解了, 还是只会ABC, 怎么办呢？ 在家的效率很差, 很有发癫的冲动, 或是进行强迫性摆烂行为。 唯一的梦想是 codeforces 上 1900, 可能还有一个出codeforces div2的梦想, 但那个不着急。写不出来什么了, 我高中以来没有什么表达的冲动, 憋这几行字就是极限了, 有点伤心。 D 题意： 对于一个数组 \\(a\\) , 如果 \\(gcd(a_{i},i)=1\\) 就可以删除第 \\(i\\) 个元素并把后面的元素前移并重新标号。 现在给定 \\(n, m(n \\leq 3e5, m \\leq 1e12)\\) , 要求求满足以下条件的有多种删空数组的方式的数组的数目： - \\(a\\) 的长度小于 \\(n\\) - \\(a_{i} \\leq m\\) 思路： 正难则反, 考虑仅有一种删空方式的数组的删除序列, 一定是 \\(1,1,1,...\\) 形式的。这要求第一个数随便取, 第 \\(i\\) 个数必须与 \\([2,i]\\) 内的每个数不互质, 所以我们把 \\([2,i]\\) 内的每个质数乘起来就好了。 代码：link E 题意： 给定 \\(n * m(n* m \\leq 4e5)\\) 的棋盘, 初始存在一些仙人掌, 你要再种最少数量的仙人掌使得没有第一行到第 \\(n\\) 行的路径, 种仙人掌的格子之间不能有公共边。 思路： 仙人掌分割第一行与第 \\(n\\) 行等价于存在一条第一列到第 \\(m\\) 列的仙人掌路径, 于是去写一个 bfs, 于是过了。 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667int N, M, T;int dx[5]={1,1,1,-1,-1}; //注意 dx[0]/dy[0] 的不同int dy[5]={1,1,-1,-1,1};int dx2[5]={0,1,0,-1,0};int dy2[5]={0,0,-1,0,1};struct node {int x, y;};int main(){// freopen(&quot;testdata.in&quot;,&quot;r&quot;,stdin); T=read(); while(T--){ N=read(); M=read(); vector&lt;vector&lt;int&gt;&gt; mp(N+2, vector&lt;int&gt;(M+1)), dis(N+1, vector&lt;int&gt;(M+1, inf)), pre(N+1, vector&lt;int&gt;(M+1)); for(int i=1;i&lt;=N;i++){ string s; cin &gt;&gt; s; s=&quot; &quot;+s; for(int j=1;j&lt;=M;j++) mp[i][j]=(s[j]=='#'); } deque&lt;node&gt; q; //depue push_front, pusk_back 控制访问顺序, 因为我不会 lambda for(int i=1;i&lt;=N;i++){ if(mp[i][1]==1){ dis[i][1]=0; pre[i][1]=0; q.push_front((node){i,1}); } else if(mp[i][1]==0&amp;&amp;mp[i-1][1]+mp[i+1][1]&lt;1){ //不是所有第一列的点都能作为初始点 dis[i][1]=1; pre[i][1]=0; q.push_back((node){i,1}); } } while(!q.empty()){ node tmp=q.front(); q.pop_front(); int x=tmp.x, y=tmp.y; for(int i=1;i&lt;=4;i++){ int nx=x+dx[i], ny=y+dy[i]; int tag=0; for(int j=1;j&lt;=4;j++) { //判断 bfs 到的点能否种植仙人掌, 不写成函数因为我不会 lambda int nnx=nx+dx2[j], nny=ny+dy2[j]; if(nny&gt;M||nny&lt;1||nnx&gt;N||nnx&lt;1) continue; if(mp[nnx][nny]) tag=1; } if(nx&lt;1||ny&lt;1||nx&gt;N||ny&gt;M||dis[nx][ny]!=inf||tag) continue; pre[nx][ny]=i; //保存转移路径, 最后回溯 dis[nx][ny]=dis[x][y]; if(mp[nx][ny]==1) q.push_front((node){nx,ny}); else dis[nx][ny]++, q.push_back((node){nx,ny}); } } int ans=inf, pos=-1; for(int i=1;i&lt;=N;i++){ if(dis[i][M]&gt;=ans) continue; ans=dis[i][M], pos=i; } if(pos==-1) {cout &lt;&lt; &quot;NO\\n&quot;; continue;} int tmpM=M; while(tmpM) { //回溯, 第一列用 dx[0]/dy[0], 有点丑 int x=dx[pre[pos][tmpM]], y=dy[pre[pos][tmpM]]; mp[pos][tmpM]=1; pos-=x; tmpM-=y; } cout &lt;&lt; &quot;YES\\n&quot;; for(int i=1;i&lt;=N;i++){ for(int j=1;j&lt;=M;j++) mp[i][j]?cout &lt;&lt; &quot;#&quot;:cout &lt;&lt; &quot;.&quot;; cout &lt;&lt; '\\n'; } } return (0-0); } 有谁能教我 lambda 表达式, 这样我们就都不用忍受又臭又长的代码了…… F 于 22.11.21 晚尝试补题, 拖了一个月, 看看之前发癫的人。 题意： 给定一颗树, 点有点权, 初始为 \\(0\\) , 有以下两种操作： 1.询问某点点权 2.将树上所有到 从 \\(u\\) 到 \\(v\\) 的路径的距离不超过 \\(d\\) 的点的点权加上 \\(k\\) \\(n, q \\leq 2e5, d \\leq 20, k \\leq 1000\\) 思路： \\(d=0\\) 就是经典的树链剖分问题了, \\(d \\neq 0\\) 怎么办, 看到 \\(d\\) 的范围很小, 考虑暴力维护, 。 具体怎么维护呢？我们可以先简化问题, 每次只更新一个点 \\(u\\) 。我们可以设 \\(bit[u][d]\\) 记录对于每个点 \\(u\\) , 在 \\(u\\) 的子树中距离 \\(u\\) 的距离为 \\(d\\) 的节点增加的权值。这么定义是因为这些节点只有 \\(u\\) 才能 \"够到\" 。每次我们更新一个 \\(u\\) 点, 我们要更新 \\(bit[u][d]\\) , \\(bit[fa[u]][d] \\;\\&amp;\\&amp;\\; bit[fa[u]][d-1]\\) , \\(bit[fa[fa[u]]][d-1] \\;\\&amp;\\&amp;\\; bit[fa[fa[u]]][d-2].....\\) 直到 \\(d=0\\) 为止, 做到不重不漏。 每次更新一条链也是同理的, 我们把链总可以拆成自顶向下的一段或两段, 设这样的一段链 u-v 中, \\(dep[u] \\leq dep[v]\\) , 对于 \\([u+1,v]\\) 这些节点, 我们更新 \\(bit[i][d]\\) , 而对于 \\(u\\) 及以上的点, 我们更新 \\(bit[fa[u]][d] \\;\\&amp;\\&amp;\\; bit[fa[u]][d-1]\\) , \\(bit[fa[fa[u]]][d-1] \\;\\&amp;\\&amp;\\; bit[fa[fa[u]]][d-2].....\\) 这样同理向上, 跳一格, 更新两格。 在询问一点 \\(u\\) 的权值时, 我们只要把 \\(bit[u][0], bit[fa[u]][1], ..., bit[n][d]\\) 求和就行了, 复杂度 \\(O(20 * n log^2 n)\\)。 代码： link 注意若暴力上跳时, 若 \\(d\\) 还没有 \"用完\" , 但已经跳到根节点了, 就要在根节点把 \\(d\\) 用完。 Codeforces Round 829 (#Div2) 22/10/23 \\((1779\\rightarrow1816, rk400)\\) 这是 10/23 打的第一场比赛, 难度偏低, 第一次糊出五题(如果C1, C2算两题的话……)。 E 简单期望不会…… 其中 C2 和 D 都是糊一个结论过的…… 上了 \\(1800\\), 但只上了 2h…… E 题意： 给定一个长度为 \\(n\\) 的 \\(01\\) 数组 \\(a\\) , 每次随机选择两个数, 若 \\(i&gt;j \\;\\&amp;\\&amp;\\; a_{i}&lt;a_{j}\\) 则交换, 期望多少次能让数组变得有序(升序)。 思路： 不会期望, 直接被吓跑力~ 假设 \\(a\\) 中有 \\(cnt_{0}\\) 个 \\(0\\) , 那么它们一定在前 \\(cnt_{0}\\) 个位置, 我们的有效操作就是把前 \\(cnt_{0}\\) 个数中的 \\(1\\) 与后面的 \\(0\\) 交换, 设前 \\(cnt_{0}\\) 个数中有 \\(x\\) 个 \\(1\\) ， 则后面也一定有 \\(x\\) 个 \\(0\\) , 所以有效操作的概率为 \\(\\dfrac{x^2}{C_n^2}\\) , 期望为概率的倒数, 即 \\(\\dfrac{C_n^2}{x^2}\\) , 每一步加上就是最后的答案。 代码：link F 题意： 在 \\(n \\times m (n \\times m \\leq 3e5)\\) 的沙滩方格上有一些 \\(1 \\times 2\\) 的躺椅, \\(1 \\times 1\\) 的障碍和一些空格, 你想要找到 \\(1 \\times 2\\) 的空地放下自己的躺椅。障碍不可移动, 你可以花费 \\(p\\) 使得一个躺椅旋转 \\(90\\) 度放置, 也可以花费 \\(q\\) 使得一个躺椅沿着长边方向平移一格, 移动躺椅需要保证移动后的位置是没有障碍的。问找到 \\(1 \\times 2\\) 空地的最小移动花费, 或报告无法找到空地。 思路： 从 \\(1 \\times 2\\) 的躺椅联想到黑白染色, 若将躺椅的移动看成空格的移动, 不难发现每次都是黑格移到黑格, 白格移到白格, 最后我们放置躺椅的位置也是一黑一白的。 我们可以证明, 在最佳情况下, 每个躺椅的移动不超过一次, 我有一个感性的理解, 每次移动要么从黑格移到黑格, 要么从白格移到白格, 若移动两次说明既进行了一次黑格的移动也进行了一次白格的移动, 然而考虑躺椅及其周围一共八个格子, 分类讨论发现这两次移动总可以被一次更好的移动直接替代来获取 \\(1 \\times 2\\) 的空地…… 我们从所有的空地出发, 对于所有的躺椅, 从躺椅四周的点往躺椅的一端连一条带权有向边表示空地的移动, 跑一遍 \\(dijkstra\\) , 最后找一下相邻两格的权值和最小值就做完了…… 代码：link 若把图黑白染色, 也可以把本题看成将图的最大匹配 \\(+1\\) 所需的最小权值, 看题解评论有人说这个, 不知道能不能做。 Codeforces Round 830 (#Div2) 22/10/23 \\((1816\\rightarrow1769, rk1321)\\) 这是 10/23 打的第二场比赛, 难度偏高, C1 &gt; D1。糊了个D1, 但 C1, C2 不会, C1 寄了五发, 把上一场加的分全打回去了, 还倒欠 \\(10\\) 分, 警钟敲烂！ C1&amp;C2 题意： 给你长度为 \\(n\\) 的数组 \\(a\\) , \\(Q\\) 次询问给出 \\(L_{i}, R_{i}\\) , 让你找出一个子数组 \\(a[l,r]\\) , 满足 \\(L_{i} \\leq l \\leq r \\leq R_{i}\\) 且 \\(f(l,r)=sum(l,r)-xor(l,r)\\) 最大, 有多个最大值时要求 \\(r-l+1\\) 最小, 对于每个询问输出 \\(l,r\\)。 其中 \\(sum(l,r)=\\sum\\limits_{i=l}^{r}{a_{i}}\\) , \\(xor(l,r)=a_{l} \\oplus a_{l+1} \\oplus ... \\oplus a_{r}\\) 。 \\(n \\leq 1e5, a_{i} \\leq 1e9\\) , 简单版本 \\(Q=1\\) , 困难版本 \\(Q=n\\) , 钦定 \\(L_{1}=1, R_{1}=n\\) 思路： \\(Q=1\\) 的情况糊了个二分但是一直错…… 注意到做区间最小 \\(xor\\) 本来是要上 \\(trie\\) 的, 但是考虑每加一个数 \\(x\\) 时, \\(xor\\) 的变化一定小于等于 \\(sum\\) 的增量, 所以有 \\(f(l,r) \\leq f(l,r+1)\\) , 数组长度越长答案越优, 最大值肯定要选取整个数组, 接下来我们二分长度找到最短长度使得最大值不下降即可。 \\(Q=n\\) 的情况就要利用到一些性质了…… 首先, \\(0\\) 蛋用没有, 没有任何贡献, 还占长度, 答案去掉前导 \\(0\\) 或后缀 \\(0\\) 一定更优。 假设现在我们选取给定的 \\([L_{i},R_{i}]\\) 整个子数组, 现在我们考虑什么情况下 \\(f(L_{i},R_{i})\\) 会减少, 删去一个数, 若 \\(xor\\) 的减量小于 \\(sum\\) 的减量时更劣, 因此每个二进制位只能被删除操作覆盖最多一次, 否则第二次覆盖时 \\(xor\\) 的减量更小(0 -&gt; 1, 或是第一次时就不能覆盖, 0 -&gt; 1), 因为 \\(10^9 &lt; 2^{30}\\) , 我们只考虑 \\([L_{i},R_{i}]\\) 内前 \\(31\\) 个非零数和后 \\(31\\) 个非 \\(0\\) 数中选出左右端点即可, 若非 \\(0\\) 数凑不齐 \\(31\\) 个……直接双重循环暴力吧……注意特判 \\([L_{i},R_{i}]\\) 全为 \\(0\\) 的情况。 具体实现上, 需要再处理一个非 \\(0\\) 数字数目的前缀和, 以及每个数前面/后面的第一个非 \\(0\\) 数字。 代码：C1, C2 事实上, C1 代码太丑了, 直接看 C2 就行。 D2 题意： 一开始给你一个集合, 只包含 \\(0\\) , 接下来有 \\(Q \\leq 2e5\\) 次操作, 每次操作可能是下列形式的一种 - 往集合中添加 \\(x\\) - 在集合中删除 \\(x\\) (保证 \\(x\\) 已在集合中) - 查询集合的最小未出现过的 \\(x\\) 的倍数(x-mex) D1 没有删除操作, 你直接记忆化暴力, 用一个 \\(last[k]\\) 存储上一次询问 k-mex 的答案, 下次再询问到时从那儿再更新就行了, 有删除操作时我们也可以仿照 D1。 具体地, 我们开一个 map&lt;int,set&gt; del; 来用 \\(del[x]\\) 表示 \\(x\\) 未出现的倍数, 再开一个 map&lt;int,vector&gt; change; 用 \\(change[x]\\) 来表示删除 \\(x\\) 后改变了哪些数的 \\(last\\) , 询问的时候更新 \\(last\\) 的时候顺带更新 \\(change\\) 就好了。 代码：link 有点抽象…… Codeforces Round #831 (Div. 1 + Div. 2) 22/10/29 \\((1769\\rightarrow1819, rk712)\\) 第一次场上做出 E, 泪目, 虽然这个 E 只有 1800…… F 题意： 给你一个含有 \\(n \\leq 2000\\) 个整数的数组 \\(a(1 \\leq a_{i} \\leq n)\\) , 对于每个 \\(a_{i}\\) 添加一个元素集 \\(\\{a_{i}\\}\\) 定义一次操作由以下两步构成： - 选择两个集合 \\(S, T\\) , 满足 \\(S \\cap T = \\emptyset\\) - 删除集合 \\(S, T\\) , 添加集合 \\(S \\cup T\\) 在进行 \\(0\\) 次或多次操作后, 我们构造一个可重集合 \\(M\\) 表示当前剩下的所有集合的大小。举个例子, 若当前剩下的集合为 \\(\\{5\\}, \\{8\\}, \\{2,5,12,4\\}\\) , 则 \\(M\\) 为 \\(\\{1,1,4\\}\\) 。现在让你求出所有不同的 \\(M\\) 的数量, 对 \\(998244353\\) 取模。 思路： 让我们层层推进。 1、所有的 \\(M\\) 集合都可以通过在后面补 \\(0\\) 变成大小为 \\(n\\) 的集合, 记 \\(M\\) 集合中的元素为 \\(m_{1}, m_{2}, m_{3}, ..., m_{n}\\) , 让我们以 \\(m_{1} \\geq m_{2} \\geq m_3 \\geq ... \\geq m_{n} \\geq 0\\) 为 \\(M\\) 中的元素排序, 直觉告诉我们, 一个 \"更大\" 的合法集合如 \\(M=\\{11,3,2,1,0,0...\\}\\) 可以直接拆成 \"更小\" 的合法集合, 如 \\(M'=\\{6,5,3,2,1,0,0...\\}\\) 2、现在定义何为 \"更大\" , 令 \\(M\\) 为原集合的前缀和集合, \\(\\{11,3,2,1,0,0...\\} \\Rightarrow \\{11,14,16,17,17,17...\\}\\) 。 对于两个集合 \\(M, M'\\) , 若 \\(\\forall\\;i \\in [1,n], m_{i} \\geq m'_{i}\\) 则 \\(M\\) 更大。 3、记 \\(cnt_{i}\\) 为 \\(a\\) 数组中数字 \\(i\\) 出现的次数, 可以看出来, \"最大\" 的集合只有一个, 我们把出现过一次及以上的数的个数记为 \\(m_{1}\\) , 出现过两次及以上的数的个数记为 \\(m_{2}\\) …… 这样我们可以定义出 \\(M\\) , 若按照刚刚的前缀和的定义, \\[m_{i}= \\sum\\limits_{j=1}^{n}{min(cnt_{j},i)}\\] (注意上文的 \\(j\\) 不是 \\(a_{j}\\)) 现在回到原始定义, 那么一个合法的 \\(M\\) 集合就要满足： \\[\\forall k \\in [1,n], \\sum\\limits_{i=1}^{k}{m_{i}} \\leq \\sum\\limits_{j=1}^{n}{min(cnt_{j},k)} \\;\\;and\\;\\; \\sum\\limits_{i=1}^{n}{m_{i}}=n\\] 我们考虑进行 DP 。 4、设 \\(lim[i]=\\sum\\limits_{j=1}^{n}{min(cnt_{j},i)}\\) , 我们考虑一个三维的 \\(f[p][sum][las]\\) 表示 \\(M\\) 已经填了 \\(p\\) 位, \\(p\\) 位的和为 \\(sum\\) , 第 \\(p\\) 位(最后一位)为 \\(las\\) 时的方案数, 则有如下转移： \\[f[p][sum][las]=\\sum f[p-1][sum-x][x], x \\geq las, sum \\leq lim[p]\\] \\(x \\geq las\\) 是因为 \\(m_{1}, m_{2}, ..., m_{n}\\) 单调不降, 转移后再令： \\[f[p][sum][las]=\\sum\\limits_{i=las}^{n}{f[p][sum][i]}\\] 做后缀和可以把 \\(O(n^4)\\) 的复杂度降到 \\(O(n^3)\\) , 空间复杂度可以通过滚动数组降到 \\(O(n^2)\\) , 然而 \\(O(n^3)\\) 的时间复杂度还是无法通过本题。 5、考虑 \\(las\\) 作为最后一个数, 有 \\(n \\geq p *las\\) , 所以 \\(las \\leq n/p\\) , 总的复杂度为调和级数形式, \\(O(n*n \\; \\log \\; n)=O(n^2\\;log\\;n)\\) 。 代码：link 因为本题的转移在前缀和优化后其实就是一个 \\(O(1)\\) 的赋值, 我的滚动数组会出现奇奇怪怪的错误, 就直接开两个数组 \\(f, g\\) 了, 同时后缀和的开头要大一点, 要让后面的完全覆盖前面的值, 同时注意 \\(g\\) 数组的初始化…… Codeforces Round 832 (#Div2) 22/11/4 \\((1819\\rightarrow1792, rk1231)\\) 进行一个烂的摆, D不会, A, B各吃一发罚时, 寄。 D 题意： 长度为 \\(n\\) 的数组 \\(a\\) , \\(Q\\) 次独立询问, 每次询问给出一个 \\([L_{i},R_{i}]\\) 区间, 你可以在区间内进行任意次操作： -选择 \\(l, r (L_{i} \\leq l \\leq r \\leq R_{i})\\) 且 \\(r-l+1\\) 为奇数 -将 \\(a_{l}, a_{l+1}, ..., a_{r}\\) 赋值为 \\(a_{l} \\oplus a_{l+1} \\oplus ... \\oplus a_{r}\\) 问能将 \\(a[L_{i},R_{i}]\\) 区间整体置 \\(0\\) 的最小操作次数, 或输出 \\(-1\\) 表示不可能。 \\(n, Q \\leq 2e5, 1 \\leq a_{i} \\leq 2^{30}\\) 思路： 寄！就差最后一步…… 若区间异或和不为 \\(0\\) , 直接输出 \\(-1\\) 。 若区间全为 \\(0\\) , 直接输出 \\(0\\) 。 若区间长度为奇数, 我们可以直接操作整个区间, 输出 \\(1\\) 。 若 \\(a[L_{i}]=0\\) 或 \\(a[R_{i}]=0\\) , 我们可以忽略这一个长度, 然后输出 \\(-1\\) 同上。 重点 , 若以 \\(a[R_{i}]\\) 为右端点, 存在某一段区间异或和为 \\(0\\) 且这段区间的左端点 \\(lb \\geq L_{i}\\) 且 \\(lb\\) 与 \\(R_{i}\\) 奇偶性相同, 输出 \\(2\\) 。 否则剩余情况输出 \\(-1\\) 。 若答案 \\(\\geq 3\\) , 中间的区间一定能被并入两边区间, 你可以手玩一下, 若中间的并入两边的某一个区间后新的区间长度为奇数就正好, 若并上两边的区间长度都为偶数我们就直接三个区间并在一起。 我们维护以一个点为右端点的最小的异或和为 \\(0\\) 的区间, 只要开一个 二维 \\(map\\) , 第一维大小为 2, 来回跳就可以了。 代码：link E 2900 的 E, 补你** CodeTON Round 3 (Div.1+Div.2) 22/11/6 \\((1792\\rightarrow1732, rk2536)\\) 思维僵化, 手速慢, 吃罚时, C写了三遍……, D 看错一个范围(\\(a_{i} \\leq m \\Rightarrow a_{i} \\leq n\\)), 进行一个分数回调, 从来不开窍, 我癫都懒得发了。 E 题意： 给你一个长度为 \\(n(n \\leq 2e5)\\) 的括号序列, 问你将它的所有(\\(n*(n+1)/2\\)) 的子串变为合法括号序列的最少操作数之和。你有两种操作可用, 分别是选择子串循环右移一格以及插入一个任意括号。 思路： 经典括号匹配问题, 对于一个合法括号序列, 记 \\((\\) 为 \\(+1\\), \\()\\) 为 \\(-1\\) , 它的前缀和不能掉到 \\(0\\) 以下且总和为 \\(0\\)。 首先, 因为总和必须为 \\(0\\) , 要加的左右括号数是确定的。 然后？然后不会了。 使用 codeforces 题解评论区 \\(\\color{orange}{\\tt{adamant}}\\) 的解法： 一个可以手玩出的结论是, 设一个括号序列的最小前缀和为 \\(m(m \\leq 0)\\), 总和为 \\(c\\) , 需要的最小操作总数为 \\(|m|+max(0,c)\\) 。可以这么考虑：不管最后哪种括号更多, 中间前缀和小于 \\(0\\) 的部分都要通过找一个末尾为 \\((\\) 的子串右移来把前缀和 \"拉上来\", 右括号更多就可以虚空造左括号来拉前缀和, 左括号更多在拉完前缀和后还要造更多的右括号使得总和为 \\(0\\)。 我们记这个括号序列为 \\(s=(m;c)\\) , 现在考虑分治的思想, 对于括号序列 \\(s_{1}=(m_{1};c_{1}), s_{2}=(m_{2};c_{2}), s_{1}+s_{2}=(min(m_{1},c_{1}+m_{2});c_{1}+c_{2})\\) 左边的 \\(min\\) 表示最小前缀和 \\(m\\) 要么出现在 \\(s_{1}\\) 中, 要么出现在 \\(s_{2}\\) 中。 现在考虑怎么做这个分治, 先把括号序列分为左右两部分(\\([0,mid),[mid,n]\\)), 算好子串在单个部分内的贡献, 再算子串跨部分的贡献(下文 \\(c_{1}, c_{2}\\) 等来自 \\(s_{1}=[start,mid), s_{2}=[mid,p]\\) 两个子串)。 枚举左边的每个括号作为子串的起点, 按照 \\(c\\) 将右边排序, 找到最远的终点使得 \\(c_{1}+c_{2} \\leq 0\\) , 这样左边的子串的 \\(c\\) 贡献为 \\(0\\), 右边的子串的 \\(c\\) 贡献为 \\(c_{1}+c_{2}\\)。 枚举左边的每个括号作为子串的起点, 按照 \\(m\\) 将右边排序, 找到最远的终点使得 \\(m_{2} \\leq m_{1} - c_{1}\\) , 这样左边的子串的 \\(m\\) 贡献为 \\(|m_{2}+c_{1}|\\), 右边的子串的 \\(m\\) 贡献为 \\(|m_{1}|\\), 不用担心绝对值符号, 因为取 \\(|m_{2}+c_{1}|\\) 时, \\(m_{2}+c_{1} \\leq m_{1} \\leq 0\\)。 于是应该做完了…… 代码：link 虽然有点复杂, 但也是一种做法嘛…… 注意代码里前缀后缀满天飞……我尽量注释好了…… 复杂度是 \\(O(n log^2 n)\\) 的吧…… Codeforces Round 832 (#Div2) 22/11/12 \\((1731\\rightarrow1801, rk374)\\) 最后糊出了D, 好起来了。 E 笛卡尔树, 不会 F 构造, 摆 今天的补题就到这里了, 大家下场再见 Pinely Round 1 (Div. 1 + Div. 2) 22/11/20 \\((1801\\rightarrow1906, rk460)\\) 上紫了！运气真的很好！ ABC 差不多手速拉满 + 0 dirt D 在赛时的最后 15 mins 才冲出来, 没有 ABC 的手速和上午刚打的数论小板子我这样的菜鸡估计会寄在 China Round 的 D, 最后贴线上紫。 成功在 NOIP 前最后一场比赛完成退役前上紫的既定目标, 现在别无所求, NOIP 随便打了。 E 题意： 通过 01矩阵 给你一个 \\(n \\leq 4000\\) 节点的无向图, 问你需要至少多少次操作才能让图联通, 每次操作是这样的： - 任意选择一个 \\(u\\) 节点 - 对于剩余的所有节点, 如果有 u-v 边, 就断掉, 否则就加上 要求输出最少操作次数和操作序列。 思路： 考虑一下什么时候答案为 \\(1\\) , 当场上有孤立点时, 直接操作那个孤立点就行。 我们可以得出一个粗糙的策略, 找到度数最小的点并通过操作与它相邻的所有点把那个点独立, 最后操作那个点。 这个结论对于 E 而言太过简单, 我们想一想反例, 在一个联通块(大小为 siz)中若出现一个度数不为 siz-1 的点, 我们可以直接操作这个点一次, 虽然它的边会断掉, 但它还能连回联通块, 而且也可以连上其他的所有点： 就像这样。 我们还可以发现, 我们不用“死扣一个点”, 如果场上有三个及以上联通块, 我们可以在 1, 3 联通块中任意取一个点出来, 这样保证联通块 2 与其他联通块相联通, 只要两次操作。 当场上只有两个联通块, 内部是完全图(团)的时候, 我们对于节点的操作就相当于把一个节点从一个图拉到了另一个图中, 答案为 min(siz1,siz2) 总结一下零零散散的性质： 图已联通时(联通块数量为 \\(1\\) )答案为 \\(0\\) 有孤立点时答案为 \\(1\\) , 选择那个孤立点 若存在某个联通块内部不是完全图(团)的时候选择其中度数最小的节点, 答案为 \\(1\\) 若有两个联通块, 答案为 \\(min(siz_{1},siz_{2})\\) , 输出较小联通块内的所有节点 若有三个及以上联通块, 答案为 \\(2\\) , 任选两个联通块, 各挑一个点输出。 代码：link","link":"/2024/07/05/Codeforces-exercises-2022/"},{"title":"Matlab 基础入门","text":"Matlab 基本入门和有关数学建模的一些基本库的应用 参考资料 数学建模算法与应用第3版(司守奎,孙玺菁) 数学建模算法与应用第3版参考答案(司守奎,孙玺菁) 零基础入门 Matlab Matlab 利用 .m 后缀的脚本文件编程 (Ctrl+N 创建) clear 清空工作区 clc 清空命令行窗口 clear all 清除工作空间的所有变量, 函数(比 clear 更彻底) F5 运行程序, F9 运行选中部分 注释 % 多行注释 Ctrl+R 解注释 Ctrl+T Ctrl+I 智能调整缩进 语句末尾的分号控制语句是否显示, 加入分号就不显示 可以用 %% 在首尾括住一块内容, 然后按 Ctrl+Enter 运行这一部分 disp() 函数输出文本/数字/矩阵 Matlab debug: 设置断点, 按 F10 步进 Ctrl+C 强行中断程序进行 format long 小数点后保留 15 位(默认为 format short) 数据类型: 数字(整数,浮点数,复数), 字符和字符串(' '和“ ”), 矩阵 ([ ]) 矩阵 创建矩阵 直接输入 a=[1 2 3;4 5 6] 或 a=[1:3;4:6] 或 a=[1:1:3;4:1:6] 函数创建矩阵: 生成全 0 矩阵(n * m) zeros(n,m) (生成 n*n 方阵: zeros(n)) 生成全 1 矩阵(n * m) ones(n,m) 生成单位矩阵 (n * m) eye(n,m) 生成 (0,1) 随机数矩阵 (n * m) rand(n,m) 生成 [l,r] 随机整数矩阵 (n * m) randi([l,r],n,m) 生成 \\(\\mu=0, \\sigma=1\\) 正态分布随机数矩阵 (n * m) randn(n,m) 导入本地文件生成矩阵 矩阵元素修改 改 i 行 j 列元素为 k: a(i,j) = k 改 i 行为 k: a(i,:) = k 改 [i1,i2,i3] 行的 [j1,j2,j3] 为 k: a([i1,i2,i3],[j1,j2,j3]) = k 若改变超出范围的下标, 其他位置会自动填 0 删除第 i 列: a[:,i] = [] 删除最后一行: a[end,:] = [] 线性索引删除第 i 个元素: a(i) = [] 矩阵剩下部分会排成一行 矩阵拼接 A B 行数相同, 横向拼接 C=[A B] 或 C = cat[2,A,B] A B 列数相同, 纵向拼接 C=[A; B] 或 C = cat[1,A,B] 矩阵重构与重排 如果 A 中元素数为, m*n, 则可以令 B = reshape(A,m,n) 来重构 A 矩阵(按照线性索引) 如果不知道有多少列，可以令 B = reshape(A,m,[]) sort 函数对向量或矩阵排序, sort(A,dim) 或 sort(A,dim,‘descend’) 实行升序或降序排列 若 dim = 1, 则沿行方向排序; 若 dim = 2, 则沿列方向排序 sortrows 函数基于某一列给整个矩阵排序, 同一行元素不变, 即 sortrows(A,row) 若有多个维度排序, 则用 sortrows(A,[r1,r2,...]) 传向量, 即先以 r1 列排序, 若相同再以 r2 列排序, etc. 矩阵的运算 调用函数运算 sum(A,dim) 或 sum(A(:)) 返回按每行/每类求和的向量或矩阵内元素总和 prod(A,dim) 求乘积, 与 sum 类似, prod(A(:)) = prod(A,\"all\") diff(A) 求 A 按行的差分矩阵 (n * m) -&gt; ((n-1) * m) mean(A,dim) 求平均数 median 求中位数; mode 求众数; var 求方差; std 求标准差; min 求最小值; max 求最大值 矩阵加减 共五种兼容运算, 见下图: 矩阵乘/除/乘方/转置 * 代表矩阵相乘, .* 代表矩阵对应元素相乘(满足五种兼容) / 代表右除(若 x*A = B , 则 x = B/A) \\ 代表左除(若 A*x = B, 则 x = A\\B) ./ 代表矩阵对应元素相除 ^ 表示矩阵乘方(与数字), .^ 代表矩阵对应元素乘方对应次数 ' 矩阵共轭转置, \\(A^H = A^{'}\\) , .' 矩阵转置, \\(A^T = A^{.'}\\) 矩阵关系运算 == 等于; ~= 不等于; &gt; 大于; &gt;= 大于等于; &lt; 小于; &lt;= 小于等于 生成一个逻辑值矩阵, 矩阵的每个元素为 0 或 1, 可以进行上文的五种兼容运算 逻辑 运算符 &amp; (and) 与; | (or) 或; ~ (not) 非; xor 异或 以上运算都仅返回 0/1 xor(3,4) = 0 因为这里把 3, 4 都看成了逻辑值 1 来运算 A &amp; B 对 A, B 的对应元素求 &amp;, 返回逻辑矩阵, 满足兼容运算 &amp;&amp; 和 || 只能对标量逻辑运算, 具有短路功能, 提高效率 integral(f,a,b) 对 \\(f\\) 函数从 a 到 b 进行积分操作 常用函数 mod(x,p) 求 \\(x \\; mod \\;p\\) 的值 all(A,dim) 当 A 中全为非零值时返回 1 向量, 否则返回 0 向量 any(A,dim) 当 A 中存在非零值时返回 1 向量, 否则返回 0 向量 find 函数: 12345678A = [0,1,2 0,1,2 2,0,1];find(A) = [3;4;5;7;8;9] %返回线性索引, 先列后行编号find(A,2) = [3;4]find(A,2,'last') = [8;9][row,col] = find(A) %row = [3;1;2;1;2;3]; col = [1;2;2;3;3;3][row,col,v] = find(A) %v = [2;1;1;2;2;1] 提取 A 中所有非零元素 语法结构 条件结构 if 语句格式: if - elseif - ... - elseif - else - end if 和 end 不能省略 对于一个矩阵 A, 其逻辑值为 all(A(:)) switch 语句格式: switch - case -...- case - otherwise - end switch 后面跟着开关表达式, 其运算结果不能为向量或矩阵 循环结构 123456789for i = 1:5 disp(i) % 自带两个 '\\n', 本处为了节约空间只换行一次end%---------------------12345 若 for i = A , 则 i 会循环 A 的列数次数, 每次输出 n*1 的列向量 while 语句格式: while - end 123456789f(1) = 1; f(2) = 1;n = 2;while f(n) &lt;= 99999 n = n+1; f(n) = f(n-1) + f(n-2);endn%---------------------n = 26 break 和 continue 关键字和 C++ 一样, 不讲了 函数 自定义函数 需要单独开一个 函数名.m 结尾的文件放函数(好像不用其实？只要放在代码末尾就行), 嵌套的子函数就不用了 基本结构: 123456789101112function[输出形参表: output1, ..., outputn] = 函数名(输入形参表: input1, ..., inputn)注释...代码...end%-----------------------------------%eg. 编写 max_min_values.m 求向量最大最小值function [mn,mx] = max_min_values(X)x1 = sort(X);x2 = sort(X,'descend');mn = x1(1);mx = x2(1);end 匿名函数 效率更高, 不用单开 .m 文件 基本格式: f = @(参数1, 参数2, ...) 函数表达式 12%eg. f(x,y) 返回 x^2+y^2f = @(x,y) x^2+y^2; 123456%eg. 双重匿名函数, 返回值为一个一层的匿名函数f = @(a,b)@(x) a*x+b;g = f(3,4);g(1)%---------------------------&gt;&gt; 7 12345%eg. 多行匿名函数, [] 括起来PlotSin=@(x,y) [figure; plot(x,y,'r -*');title('Sin(x)');];x=-pi:0.05:pi; % x 为一个向量, 在 -pi 到 pi 之间取遍所有 0.05 的步长y=sin(x);PlotSin(x,y); %画一条正弦曲线 其中 figure 函数用于创建一个新的图窗窗口 例题: \\(e^x+x^a+x^{\\sqrt{x}} = 100\\) , 针对 \\(a \\in [0,2]\\) 的不同取值求解 123456789f = @(a)@(x)exp(x)+x^a+x^(sqrt(x))-100;a = 1; A = 0:0.5:2;x = @(a)fzero(f(a),4);X = @(A)arrayfun(@(a)x(a),A);x(a)X(A)%------------------------------------------4.3153 %a = 14.3506 4.3392 4.3153 4.2654 4.1635 %a = 0, 0.5, 1, 1.5, 2 fzero 函数: 求非线性函数的根 语法 fzero(func,x0), func 为函数表达式, x0 为猜测的零点 arrayfun 函数: 将函数应用于每个数组元素 语法 B = arrayfun(func,A) 将 A 的元素应用于 func, 输出串联在 B 中, 有 B(i) = func(A(i)) 特殊函数 常用函数 abs(x) 求 x 的绝对值, 若 x 为复数, 则求其模长 mod(x,p) 求 \\(x \\; mod \\;p\\) 的值 sqrt(x) 对 x 开根号, 复数也可以, 放向量也可以 exp(x) 返回 \\(e^x\\) ​ log(x) 返回 \\(ln(x)\\)​​ log2(x)/log10(x) 返回以 2 为 round(x) 四舍五入 x, 如果含 0.5, 则朝远离 0 的方向调整. round(1.5)=2, round(-0.5)=-1 isempty(A) 若 A = [], 返回 1, 否则返回 0, 有 (length(A)==0) == (isempty(A)) ismember(A,B) 如果 A 中某位置的数据能在 B 中找到, 函数返回一个在该位置为 1 的数组 eg. A=[1:3], x=[2:4], ismember(x,A) = [1,1,0] meshgrid(x,y) 基于向量 x 和 y 中包含的坐标返回二维坐标 rnd(seed) 设置随机数种子为 seed, 默认为 rnd('shuffle') , 随机数种子一直变化 三角函数 图形绘制 二维绘图 plot 函数 plot(x,y) x, y 为两个向量(或矩阵), 本质为两个向量(或矩阵)构造的点连线 plot(x) 默认横坐标为 1, 2, 3, ... sizeof(x), 纵坐标为 x 中的数 若作为矩阵, 则一次绘制多个函数 或通过 plot(x1,y1,x2,y2,x3,y3) 一次绘制多个函数 linspace(x1,x2,num=100) 返回包含 x1 和 x2 之间的 num 个等间距点的行向量 plot(x,y,LineSpec) 其中 LineSpec 指定线型, 标记和颜色, 如 '--or' 为带有圆形标记的红色虚线 fplot 函数 fplot(f) 在默认区间 \\([-5,5]\\) (对于 x) 绘制由函数 y = f(x) 定义的曲线 fplot(f,[l,r]) 在区间 \\([l,r]\\) (对于 x) 绘制由函数 y = f(x) 定义的曲线 当函数对步长要求苛刻时, 或者想少写点代码的话, 就用 fplot(), 以 sin(1/x) 为例: fplot(funx,funy,[l,r]) 在区间 \\([l,r]\\)​ (对于 t) 绘制由函数 x = funx(t) 和 y = funy(t) 定义的曲线 所有的 fplot 式子後面都可以加上 LineSpec 指定線型, 標記和顏色, 你們不會受到任何處分！-發自我的電腦 fimplicit 函数 可以实现隐函数绘图, 默认 \\(f = 0\\) 其他绘图函数 semilogx(X,Y,LineSpec) 在 x 轴上以 10 为底的对数刻度, 在 y 轴上使用线性刻度来绘制 x 和 y 坐标 与 linspace(x1,x2) 对应的, 有 logspace(x1,x2) 在 log 下在 \\([10^{x_1},10^{x_2}]\\)​ 内均匀取点 polarplot(theta,rho,LineSpec) 在极坐标系下绘图, theta 为弧度角, rho 为半径值 统计图: bar(x,y) 创建一个条形图, y 中的每个元素对应一个条形, 横坐标为 x histogram(x,nbins) 创建基于 x 的直方图, 有 nbins 根柱子 pie(X) 使用 X 中的数据绘制饼图。饼图的每个扇区代表 X 中的一个元素 如果 sum(X) ≤ 1，X 中的值直接指定饼图扇区的面积。如果 sum(X) &lt; 1，pie 仅绘制部分饼图 如果 sum(X) &gt; 1，则 pie 通过 X/sum(X) 对值进行归一化，以确定饼图的每个扇区的面积 pie3(X) 生成三维饼图 scatter(x,y) 在向量 x 和 y 指定的位置创建一个包含圆形标记的散点图 scatter3(x,y) 生成三维散点图 quiver(X,Y,U,V) 画一个 (X,Y) -&gt; (U,V) 的向量 图形属性设置 LineSpec 部分 图形标注部分(支持 \\(\\LaTeX\\)): title(标题) xlabel(x轴说明) ylabel(y轴说明) text(x,y,图形说明) legend(图例1, 图例2, ...) axis 函数控制坐标轴 grid on/off 开关网格线 hold on/off 控制保持原有图形 或 刷新图形窗口 三维绘图 plot3(x,y,z) 与 fplot3(x,y,z) 函数, 和二维类似 三维曲面 先生成网格坐标矩阵 [X,Y] = meshgrid(x,y) 然后 mesh(X,Y,Z,c) 或 surf(X,Y,Z,c) 绘制曲面, 其中 c 指定曲面颜色, 默认正比于曲面高度 句柄与窗口 句柄: 可以理解为图像的别名, 设 h = plot(x,y) get(h) 获取图像属性, set(h) 更改图像 eg. set(h,'Color','red') 子图: 同一图形窗口中的不同坐标系下的图形为子图 subplot(m,n,p) 将图形窗口划分为 m*n 个活动区, p 指定当前活动区 subplot(m,n,p) 下方区域的图形属性设置全都属于当前子图 MCM/ICM Matlab 使用 optimproblem 创建和求解优化问题 可以求解线性和非线性规划问题 语法: 12prob = optimproblemprob = optimproblem(Name,Value) x = optimvar(name) 创建标量优化变量。优化变量是符号对象, 可以使用它来为目标函数和问题约束创建表达式 x = optimvar(name,n) 创建由优化变量组成的 n×1 向量 在问题中设置一个目标函数: prob.Objective = x'*a*x 在问题中创建线性不等式约束: prob.Constraints.con = sum(x.^2)==1 检查问题: show(prob) 求解, sol = solve(prob), sol.x 就是答案 \\[ 已知矩阵 A = \\begin{bmatrix} 1 &amp; 4 &amp; 5 \\\\ 4 &amp; 2 &amp; 6 \\\\ 5 &amp; 6 &amp; 3 \\end{bmatrix} , x= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} , 求二次型 f(x_1,x_2,x_3) = \\mathbf{x^T Ax}\\; \\\\ 在单位球面 x_1^2+x_2^2+x_3^2=1 上的最小值 \\] 可以证明其答案等于 \\(A\\) 的最小特征值 可以把其问题归结为如下的非线性规划问题(s.t. = subject to): \\[ 求\\;min \\;\\mathbf{x^T Ax},\\;\\newline s.t.=\\left\\{ \\begin{matrix} x_1^2+x_2^2+x_3^2=1, \\\\ x_i \\in R,\\;\\;i=1,2,3 \\end{matrix} \\right. \\] 1234567891011121314151617clc; clear; format long;a=[1 4 5;4 2 6;5 6 3];val = eig(a);minv = min(val);prob = optimproblem; x = optimvar('x',3); # 3 个变量prob.Objective = x'*a*x;prob.Constraints.con = sum(x.^2)==1;x0.x=rand(3,1); % 非线性就要设置 x0 初值扔进去[sol,fval,flag,out] = solve(prob,x0),sol.x;fval, sol.x#----------------------------------------fval = -3.66868ans = -0.31298 -0.57734 0.75412 \\[ 求\\;max\\;z=2x_1+3x_2-5x_3, \\;\\newline s.t.= \\left\\{\\begin{matrix} x_1+x_2+x_3=7, \\\\ 2x_1-5x_2+x_3\\geq 10, \\\\ x_1+3x_2+x_3 \\leq 12, \\\\ x_1, x_2, x_3 \\geq 0. \\end{matrix}\\right. \\] 123456789101112131415clc; clear;prob = optimproblem('ObjectiveSense','max'); % 指定最大化 objectivex = optimvar('x',3,'LowerBound',0); % 设置变量下限为 0 prob.Objective = 2*x(1)+3*x(2)-5*x(3);prob.Constraints.c1 = x(1)+x(2)+x(3)==7;prob.Constraints.c2 = 2*x(1)-5*x(2)+x(3)&gt;=10;prob.Constraints.c3 = x(1)+3*x(2)+x(3)&lt;=12;[sol, fval, flag, out] = solve(prob), sol.x;fval, ans#---------------------------------------fval = 14.5714ans = 6.4286 0.5714 0 设置 optimproblem('Type','integer') 可以解决整数的线性规划问题 遗传算法(Genetic Algorithm) 在上文的基础上还可以求解非线性整数线性规划 遗传算法通常实现方式为一种计算机模拟。对于一个最优化问题, 一定数量的候选解 (称为个体) 可抽象表示为染色体, 使种群向更好的解进化。传统上, 解用二进制表示 (即 0 和 1 的串), 但也可以用其他表示方法。进化从完全随机个体的种群开始, 之后一代一代发生。在每一代中评价整个种群的适应度, 从当前种群中随机地选择多个个体 (基于它们的适应度), 通过自然选择和突变产生新的生命种群, 该种群在算法的下一次迭代中成为当前种群。 在遗传算法里, 优化问题的解被称为个体, 它表示为一个变量序列, 叫做染色体或者基因串。染色体一般被表达为简单的字符串或数字符串, 不过也有其他的依赖于特殊问题的表示方法适用, 这一过程称为编码。首先, 算法随机生成一定数量的个体, 有时候操作者也可以干预这个随机产生过程, 以提高初始种群的质量。在每一代中, 都会评价每一个体, 并通过计算适应度函数得到适应度数值。按照适应度排序种群个体, 适应度高的在前面。这里的“高”是相对于初始的种群的低适应度而言。 下一步是产生下一代个体并组成种群。这个过程是通过选择和繁殖完成, 其中繁殖包括交配 (crossover, 在算法研究领域中我们称之为交叉操作) 和突变 (mutation)。选择则是根据新个体的适应度进行, 但同时不意味着完全以适应度高低为导向, 因为单纯选择适应度高的个体将可能导致算法快速收敛到局部最优解而非全局最优解, 我们称之为早熟。作为折中, 遗传算法依据原则: 适应度越高, 被选择的机会越高, 而适应度低的, 被选择的机会就低。初始的数据可以通过这样的选择过程组成一个相对优化的群体。之后, 被选择的个体进入交配过程。一般的遗传算法都有一个交配概率 (又称为交叉概率), 范围一般是0.6~1, 这个交配概率反映两个被选中的个体进行交配的概率。例如, 交配概率为0.8, 则80%的“夫妻”会生育后代。每两个个体通过交配产生两个新个体, 代替原来的“老”个体, 而不交配的个体则保持不变。交配父母的染色体相互交换, 从而产生两个新的染色体, 第一个个体前半段是父亲的染色体, 后半段是母亲的, 第二个个体则正好相反。不过这里的半段并不是真正的一半, 这个位置叫做交配点, 也是随机产生的, 可以是染色体的任意位置。再下一步是突变, 通过突变产生新的“子”个体。一般遗传算法都有一个固定的突变常数 (又称为变异概率), 通常是0.1或者更小, 这代表变异发生的概率。根据这个概率, 新个体的染色体随机的突变, 通常就是改变染色体的一个字节 ( 0 变到 1 , 或者 1 变到 0 )。 经过这一系列的过程 (选择、交配和突变), 产生的新一代个体不同于初始的一代, 并一代一代向增加整体适应度的方向发展, 因为总是更常选择最好的个体产生下一代, 而适应度低的个体逐渐被淘汰掉。这样的过程不断的重复: 评价每个个体, 计算适应度, 两两交配, 然后突变, 产生第三代。周而复始, 直到终止条件满足为止。 以上内容没有任何意义, 你只要会掉包就行 ga 函数: x = ga(fitnessfcn,nvars,A,b,[],[],LB,UB,nonlcon,IntCon,options) fitnessfcn 为适应目标函数 nvars 为目标函数自变量的个数 \\(A\\) 与 \\(b\\) : 线性不等式约束 $A * x b $ 中的矩阵 \\(Aeq\\) 与 \\(beq\\) : 线性等式约束 $Aeq * x = beq $ 中的矩阵 nonlcon: 非线性约束, 该函数返回两个输出, 即: [g,h] = nonlcon(x) , g 为非线性不等式约束, 所有的不等式约束均以列向量的形式存在变量 g 中。h为非线性等式约束, 也以列向量的形式存储 LB UB 为下界和上界 IntCon: 若变量 x 中存在整数变量, 则在这里设置, 指定这些下标的变量都是整数变量 options: 调整种群规模, 迭代次数等 \\[ 求解下列问题: max\\;z = x_1^2+x_2^2+3x_3^2+4x_4^2+2x_5^2-8x_1-2x_2-3x_3-x_4-2x_5,\\newline \\] \\[ s.t.= \\left\\{\\begin{matrix} 0 \\leq x_i \\leq 99, 且 x_i 为整数(i=1,2,\\cdots, ),\\\\ x_1+x_2+x_3+x_4+x_5 \\leq 400, \\\\ x_1 +2x_2+2x_3+x_4+6x_5 \\leq 800, \\\\ 2x_1+x_2+6x_3 \\leq 200, \\\\ x_3+x_4+5x_5 \\leq 200. \\end{matrix}\\right. \\] \\[ sol.\\;构造矩阵 A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 2 &amp; 1 &amp; 6 \\\\ 2 &amp; 1 &amp; 6 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 5 \\end{bmatrix} , b = \\begin{bmatrix} 400 \\\\ 800 \\\\ 200 \\\\ 200 \\end{bmatrix} , 有 Ax \\leq b \\] 1234567891011clc; clear;c1 = [1,1,3,4,2]; c2=[-8,-2,-3,-1,-2]; obj = @(x) -sum(c1.*x.^2 + c2.*x); % 目标函数取反, 因为默认求最小值a = [1,1,1,1,1;1,2,2,1,6 2,1,6,0,0;0,0,1,1,5]; b = [400,800,200,200]';[x,f,flag,out] = ga(obj,5,a,b,[],[],zeros(1,5),99*ones(1,5),[],[1:5]);x, f#-------------------------------x = 50 99 0 99 20f = -51568 #ans = 51568 ga(obj,5,a,b,[],[],zeros(1,5),99*ones(1,5),[],[1:5]) 调用遗传算法 ga 来求解优化问题。 具体参数含义如下： obj: 目标函数，表示要优化的目标。 5: 决策变量的数量，即 x 向量的长度为 5。 a: 线性约束的系数矩阵。 b: 线性约束的右侧值。 []: 这两个空的方括号表示没有等式约束（默认为空）。 zeros(1,5): 决策变量的下限，这里是 [0, 0, 0, 0, 0]，即所有变量的最小值为 0。 99*ones(1,5): 决策变量的上限，这里是 [99, 99, 99, 99, 99]，即所有变量的最大值为 99。 []: 不指定选择交叉和变异的具体方式。 [1:5]: 遗传算法中每个决策变量都是整数变量。 [x,f,flag,out] = ... 使用遗传算法并将其返回结果赋值给四个输出变量： x: 最优解，即求解的决策变量的值。 f: 最优解对应的目标函数值。 flag: 结束条件的标志，指示算法结束的原因。 out: 包含优化过程的其他信息，如迭代次数、每代最好的适应度等。 \\[ eg. 求 \\; x+10sin(5x)+7cos(4x) \\; 的最大值 \\] 1234567clc; clear;obj = @(x) -(x+10*sin(5*x)+7*cos(4*x)); [x,f,flag,out] = ga(obj,1,[],[],[],[],5,10,[],[]);x, f%--------------------------------x = 7.8567f = -24.8554 % ans = 24.8554 蒙特卡洛法 应用: 算面积、超越积分数值计算、不规则图形面积、风险评估 unifrnd(): r = unifrnd(a,b) 从具有下部端点 a 和上部端点 b 的连续均匀分布中生成一个随机数 r = unifrnd(a,b,sz) 生成一个均匀 [a,b] 间随机数数组，其中大小向量 sz 指定 size(r) 123456sz = [2 3];r2 = unifrnd(0,1,sz)%--------------------------------r2 = 0.0975 0.5469 0.9649 0.2785 0.9575 0.1576 计算 \\(y = x^2\\) 和 \\(y = 12-x\\) 与 \\(x\\) 轴在第一象限围成的曲边三角形的面积 123456clc; clear; n=10^8;x = unifrnd(0,12,[1,n]); y = unifrnd(0,9,[1,n]);frequency = sum(y&lt;x.^2 &amp; x&lt;=3) + sum(y&lt;12-x &amp; x&gt;=-3);area = 12*9*frequency/n%--------------------------------area = 76.5024 计算 \\(y = sin(x^2), y = \\frac{sin(x)}{x}\\) 和 \\(y = e^{-x^2}\\) 在 \\(x \\in [0,2]\\) 上的面积 12345678910clc; clear; n=10^6; hold on;y1 = @(x) sin(x.^2);y2 = @(x) sin(x)./x;y3 = @(x) exp(-(x.^2)); x = unifrnd(0,2,[1,n]);y = unifrnd(0,1,[1,n]);frequency = sum(y&gt;=y3(x) &amp; y&lt;=y2(x) &amp; y&lt;=y1(x))area = 1*2*frequency/n%--------------------------------area = 0.4190 图论绘图 12345678910111213141516clc; clear;a1 = zeros(6); a1(1,[2:4])=1; a1(2,[3,6])=1; a1(3,4)=1; a1(4,5)=1; a1(5,6)=1;s = cellstr(strcat('v',int2str([1:6]')));G1 = graph(a1,s,'upper');plot(G1,'Layout','circle','NodeFontSize',12)a2 = zeros(6);a2(1,[2:4])=[7,3,12]; a2(2,[3,6])=1; a2(3,4)=8; a2(4,5)=9; a2(5,6)=3;G2 = graph(a2,s,'upper'); figure;plot(G2,'Layout','circle','EdgeLabel',G2.Edges.Weight)a3 = zeros(6);a3(1,3)=3; a3(2,[1,3])=[7,1]; a3(3,4)=8; a3(4,1)=12; a3(5,[4,6])=[9,3]; a3(6,2)=1;G3 = digraph(a3,s); figure;plot(G3,'EdgeLabel',G3.Edges.Weight,'Layout','circle') 旅行商问题(TSP) 用图论的术语说, 就是在一个赋权完全图中, 找出一个有最小权的 Hamilton 圈。称这种圈为最优圈 一个可行的办法是首先求一个 Hamilton 圈 C, 然后适当修改 C 以得到具有较小权的另一个 Hamilton 圈。修改的方法叫做改良圈算法: 设初始圈 \\(C = v_1v_2 \\cdots v_nv_1\\) 对于 \\(1 \\leq i &lt; i+1 &lt; j \\leq n\\), 构造新的 Hamilton 圈 \\[ C_{ij} = v_1v_2 \\cdots v_i \\underline{v_j v_{j-1} v_{j-2} \\cdots v_{i+1}}_{(reverse)} v_{j+1} v_{j+2} \\cdots v_n v_1 \\] 它是由 \\(C\\) 中删去边 \\(v_iv_{i+1}\\) 和边 \\(v_jv_{j+1}\\) , 添加边 \\(v_iv_j\\) 和边 \\(v_{i+1}v_{j+1}\\) 得到的。若: \\[ w(v_iv_j) + w(v_{i+1}v_{j+1}) &lt; w(v_iv_{i+1}) + w(v_jv_{j+1}), \\] 则以 \\(C_{ij}\\) 代替 \\(C\\) , \\(C_{ij}\\) 叫做 \\(C\\) 的改良圈 重复以上过程, 在无法改进后停止算法, 可以选择不同的初始圈多进行几次得到更好结果 eg. 对下图六城市找最小权的 Hamilton 圈 L M N Pa Pe T L 56 35 21 51 60 M 56 21 57 78 70 N 35 21 36 68 68 Pa 21 57 36 51 61 Pe 51 78 68 51 13 T 60 70 68 61 13 有初始圈 5-&gt;1-&gt;2-&gt;3-&gt;4-&gt;6-&gt;5, 长度为 274, 代码见下: 123456789101112131415161718192021222324252627282930313233343536373839clc, clear; % 旅行商问题简单来说就是计算出发地经过若干地点再返回原地的最短路径% 这个程序围绕数学建模书 59 页问题,从地点 5 开始出发，最后再回到地点 5a=zeros(6); % 初始化邻接矩阵a(1,2)=56; a(1,3)=35; a(1,4)=21; a(1,5)=51; a(1,6)=60;a(2,3)=21; a(2,4)=57; a(2,5)=78; a(2,6)=70; a(3,4)=36; a(3,5)=68; a(3,6)=68;a(4,5)=51; a(4,6)=61; a(5,6)=13; % 对邻接矩阵进行赋值a=a+a';% 利用对称矩阵的性质获得最终得邻接矩阵L=size(a,1); % 获取矩阵的行数，如果是size(a,2)就是获取矩阵的列数c=[5 1:4 6 5]; % 这样写是为了方便，实际上c=[5 1 2 3 4 6 5],这个定义的是初始圈[circle,long]=modifycircle(a,L,c); % 调用下面的修改圈的子函数circle, long % 注意如果在变量名后面加上分号的话就不会在窗口显示它的值，如果想显示值就不要加分号function [circle,long] = modifycircle(a,L,c) for k=1:L % 因为要走过六个地方，于是要遍历循环 flag=0; % 预先设置是否知道已经修改了边,如果循环后flag还是0，说明并没有修改初始圈 long=0; % 初始化总路程 for m=1:L-2 % 因为算法中的i是1&lt;=i&lt;i+1&lt;j&lt;=n,i与n最少差2 for n=m+2:L % 原算法中的j是至少比i大2的 if a(c(m),c(n))+a(c(m+1),c(n+1))&lt;a(c(m),c(m+1))+a(c(n),c(n+1)) % 如果修改后的新圈比旧圈权重小 c(m+1:n)=c(n:-1:m+1); % 将原来的路线倒过来，结合上面讲解的算法 flag=flag+1; % 如果修改了圈就将flag的值加1 end end end if flag==0 % 如果循环结束后都没有修改圈的话 long=274 % 那么所走路线就是初始圈，路程就是初始路线的总路程 else for i=1:L long=long+a(c(i),c(i+1)) % 遍历更新总路程 end circle=c; % 更新路线圈 return % 在条件块（例如 if 或 switch）或循环控制语句（例如 for 或 while）使用 return 时需要小心。当 MATLAB 到达 return 语句时，它并不仅是退出循环，还退出脚本或函数，并将控制权交还给调用程序或命令提示符。 end endend%-------------------------------circle = 5 4 1 3 2 6 5long = 211 你也可以用 0-1 整数规划模型解决 TSP 问题, 只要引进这样的 0-1 变量: \\[ x_{ij} = \\left\\{\\begin{aligned} 1, \\;\\;&amp;当最短路径经过\\;i\\;到\\;j\\;的边时\\\\ 0, \\;\\;&amp;当最短路径不经过\\;i\\;到\\;j\\;的边时 \\end{aligned}\\right. \\] 然后目标是要求一个 \\(min \\; z = \\sum\\limits_i\\sum\\limits_j w_{ij} x_{ij}\\) 这样的东西, 主要约束 \\(x_{ij}\\) 插值 拉格朗日插值和牛顿插值只有理论上意义, 实际应用不大, 故略去 分段线性插值 直接把 x, y 散点拿 plot 一画就可以了, 略 三次样条插值 在给定两点间的表达式均为一个三次多项式, 在整个区间上有连续的二阶导数 用 spline 函数, s = spline(x,y,xq) 返回与 xq 中的查询点对应的插值向量 s。s 的值由 x 和 y 的三次样条插值确定。 pchip 分段三次插值 用多项式作插值函数, 随着插值节点(或插值条件)的增加, 插值多项式次数也相应增加, 高次插值不但计算复杂且往往效果不理想, 甚至可能会产生龙格振荡现象。 利用多项式对某一函数作近似逼近, 计算相应的函数值, 一般情况下, 多项式的次数越多, 需要的数据就越多, 而预测也就越准确。然而, 插值次数越高, 会产生插值结果越偏离原函数的现象, 这称为龙格振荡现象。 分段插值就没这问题了 1234567x = -3:3; y = [-1 -1 -1 0 1 1 1]; xq1 = -3:0.01:3;p = pchip(x,y,xq1);s = spline(x,y,xq1);plot(x,y,'o',xq1,p,'-',xq1,s,'-.')legend('Sample Points','pchip','spline','Location','SouthEast') griddedInterpolant 函数 我勒个超级插值函数 F = griddedInterpolant(X1,X2,...,Xn,V) 使用作为一组 n 维数组 X1,X2,...,Xn 传递的样本点的完整网格创建二维、三维或 N 维插值。V 数组包含与 X1,X2,...,Xn 中的点位置关联的样本值。每个数组 X1,X2,...,Xn 的大小都必须与 V 相同。 F = griddedInterpolant(___,Method) 指定插值方法: 'linear'、'nearest'、'next'、'previous'、'pchip'、'cubic'、'makima' 或 'spline' 你可以在上述任意语法中指定 Method 作为最后一个输入参量。 计算对应的函数值的使用格式为 vq = F(xq1,xq2,...,xqn) 12345678x = -5:0.8:5; y = x';z = sin(x.^2 + y.^2) ./ (x.^2 + y.^2);F = griddedInterpolant({x,y},z); %二维, (x,y) 点, z 为函数值xq = -5:0.1:5;yq = xq';vq = F({xq,yq}); % 把原来 0.8 的步长插值成 0.1 的surf(xq,yq,vq) 如果改为 F = griddedInterpolant({x,y},z,'cubic') , 能获得更好的三次插值 拟合 最小二乘法 先看线性的, 根据一些基础的线性代数知识, \\(\\mathbf{A^T Ax= A^T b}\\) 肯定有解, 故 \\(\\mathbf{x} = \\mathbf{(A^T A)^{-1} A^T b}\\) 为所求的拟合矩阵的系数, \\(f(x) = \\displaystyle \\sum a_ix^i\\) 我们还是直接套库, 就不自己写了 p = polyfit(x,y,n) 返回次数为 n 的多项式 p(x) 的系数, 该阶数是 y 中数据的最佳拟合 (基于最小二乘指标)。p 中的系数按降幂排列, p 的长度为 n+1，其中 \\(p(x) = p_1x^n + p_2x^{n-1} + \\dots + p_nx + p_{n+1}\\) y = polyval(p,x) 计算多项式 p 在 x 的每个点处的值 12345678x = [0:7];y = [27,26.8,26.5,26.3,26.1,25.7,25.3,24.8];p = polyfit(x,y,1);x1 = linspace(0,7);y1 = polyval(p,x1);hold on;plot(x,y,'o');plot(x1,y1); 若样本点不是线性的, 我们可以尝试用常见的非线性函数来拟合, 例如多项式、双曲线 \\(y= \\displaystyle \\frac{a_1}{x}+a_2\\)、指数曲线 \\(y = a_1 e^{a_2x}\\)​ 我们这次不用库了, 用工具箱！ Matlab - APP - Curve Fitting Tool 进入, Polynomial, Power, Sum of Sine, Logistic(S 型) 等多种拟合方式任君挑选 线性前提下, \\(R^2\\) 越接近 1 越好, 否则 SSE 越低越好 做题可以先把散点图画出来, 分段拟合 微分方程 解析解 调库，调库！ \\(eg. 解 \\;\\;y'' + 2y' +y-x^2 =0, y(0)=0, y'(0)=15\\) ​ 123y=dsolve('D2y+2*Dy+y-x^2=0','y(0)=0','Dy(0)=15','x')%---------------------------y = 13*x*exp(-x) - 6*exp(-x) - 4*x + x^2 + 6 其中 Dty 代表 \\(y^{(t)}\\), 格式为 y = dsolve('f1','f2',...,'x') 你也可以: 1234567syms y(x) % 定义符号变量, 说明 y 是 x 的函数eq = y - diff(y,x) == 2*x; % diff(y,x) = dy/dx = y'dsolve(eq) % 通解dsolve(eq,y(0)==3) % 特解%----------------------------ans = 2*x + C1*exp(x) + 2ans = 2*x + exp(x) + 2 \\[ eg.\\;求 \\left\\{\\begin{aligned} &amp;\\frac{dx}{dt} = y \\\\ &amp;\\frac{dy}{dt} = -x \\end{aligned}\\right. \\;\\;且 \\;x(0)=0, y(0)=1\\;的解 \\] 123456789syms x(t) y(t) % 定义两个符号变量eq1 = diff(x,t) == y;eq2 = diff(y,t) == -x;eqs = [eq1 eq2]; % 方程矩阵cond = [x(0)==0, y(0)==1]; % 条件矩阵[xsol(t),ysol(t)] = dsolve(eqs,cond)%------------------------------xsol(t) = sin(t)ysol(t) = cos(t) 求不出解析解, 就去求数值解, ode45, 启动！ 数值解 ode45 可以求解非刚性问题(函数变化缓慢) (刚性问题用 ode15s ) 格式为: [t,y] = ode45(odefun,tspan,y0) 其中 odefun 是用匿名函数定义 \\(f(t,y)\\)​ 的函数文件名(或匿名函数返回值), tspan = [t0 tf] 为一个积分区间, 奇妙的 matlab 会自动选择步长并求经过的点集的数值解, 你可以之后 plot 把图象画出来, y0 为初始条件, 长度与 odefun 的输出长度相同 ode45 只能解一阶微分方程, 高阶的要先拆成一阶微分方程组 eg. 解 \\(y_1'' - \\mu(1-y_1^2)y_1'+y_1 = 0,\\;其中\\; \\mu=1, y_1(0)=2, y_1'(0)=0\\) 设 \\(y_1' = y_2\\) 有 \\(y_1' = y_2, \\;y_2'=\\mu(1-y_1^2)y_2-y_1\\) 12345678910clc; clear; [x y] = ode45(@(x,y) df(x,y),[0,20],[2,0]); %调用时导参, 再套一层匿名函数plot(x,y(:,1),'-o',x,y(:,2),'-o')title('Solution of van der Pol Equation (\\mu = 1) with ODE45');xlabel('Time t'); ylabel('Solution y'); legend('y_1','y_2');function dydt = df(x,y) dydt = zeros(2,1); dydt(1) = y(2); dydt(2) = (1-y(1)^2)*y(2)-y(1);end 步长大概为 0.1 左右, 而且不固定, 真抽象啊 matlab 求微分方程 \\(y'=-2y+2x^2+2x,\\;y(0)=1, 0 \\leq x \\leq 0.5\\) 的数值解, 并在同一个界面上画出数值解和解析解的曲线 12345678clc, clear, close all, syms y(x)y = dsolve(diff(y) == -2*y+2*x^2+2*x, y(0)==1);dy = @(x,y) -2*y+2*x^2+2*x;[sx,sy] = ode45(dy,[0,0.5], 1);fplot(y,[0,0.5]); hold on;plot(sx,sy,'*'); legend({'符号解','数值解'})xlabel('$x$','Interpreter','latex');ylabel('$y$','Interpreter','latex','Rotation',0); % 不然字母 y 会旋转 人口模型 马尔萨斯人口模型 对于这个模型, 有假设: (1) 设 x(t) 表示 t 时刻的人口数, 且 x(t) 连续可微 (2) 人口的增长率 r 是常数 (增长率=出生率-死亡率) (3) 人口数量的变化是封闭的, 即人口数量的增加与减少只取决于人口中个体的生育和死亡, 且每一个体都具有同样的生育能力和死亡率 由假设, t 时刻到 t+Δt 时刻人口的增量为 x(t+Δt) -x(t) = rx(t)Δt, 有 dx = rx * dt, x(0) = x0, 解得 \\(x(t) = x_0e^{rt}\\) , 显然扯淡, 于是要修正 Logistic 模型 也称阻滞增长模型 在人口较少时, 可以把增长率 r 看成常数, 那么当人口增加到一定数量之后, 就应当视 r 为一个随着人口的增加而减小的量, 即将增长率 r 表示为人口 x(t) 的函数 r(x), 且 r(x) 为 x 的减函数, 有假设: (1) 设 \\(r(x)\\) 为 \\(x\\) 的线性函数, \\(r(x) = r-sx\\) (先假设为一次函数) (2) 自然环境所能容纳的最大人口数为 \\(x_m\\), 即当 \\(x=x_m\\) 时, 增长率 \\(r(x_m) = 0\\) 由假设, \\(\\displaystyle r(x) = r\\left(1 - \\frac{x}{x_m}\\right)\\) , 有 \\[ \\left\\{\\begin{aligned} &amp;\\frac{dx}{dt} = r\\left(1-\\frac{x}{x_m}\\right)x, \\\\ &amp;x(t_0) = x_0. \\end{aligned}\\right. \\] 解为 \\(x(t) = \\displaystyle \\frac{x_m}{\\displaystyle1+\\left(\\frac{x_m}{x_0}-1\\right)e^{-r(t-t_0)}}\\) 这个模型有一些特点, \\(x(t)\\) 单增, 变化率 \\(dx/dt\\) 在 \\(x = \\frac{x_m}{2}\\) 时最大, 总体就是生物的 \\(S\\) 型曲线 实际应用还是 Matlab - APP - Curve Fitting Tool - Logistic 拟合 或者你用 Custom Equation, 然后把算出来的 x(t) 填进去, 例如: 拟合的过程中能估计出来 \\(x_m\\) 和 \\(r\\)​, 得到了函数就可以拿来估计后续了 相互作用模型 种群竞争模型 略 互惠共存模型 略 弱肉强食模型(Lotka-Volterra模型) 此类问题广泛存在于自然界中, 如大鱼吃小鱼、狼群与羊群等 设 \\(t\\) 时刻第一个种群的数量和第二个种群的数量分别为 \\(x_1(t),\\; x_2(t)\\) , 初始种群数量为 \\(x_1^0, \\;x_2^0\\) 有一些基本假设: (1) 第一个种群的生物捕食第二个种群的生物, 其种群数量的变化除了自身受 Logistic 规律的制约外, 还受到被捕食的第二个种群的数量影响 (2) 第二个种群的数量变化除了自身受自限规律影响外, 还受其天敌第一个种群的数量影响。第二个种群的种群数量越多, 被捕杀的机会越多, 从而第一个种群的繁殖越快 (3) 设两个种群的自然增长率分别为 \\(r_1\\) 和 \\(r_2\\), 各自独自生存的生存极限数分别为 \\(K_1\\) 和 \\(K_2\\) 由假设, 有 \\(\\displaystyle \\frac{dx_1}{dt} = r_1\\left(1-\\frac{x_1}{K_1}\\right)x_1 + b_{12}x_1x_2, \\;\\; \\frac{dx_2}{dt} = r_2\\left(1-\\frac{x_2}{K_2}\\right)x_2 - b_{21}x_1x_2\\) 其中 \\(b_{12}, b_{21} &gt; 0\\), 为两个种群的接触系数, 并不一定相同, 再加上初始条件 \\(x_1(0)=x_1^0,\\;x_2(0)=x_2^0\\) 就可以开解微分方程了 eg. 已知 捕食者-被捕食者 方程组: \\[ \\left\\{\\begin{aligned} &amp;\\frac{dx}{dt} = 0.2x-0.005xy,\\;x(0)=70, \\\\ &amp;\\frac{dy}{dt} = -0.5y+0.01xy,\\;y(0)=40. \\end{aligned}\\right. \\] 式中: \\(x(t)\\) 表示 \\(t\\) 个月后兔子的总体数量, \\(y(t)\\) 表示 \\(t\\) 个月后狐狸的总体数量 请研究以下问题: (1). \\(x(t),y(t)\\) 变化的周期; (2). \\(x(t)\\) 总体数量的最值; (3). \\(y(t)\\) 总体数量的最值。 我们先令 \\(0.2x-0.005xy = -0.5y+0.01xy = 0\\), 解得临界点 \\((50,40)\\) ​ 上图是炫酷的向量场图象, x, y 就绕这个来变化 好像没什么用？还是画出 \\(x(t),\\;y(t)\\)​ 解的图象吧…… 需要用到一个函数: y = deval(sol,x) 可以计算 x 中包含的点处的微分方程问题的解 sol, y = deval(___,idx) 只返回带有向量 idx 中所列索引的解分量 可能还需要一个函数: x = fminbnd(fun,x1,x2) 返回一个值 x，该值是 fun 中描述的标量值函数在区间 x1 &lt; x &lt; x2 中的局部最小值 [x,fval] = fminbnd(___) 返回 x 与目标函数在 fun 的解 x 处计算出的值 12345678910111213141516171819202122clc; clear; close all; % 下文范围默认 [0,100]dxy = @(t,z) [0.2*z(1)-0.005*z(1)*z(2); -0.5*z(2)+0.01*z(1)*z(2)];sol = ode45(dxy,[0,100],[70;40]);xt = @(t)deval(sol,t,1); yt = @(t)deval(sol,t,2);hold on;fplot(xt,[0,100],'--','LineWidth',1.5);fplot(yt,[0,100],'LineWidth',1.5);xlabel('$t$','Interpreter','latex');legend({'$x(t)$','$y(t)$'},'Interpreter','latex');[xt1,fx1] = fminbnd(xt,0,100) % 求 x 的最小点及最小值[xt2,fx2] = fminbnd(@(t)-xt(t),0,100) % 求 x 的最大点及最大值[yt1,fy1] = fminbnd(yt,0,20) % 求 y 在 [0,20] 上的最小点及最小值[yt2,fy2] = fminbnd(yt,20,40) % 求 y 在 [20,40] 上的最小点及最小值T = yt2-yt1 % 求周期 (事先画图估计过在 20 左右)[yt3,fy3] = fminbnd(@(t)-yt(t),0,100) % 求 y 的最大点及最大值%-----------------------------------------------xt1, fx1 = 8.9150, 34.2513xt2, fx2 = 60.6487, -69.8810 % 取反后实为 69.8810yt1, fy1 = 14.9932, 21.5086yt2, fy2 = 35.2133, 21.5644T = 20.2201yt3, fy3 = 64.7762, -66.7664 % 取反后实为 66.7664 传染病模型(笔记待补) SI 模型和 Logistic 模型一样, 略 SIS 模型进一步假设了每天被治愈的病人占总人数的比例为 \\(\\mu\\), 以及痊愈的病人还能被感染 SIR 假设: (1) 人群分健康者、病人和病愈后因具有免疫力而退出系统的移出者三类。设任意时刻 t 这三类人群占总人口的比例分别为 \\(s(t), i(t)\\) 和 \\(r(t)\\) (2) 病人的日接触率为 \\(\\lambda\\), 日治愈率为 \\(\\mu\\), 传染强度 \\(\\sigma = \\lambda/\\mu\\) (3) 人口总数 N 为固定常数 总体方程组为: \\[ \\left\\{\\begin{aligned} &amp;\\frac{di}{dt} = \\lambda s i- \\mu i,\\\\ &amp;\\frac{ds}{dt} = -\\lambda s i, \\\\ &amp;\\frac{dr}{dt} = \\mu i, \\\\ &amp;i(0)=i_0, s(0)=s_0, r(0)=0 \\end{aligned}\\right. \\] K-Means 聚类算法 算法目标是将数据集划分为 K 个簇(Clusters), 最小化簇内的点到簇中心(centroid)的距离总和 主要还是调库, 核心代码就一行 [idx centroid] = kmeans(data, Clusternum) 在实际绘图中为了好看你还可以画出外层凸包 二维最清晰好看, 可惜只能有两个指标 123456789101112131415161718192021222324252627282930313233343536373839404142% 生成随机数据（三个簇）rng default; % 设置随机种子保证结果可重复data1 = randn(50, 2)*0.5 + [0, 0];data2 = randn(50, 2)*0.5 + [3, 3];data3 = randn(50, 2)*0.5 + [0, 3];X = [data1; data2; data3];% 应用K-means算法进行聚类k = 3; % 聚类数目[idx, centroids] = kmeans(X, k);% 分割数据到各个聚类clusters = cell(k, 1);for i = 1:k clusters{i} = X(idx == i, :);end% 计算每个聚类的凸包并绘图figure;hold on;colors = {'r', 'g', 'b'}; % 每个聚类的颜色for i = 1:k % 当前聚类的数据 currentCluster = clusters{i}; % 绘制数据点 scatter(currentCluster(:, 1), currentCluster(:, 2), [], colors{i}, 'filled'); % 计算凸包 hullIndices = convhull(currentCluster(:, 1), currentCluster(:, 2)); hullPoints = currentCluster(hullIndices, :); % 绘制凸包边界 plot(hullPoints(:, 1), hullPoints(:, 2), 'Color', colors{i}, 'LineWidth', 1.5); % 填充凸包区域，颜色半透明 fill(hullPoints(:, 1), hullPoints(:, 2), colors{i}, ... 'FaceAlpha', 0.1, 'EdgeColor', 'none'); % FaceAlpha 控制透明度end% 可选：绘制聚类中心scatter(centroids(:, 1), centroids(:, 2), 100, 'k', 'x', 'LineWidth', 2);title('K-means聚类结果及凸包（半透明填充）');xlabel('X1');ylabel('X2'); 代码由 deepseek 编写, 在 legend 方面可能有点问题, 我就直接去掉了","link":"/2025/02/06/Matlab/"},{"title":"EECS498 笔记","text":"课程视频 19fall 课程网站 课程时间表 19fall 教材 课程官方笔记(cs231n) 我的 Assignments 深度学习下的计算机视觉基础, 作为cs231n 的替代, 加大作业量 前置 Python 基础 (建议)配一个 GPU 版本的 Pytorch (建议)配一个 Jupyter Notebook 如果不行, 课程也推荐你使用 Google Colab 完成作业 Lecture 1 介绍 一些概念介绍: 计算机视觉 (Computer vision) 是一门研究如何使机器“看”的科学, 更进一步的说, 就是指用计算机代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图像处理 感知器 (Perceptron) 是弗兰克·罗森布拉特在1957年所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈神经网络, 是一种二元线性分类器。罗森布拉特给出了相应的感知机学习算法, 常用的有感知机学习、最小二乘法和梯度下降法。譬如, 感知机利用梯度下降法对损失函数进行极小化, 求出可将训练数据进行线性划分的分离超平面, 从而求得感知机模型。 业界主要用深度学习来做计算机视觉(从 2012 年的 Alexnet 开始) 深度学习是机器学习的一个子集 这节课的剩余部分讲了计算机视觉的发展历史 Lecture 2 图像分类 电脑看图像只是一个巨大的数字表格, 对图像的简单更改都可能会改变整个表格, 这是一个语义鸿沟(semantic gap) 图像分类有多个挑战: 视角变化(viewpoint variation) / 类内变化(intraclass variation) / 细粒度分类(fine-grained classification) / 背景干扰(background clutter) / 光照变化(illumination changes) / 变形(deformation) / 遮挡(occlusion) / etc. 图像分类是其他计算机任务的基石, 例如图像字幕(image captioning) 机器学习: 数据驱动的方法 收集数据 -&gt; 训练分类器 -&gt; 应用, 大体是下面这样的: 123456def train(images, labels): # Machine learning! return modeldef predict(model, test_images): # Use model to predict labels return test_labels 常见的图像分类数据集: MNIST: 0~9 的手写数字, 28x28 灰度图, 训练集大小 50k, 测试集大小 10k CIFAR10: 10 类物体, 32X32 RGB 图像, 训练集大小 50k, 测试集大小 10k ImageNet: 1000 类物体, 可处理为 256x256 图像, 训练集大小 1.3M, 测试集大小 100k, 模型要求给出五种分类, 有一种分类与图片分类相同视为成功 MIT Places: 365 类场景, 可处理为 256x256 图像, 训练集大小 8M, 测试集大小 328.5k Omniglot: 1623 类语言字符, 每类字符仅有 20 个例子, 用于 Low-shot learning 最简单的分类方法 K最近邻算法(k-Nearest Neighbor)(kNN) 将测试集和训练集的每个图像使用一个相似度函数比较并输出相似度最高的对应的标签 需要用到一些距离度量(distance metric), 例如像素值的曼哈顿距离, 然后加起来之类的 这种算法不能满足我们对于测试高性能的需要, 而且的确太简单了 另一种 K最近邻 思想的算法是构造出一种决策边界(decision boundaries) 这种方法很容易被离群值影响 简单的优化是找 K 个最近邻居去个众数, 使得决策边界更加平滑 距离度量也可以改为欧几里得距离使其更平滑 K最近邻算法 可视化网站 一些概念介绍: 在机器学习的上下文中, 超参数是在开始学习过程之前设置值的参数, 而不是通过训练得到的参数数据。通常情况下, 需要对超参数进行优化, 给学习机选择一组最优超参数, 以提高学习的性能和效果。 这里的 K, 以及距离度量函数就属于超参数(hyperparameter), 不要用训练集或测试集来设置超参数, 更好的想法是把数据分为三部分: 训练集、验证集和测试集; 我们用验证集来决定超参数的设置, 只在最后在测试集上测试一次 一种更更好的方法是交叉验证, 训练代价会很昂贵, 但效果最好 随着输入空间的维度增加, 这个算法需要的数据量会对应指数级增加 Assignment 1 Pytorch &amp; kNN Pytorch Pytorch 中数据的核心表现形式也是张量(tensor), 类似于 Numpy 的多维数组, 但具有更强大的功能, 例如支持 GPU 加速和自动梯度计算 张量的秩(rank)是数组的维度, 张量的形状(shape)是每一维的大小 我们直接以二维张量举例: 12345678910111213141516171819202122232425262728# 通过 python list 创建二维张量b = torch.tensor([[1, 2, 3], [4, 5, 5]])print('Here is b:')print(b)print('rank of b:', b.dim())print('b.shape: ', b.shape)# 通过二维下标访问张量print('b[0, 1]:', b[0, 1])print('b[1, 2]:', b[1, 2])# 修改二维张量b[1, 1] = 100print('b after mutating:')print(b)#---------------------------------------Here is b:tensor([[1, 2, 3], [4, 5, 5]])rank of b: 2b.shape: torch.Size([2, 3])b[0, 1]: tensor(2)b[1, 2]: tensor(5)b after mutating:tensor([[ 1, 2, 3], [ 4, 100, 5]]) 类似于 matlab, 有一些函数来很方便地创建特殊张量, 例如 torch.zeros torch.ones torch.eye 和 torch.rand, 分别创造全 0 张量, 全 1 张量, 单位矩阵和 [0,1] 随机数张量 例如 v = torch.full((2,3,4),7), 代表一个 shape 为 (2,3,4), 每一项都为 7 的张量 例如 x = torch.linspace(10,20,steps=6,dtype=torch.float64) , 代表 x = tensor([10, 12, 14, 16, 18, 20]) 且 x.dtype == torch.float64 例如 v = torch.arange(10), 代表 v = tensor([0,1,2,3,4,5,6,7,8,9]), 直接 [0:10] 简单, 但不是张量 例如 torch.randint(low=0, high, shape) 创建一个每个数为整数, 在 [low, high) 之间, 有 shape 大小的张量 张量有不同的数据类型, 可以通过 .dtype 来查看(eg. int16, int32, int64, float32, uint8, ...) 可以通过 .to(TYPE) 来转换类型, eg. b = a.to(torch.float32) 或直接通过 b = a.float() 来转换为 float32 张量数组也可以和 python list 一样用 : 来进行切片, 对原张量的切片相当于原张量切片部分上的指针, 要想弄一个新张量得用 .clone() 函数 我们可以用一个 index list 来重新排列张量, 或者给张量的一些不同位置元素同时赋值: 1234567891011121314151617181920212223242526272829303132333435a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])print('Original tensor:')print(a)idx = [0, 0, 2, 1, 1] # index arrays can be Python lists of integersprint('\\nReordered rows:')print(a[idx])idx = torch.tensor([3, 2, 1, 0]) # index arrays can be int64 torch tensorsprint('\\nReordered columns:')print(a[:, idx])idx = [0, 1, 2]print('\\nReordered columns:') print(a[idx, idx]) # [a[0, 0], a[1, 1], a[2, 2]]#--------------------------------Original tensor:tensor([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]])Reordered rows:tensor([[ 1, 2, 3, 4], [ 1, 2, 3, 4], [ 9, 10, 11, 12], [ 5, 6, 7, 8], [ 5, 6, 7, 8]])Reordered columns:tensor([[ 4, 3, 2, 1], [ 8, 7, 6, 5], [12, 11, 10, 9]])Reordered columns:tensor([ 1, 6, 11]) 在做切片的时候还可以用数学表达式来约束 123456789a = torch.tensor([[1,2], [3, 4], [5, 6]])a[a &lt;= 3] = 0print('\\nAfter modifying with a mask:')print(a)#-------------------------After modifying with a mask:tensor([[0, 0], [0, 4], [5, 6]]) 例: x 张量中小于 0 的元素数量统计 torch.sum(x&lt;0).item() 在 Pytorch 中 view() 函数可以调整张量的 shape, 只要前后数据个数一样即可, 例如 a = a.view(2,2) 可以将 a 调整为 2x2 的张量, a = a.view(-1) 会把整个 a 张量展平为一维张量, view 并不是创建新张量, 其本质仍是一种浅拷贝, reshape() 则是深拷贝, 有些情况下 view() 会报错, 因为view() 要求 shape 满足一定的条件 如果想要将数据降维, 可以用 flatten 方法, 格式为 torch.flatten(input,start_dim=0,end_dim=-1) transpose() 方法可以交换张量的两个维度, 格式为 torch.transpose(input, dim0, dim1) 或 input.transpose(dim0, dim1) 若想调换更多维度, 就用 permute 函数, 格式为 permute(dims), 一次换所有维度 若只想转置二维矩阵, 就用 .t(), 交换张量的第 0 维和第 1 维 张量支持加减乘除、幂次、开平方、三角函数等运算, 运算都针对张量中的每个元素来对应进行 一些其他运算 sum()/mean()/min()/max() 可以针对整个张量或某几个维度进行, 可以返回值以及对应下标(如果存在) 一些特殊运算函数(如 mean()) 会压缩张量维度, 可以设置 keepdim = True 来避免: 123456x = torch.randn(128, 10, 3, 64, 64)x = x.mean(dim=1) # 对维度 1 求平均; shape = (128, 3, 64, 64)x = x.sum(dim=2) # 对维度 2 求和; shape = (128, 3, 64)x = x.mean(dim=1, keepdim=True)# 对维度 1 求平均, 但通过设置 keepdim=True 保存维度 1; shape = (128, 1, 64)print(x.shape) 针对线性代数中矩阵的运算有另外的函数, 例如: torch.dot: 计算向量的点积, 只对一维张量生效 torch.mm: 计算矩阵之间的乘法 torch.mv: 计算矩阵与向量的乘法 torch.addmm / torch.addmv: 两个矩阵或矩阵与向量相乘，并将结果与第三个矩阵相加 torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None) → Tensor, 即 \\(out = \\beta \\;mat\\;+\\;\\alpha(mat1_i \\; @ \\; mat2_i)\\), 这里的 @ 指矩阵乘法 torch.bmm / torch.baddmm: 进行批量的矩阵乘法, 函数接受三维张量输入, 并将第一维视为矩阵个数, 后面两位视为矩阵的行数与列数 如果两个张量 shape 分别为 (n,1) 和 (1,m), 那样 * 运算会做矩阵乘法, 输出 shape 为 (n,m) 的张量 \"广播\"(broadcasting) 用于描述如何在形状不一的张量上应用算术运算, 在满足特定限制的前提下, 较小的张量\"广播至\"较大的张量, 使两者形状互相兼容, 规则要求: 每个张量至少有一个维度 迭代维度尺寸时, 从尾部的维度开始, 维度尺寸或者相等, 或者其中一个张量的维度尺寸为 1, 或者其中一个张量不存在这个维度 你可以使用 .to('cuda') 或者 .cuda() 指令将张量放到 gpu 上运算, 特定情形下速度会快上几十倍, 不知为什么我的电脑在有的任务上反而性能倒挂...... kNN 分类器 这部分为正式作业, 写一个在 CIFAR-10 上的 kNN 分类器 在本地安装环境的话缺啥安啥就行, google colab 相关的有些包改名了全部取消 import 测试集和训练集的 shape 为 (numbers, channels, height, weight), 假设我们取训练集的大小为 500, 则 shape 为 (500, 3, 32, 32) (RGB 图像的通道数为 3) 接下来我们要计算一个 shape 为 (train, test) 的距离张量 dists, 使用欧几里得距离度量, 尽量不要写二重循环因为 python 的 for 循环慢的要死, 我们应该把数据点集合为\"向量\"一起并行运算, 仅使用一重循环, 甚至不循环来提高效率, 这就需要用到\"广播\"的思想 二重循环 一重循环 不循环 1234567891011# x_train.shape == x_test.shape == (100,3,16,16)def compute_distances_one_loop(x_train, x_test): # 二重循环 num_train = x_train.shape[0] num_test = x_test.shape[0] dists = x_train.new_zeros(num_train, num_test) def compute(A, B): return torch.sqrt(torch.sum(torch.pow(A-B,2))) for i in range(num_train): for j in range(num_test): dists[i, j] = compute(x_train[i], x_test[j]) return dists 1234567891011# x_train.shape == x_test.shape == (100,3,16,16)def compute_distances_one_loop(x_train, x_test): # 一重循环 num_train = x_train.shape[0] num_test = x_test.shape[0] dists = x_train.new_zeros(num_train, num_test) def compute(A, Bs): # A 会广播来适应 Bs return torch.sqrt(torch.sum(torch.pow(A-Bs,2), dim=[1,2,3])) #keep first dim as number for i in range(num_train): dists[i,:] = compute(x_train[i],x_test) return dists 123456789# x_train.shape == x_test.shape == (100,3,16,16)def compute_distances_no_loops(x_train, x_test): # 不循环 num_train = x_train.shape[0] num_test = x_test.shape[0] dists = x_train.new_zeros(num_train, num_test) x_train = x_train.unsqueeze(1) # shape = ([100, 1, 3, 16, 16]) x_test = x_test.unsqueeze(0) # shape = ([1, 100, 3, 16, 16]) dists = torch.sqrt(torch.sum(torch.pow(x_train-x_test,2), dim=[2,3,4])) return dists 理论上来讲不循环最快但实际上我的单循环更快, 在 shape 均为 (500,3,32,32) 时, 我的双循环/单循环/无循环的用时为: 3.92s/0.1s/0.61s 计算完 dists 后要输出预测标签, pytorch 中有简单的 topk() 函数, 格式为 val, idx = torch.topk(input, k, dim=None, largest=True, sorded=True) , 其中 dim 为要操作的维度, largest 表示返回最大或最小值, sorted 决定返回结果是否按顺序排列, 函数返回值张量与索引张量 123456789def predict_labels(dists, y_train, k=1): num_train, num_test = dists.shape y_pred = torch.zeros(num_test, dtype=torch.int64) for i in range(num_test): _, idxs = torch.topk(dists[:,i], k, largest=False) tmp = y_train[idxs] val, _ = torch.mode(tmp) y_pred[i] = val return y_pred 对于 CIFAR-10数据集, 在 k=1 时准确率有 27% 左右, 在 k=5 时会高上 0.4% 我们可以通过交叉验证来设置超参数 k, 把训练集分成多个集合, 每次选一个作为验证集, 剩下的为训练集, 然后在每个 k 上都跑一遍, 需要用到两个函数, torch.chunk(input, chunks, dim = 0) 将输入张量 (input) 沿着指定维度 (dim) 均匀的分割成特定数量的张量块 (chunks), 并返回元素为张量块的元组(tuple)以及 torch.cat(tensors, dim=0) 可以沿着维度拼接张量 现在我们来对不同的 k 做一次交叉验证: 1234567891011121314def knn_cross_validate(x_train, y_train, num_folds=5, k_choices=None): if k_choices is None: # Use default values k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100] x_train_folds = list(torch.chunk(x_train,num_folds)) # tuple to list y_train_folds = list(torch.chunk(y_train,num_folds)) k_to_accuracies = {} for k in k_choices: # 对于每个 k k_to_accuracies[k] = [] for i in range(num_folds): # 选下标为 i 的为验证集, 其他为训练集 x_in = torch.cat([x_train_folds[id] for id in range(num_folds) if id != i]) y_in = torch.cat([y_train_folds[id] for id in range(num_folds) if id != i]) classifier = KnnClassifier(x_in,y_in) # 对于训练集构建分类器 acc = classifier.check_accuracy(x_train_folds[i],y_train_folds[i],k) k_to_accuracies[k].append(acc) # 获得准确度 我们跑一遍: 12345num_train, num_test = 5000, 500x_train, y_train, x_test, y_test = coutils.data.cifar10(num_train, num_test)k_to_accuracies = knn_cross_validate(x_train, y_train, num_folds=5)for k, accs in sorted(k_to_accuracies.items()): print('k = %d got accuracies: %r' % (k, accs)) 可以得到: 12345678910k = 1 got accuracies: [26.3, 25.7, 26.4, 27.8, 26.6]k = 3 got accuracies: [23.9, 24.9, 24.0, 26.6, 25.4]k = 5 got accuracies: [24.8, 26.6, 28.0, 29.2, 28.0]k = 8 got accuracies: [26.2, 28.2, 27.3, 29.0, 27.3]k = 10 got accuracies: [26.5, 29.6, 27.6, 28.4, 28.0]k = 12 got accuracies: [26.0, 29.5, 27.9, 28.3, 28.0]k = 15 got accuracies: [25.2, 28.9, 27.8, 28.2, 27.4]k = 20 got accuracies: [27.0, 27.9, 27.9, 28.2, 28.5]k = 50 got accuracies: [27.1, 28.8, 27.8, 26.9, 26.6]k = 100 got accuracies: [25.6, 27.0, 26.3, 25.6, 26.3] 将所有的 k 的准确率平均值作比较, 我们就可以选出最好的 k 为 k=10 在整个测试集上跑了十五分钟后, 最后的准确率为 33.86% Lecture 3 线性分类器 线性分类器是很多大型神经网络的基础 核心思想是一个函数 \\(f(x,W)\\), \\(x\\) 为图像, \\(W\\) 为权重, 形状为 (C, D), 其中 C 为分类数, D 为图像 \\(x\\) 的维度 对于 CIFAR-10 而言, \\(f\\) 返回十个分类的分数 一般来讲, 有 \\(f(x,W) = Wx + b\\), \\(b\\) 是一个偏移量(bias), 可以增加拟合能力(不然固定过定点), 可以并入 \\(W, x\\) 中 从代数角度理解, 这就是矩阵与向量相乘后加上了一个向量作为偏移量, 如果不考虑偏移量, \\(f(cx,W) = W(cx) = c \\times f(x,W)\\), 代表缩小图像会成比例改变分数, 但结果不变 从视觉角度理解, 我们抽出代表每个分类的 \\(W\\) 部分, 例如 \\(W_{cat}, W_{dog},...\\), 仅考虑这一部分与图像的作用, 线性分类器其实是在试图学习每类图像得分最高的\"模板\", 在 CIFAR-10 中是这样的: 这给了我们一种关于分类器的直觉, 比如说森林里的汽车可能因为背景的绿色更容易被分类为鹿或者是鸟 线性分类器只能对于一个类学习一种模板, 在上图中为了匹配不同模式的马, 它不得不构造一种类似双头马的模板, 无法很好应对类内变化 从几何角度理解, 你可以想象线性分类器是高维的一个超平面切割这个高维空间, 以二维(两个像素为例): 此时的分类器就是一条直线, 垂直于直线方向移动可以最快地使得图片趋向/远离某种分类 线性分类器无法解决异或问题, 你可以把整个异或问题想象为一个这样的平面, 显然无法找到一条直线划分两个类别(0/1) 我们如何确定权重 \\(W\\) ？ (1). 用损失函数(loss function)量化 \\(W\\) 的好坏程度 (2). 通过优化(optimization), 找到最小损失的 \\(W\\) 损失函数(loss function), 也叫目标函数(objective function), 奖励函数(reward function), etc. 对于数据点 \\(x_i\\) 与标签 \\(y_i\\), 其损失为 \\(L_i(f(x_i,W),y_i)\\), 总损失为 \\(L = \\displaystyle\\frac{1}{N} \\sum L_i(f(x_i,W),y_i)\\) SVM Loss 函数: 核心思想就是正确的类的 loss 应该最小, 设 \\(s = f(x_i, W)\\) 为得分(s = Wx(+b)), 则 \\(L_i = \\sum_{j \\neq y_i} {\\rm max}(0,s_j-s_{y_i}+ \\Delta)\\) , 针对不对的类别 \\(j\\) 有一个 loss, 至少比正确类型的得分小 \\(\\Delta\\) 例如在这里, frog 类的得分并不是 frog 中最高的, 那么别的类多出来的得分 +1 将成为损失的一部分 总的 loss 为 (2.9 + 0 + 12.9)/3 = 5.27 如果初始的分数全部随机, \\(\\sum L_i \\approx (N-1)\\Delta\\) ​ 如果找到了一个 \\(W\\), 其对应的 \\(L = 0\\), 则 \\(nW\\) 的 \\(L\\) 也会为 0 这两个 \\(W\\) 矩阵我们应该选哪个呢？这引入了正则化(regularization)的想法: 给损失函数一些限制, 通过某种规则去规范它们防止在接下来的循环迭代中过拟合, 或者加入一些额外偏好(偏向选择某个 \\(W\\) 矩阵) 一般来讲你要设置一些超参数 \\(\\lambda\\) , \\(\\lambda R(W)\\) 即为加在 \\(L\\) 后面的正则项 一个小例子, L2 正则(欧几里得)中 \\(R(W) = \\sum_k \\sum_l W_{k,l}^2\\) 这时候若 \\(w_1=[1,0,0,0], w_2=[0.25,0.25,0.25,0.25]\\), 则我们会优先选择 \\(w_2\\)​ 交叉熵损失(cross-entropy loss): 什么是分数? 我们可以在这里将分数解释为概率分布 Softmax 函数将分数变成概率, 设分数为 \\(s\\), 则 \\({\\rm Softmax}(s) = \\displaystyle\\frac{e^{s}}{\\sum e^{s_i}}\\) , 先取 exp 再标准化, 保证了概率和为 1 我们再定义 loss, \\(L_i = -{\\rm log}({\\rm Softmax}(s_i))\\), 这种定义和最大似然估计有关, 这里的 log 指 ln 这种 loss 函数会让 \\(L_i = 0\\) 变得几乎不可能, 任何微扰都能改变 \\(L_i\\), 如果初始的分数全部随机, \\(\\sum L_i = -log(N)\\) Lecture 4 最优化 感谢 ZEEKANG - 知乎 与 呼姆奴库 - 知乎 的笔记与解析 最优化(Optimization): 本质是这个式子 \\(w^{*} = {\\rm arg\\;min}_wL(w)\\), 即寻找能取到 \\({\\rm min} \\; L(w)\\)​ 的 参数(argument) 简单的方法: 随机猜 聪明的方法: 沿着损失函数的梯度(gradient)反方向迭代更新, 因为 梯度(在每个维度上偏导数所形成的向量) 表示的是函数在某一点的最陡上升方向, 所以沿着梯度的反方向移动可以减少损失函数的值, 从而优化模型 这里面有三个超参数: 初始化权重 迭代次数 对这个梯度的学习程度(learning rate), 因为计算出的梯度是局部的 聪明方法告诉我们 \\(W = W - \\eta \\nabla_WL\\), 其中 \\(\\eta\\) 表示学习程度, \\(\\nabla\\) 代表梯度算子(gradient operator), 表示对多元函数在各个坐标方向上的偏导数构成的向量, 在实践中我们用反向传播算法(backpropagation)来算梯度(见 Lecture 6), 这种梯度叫做解析梯度(analytic gradient), 数值梯度一般仅用来验证, 下面是实现的大体模板: 12345w = initialize_weights()# 标准些的梯度下降会有一些方法找到停止迭代的时机, 对于简单的实现, 只需要迭代固定次数即可for t in range(num_steps): dw = compute_gradient(loss_fn, data, w) w -= learning_rate * dw 假设我们已经有方法计算单个输入的 loss 值的梯度，那么总梯度为: \\[ \\nabla_WL(W) = \\frac{1}{N}\\sum\\limits_{i=1}^{N} \\nabla_W L_i(x_i,y_i,W) + \\lambda \\nabla_W R(W) \\] 随着 \\(N\\) 的变大, 这种开销无法承受, 在实践中我们一般使用随机梯度下降(Stochastic Gradient Descent)(SGD), 随机选择一小批样本(batch)来计算出近似梯度, 所以现在又增加了两个超参数: 批大小(batch size) 和 采样方式(data sampling) batch size 一般选择 32/64/128, 越大越慢也越好 SGD 的缺陷 第一种问题是, 如果数据在一个方向维度上变化太大，而在另一个方向维度上变化太小时, 步长设置过大会过冲震荡, 步长设置过小时会收敛缓慢, 这种问题和黑塞矩阵(Hessian matrix)的特征值比率有关, 我们在此略去 第二种问题是鞍点(saddle point), 也和黑塞矩阵有关, 在函数的黑塞矩阵在梯度为零时特征值有正有负称为鞍点 此时 SGD 将局部最小值当全局最小值了 第三个问题是, 因为我们使用的是数据集的一部分, 所以估计出的梯度可能噪音比较大(乱抖) 所以我们要给 SGD 加上动量(momentum)优化, 使用物理思想, 建立一个速度的概念, 对梯度平滑处理 动量的使用可以帮助优化算法跳出局部最小值或鞍点。在这些区域, 纯 SGD 可能会陷入停滞, 因为梯度接近 0。但是, 如果有足够的累积动量, 算法可能会由于之前的\"惯性\"而跨过这些最小值, 达到更低的损失区域 动量因子 \\(\\rho\\) (通常设置为接近 1 的值, 如 0.9 或 0.99) 决定了过去动量的保留程度。较高的 \\(\\rho\\) 值意味着保留更多的过去信息, 可以在一定程度上加快学习速度 在此基础上, 我们可以继续优化, 不使用当前点的梯度与速度合成, 而是直接\"展望\"速度向量指向点的梯度来更新, 可以帮助我们更好地调整方向, 这称为 Nesterov momentum 数学推导是这样的: \\[ \\begin{aligned} &amp;\\tilde x_t = x_t + \\rho v_t\\\\ &amp;v_{t+1} = \\rho v_t - \\alpha\\nabla f(\\tilde x_t)\\\\ &amp;\\tilde x_{t+1} = \\underline{\\tilde x_t - \\rho v_t}_{x_t} + (1 + \\rho)v_{t+1} \\end{aligned} \\] 关于梯度还有一些直觉并抽象的算法: AdaGrad 算法对 \\(W\\) 每个维度的梯度基于该维度梯度的历史累积平方和进行缩放 通过这种方式, AdaGrad 算法为每个参数独立调整学习率, 通常被称为\"自适应学习率\"。如果一个参数的梯度一直很大, 其累积的梯度平方和也会很大, 导致该参数的更新步长变小, 这有助于减小在该维度上的震荡。反之, 如果一个参数的梯度较小或者不经常发生变化, 其梯度平方和较小, 使得该参数的更新步长相对较大, 从而加快学习进度 12345grad_squared = 0for t in range(num_steps): dw = compute_gradient(w) grad_squared += dw * dw # 梯度平方, sqrt 后得到除的系数 w -= learning_rate * dw / (grad_squared.sqrt() + 1e-7) # 防止分母为 0 在此基础上, 为了避免梯度平方(grad_squared)累积过大导致步长过度减小的问题, 我们加入一个衰减因子来防止其无限增长, 称为 RMSProp, 或俗称 \"Leak AdaGrad\" 12345grad_squared = 0for t in range(num_steps): dw = compute_gradient(w) grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dw * dw w -= learning_rate * dw / (grad_squared.sqrt() + 1e-7) # 防止分母为 0 如果再结合一下动量呢？隆重介绍 Adam, 你可以看作 RMSProp + momentum 12345678moment1, moment2 = 0, 0 # 两个动量for t in range(num_steps): dw = compute_gradient(w) # 算梯度 moment1 = beta1 * moment1 + (1 - beta1) * dw # 动量速度 moment2 = beta2 * moment2 + (1 - beta2) * dw * dw # 动量速度的 leak 平方和 moment1_unbias = moment1 / (1 - beta1 ** t) # 矫正 moment2_unbias = moment2 / (1 - beta2 ** t) # 矫正 w -= learning_rate * moment1_unbias / (moment2_unbias.sqrt() + 1e-7) 这里我们要增加一些矫正, 防止初始时刻我们除以一个非常接近 0 的 moment2 当beta1 = 0.9, beta2 = 0.999时，learning_rate 设置为 1e-3/5e-4/1e-4 对于许多模型来说很好 以上的这些算法都只使用了一阶梯度, 也有有关二级梯度的算法, 能够让算法能够选择更加合适的更新方向和步长, 从而加快收敛速度, 总体思想是将 \\(f(x)\\) 在 \\(x_t\\) 二阶泰勒展开并带入 \\(x_{t+1}\\)​, 这里是具体推导, 这称为 L-BFGS 算法, 由于黑塞矩阵元素数量过多且矩阵求逆复杂度过高, 实践中很少使用二阶优化 Lecture 5 神经网络 如何克服线性分类器的局限性? 一种方法是特征变化(feature transform): 一种简单的特征变化是颜色直方图(color histogram), 通过不同颜色的出现频率分类, 抛弃空间信息 另一种方法是定向梯度方法的直方图(Histogram of Oriented Gradients)(HoG)，通过捕捉图像中局部区域的边缘方向信息来描述物体的形状和轮廓, 抛弃颜色信息 这些方法的本质都是提取特征并调整特征以获得更高的得分 神经网络的本质也类似, 只不过它有多层, 一个两层的神经网络可以这么表示: \\[ f = W_2 {\\rm max}(0,W_1x),\\;W_2 \\in \\mathbb{R}^{C \\times H}, W_1 \\in \\mathbb{R}^{H \\times D}, x \\in \\mathbb{R}^D \\] 这里我们省略了每层的偏移量, 其中 \\(D\\) 为原始的图片 flatten 后的大小(eg. 3x32x32=3072), \\(x\\) 是一个 \\(\\mathbb{R}^D\\) 上的向量表示图片 这叫做全连接网络(fully-connected neural network)或者多层感知器(multi-layer perceptron)(MLP) 类似于线性分类器, \\(W_1\\) 给出了 100 个模板, 这时候就可以对于一个类学习多种模板了, 比如说红框部分是两种不同朝向的马, 这叫做分布表示(distributed representation) \\(W_2\\) 是将隐层的特征表示映射到具体的类别预测, 计算出每个类别的分数(某种加权重组) 为什么在 \\(f\\) 中要取 \\(max(0,W_1)\\)? 这是激活函数(activation function)的一种, \\({\\rm ReLU}(x) = {\\rm max}(0,x)\\), 激活函数是非线性函数, 如果不施加的话我们得到的还是一个线性分类器 \\(f = W_2W_1 x = Wx\\) 还有很多的激活函数: 通过激活函数进行特征变化, 我们得到了新的数据分布, 就可以通过线性分类器分类了 随着神经网络的层数增加, 就可以表示出更加复杂的决策边界, 增加正则化项并调整 \\(\\lambda\\)​ 可以防止过拟合(不要通过减少层数来防止过拟合), 你可以在在线 2D 数据点分类这里试一试 在一个有界的输入空间中, 神经网络可以逼近任何函数, 你可以想象无数的 ReLU 图像拼接成一个函数, 但即使是 kNN 也能逼近所有函数, 这个性质并没有保证神经网络在实战中能学习任意函数 在讨论最优解, 优化问题的收敛时, 我们常用的是凸函数(convex function), \\(f = x^2\\) 就是凸的 当 \\(f(tx_1+(1-t)x_2) \\leq tf(x_1) + (1-t)f(x_2)\\), 其中 \\(f:X \\subseteq \\mathbb{R}^N \\rightarrow \\mathbb{R}, x_1,x_2 \\in X, t \\in [0,1]\\) 几何意义可以这样理解, 以 \\(f = x^2\\) 为例, 绿线永远在蓝线上面: 凸函数有一些很好的性质: 局部最值即为全局最值、可以使用其他优化算法、收敛更快并保证收敛 之前我们对线性分类器的优化(Softmax, SVM, ...)就是一个凸函数 但是任何层的神经网络没有证明有这样的性质。所以神经网络的优化需要非凸函数优化。这说明我们不能保证它一定会收敛, 但是, 凸函数的一些优化方法对神经网络优化经常有效, 奇妙 Lecture 6 反向传播 可以先看 3blue1brown 的视频建立一些基础了解和直觉 在优化算法中, 无一例外需要计算梯度 由于直接推导梯度的解析式过于繁杂, 数值计算梯度又不够精确, 所以我们选择反向传播(backpropagation)来计算梯度, 个人理解其本质就是复杂一点的链式法则 计算图帮助我们分解函数的计算, 见上, 蓝色节点为得分, 传入红色节点(SVM loss 计算节点)后得到 data loss, 加上 \\(R(W)\\) 正则项后得到最终损失 以 \\(f(x,y,z) = (x+y)z\\) 为例, 前向传播(forward pass)指计算 \\(f\\) 的过程, 而反向传播(backward pass)指计算输出关于每个输入的导数(部分梯度), 即 \\(\\displaystyle\\frac{\\partial f}{\\partial x}, \\displaystyle\\frac{\\partial f}{\\partial y}, \\displaystyle\\frac{\\partial f}{\\partial z}\\) 一开始, \\(\\displaystyle\\frac{\\partial f}{\\partial f} =1\\); 然后因为 \\(f = qz\\), 所以 \\(\\displaystyle\\frac{\\partial f}{\\partial z} = q = 3\\), 同理 \\(\\displaystyle\\frac{\\partial f}{\\partial q} = z = -4\\); 接着 \\(\\displaystyle\\frac{\\partial f}{\\partial y} = \\displaystyle\\frac{\\partial q}{\\partial y} \\displaystyle\\frac{\\partial f}{\\partial q} = 1 \\times (-4) = -4\\), 同理 \\(\\displaystyle\\frac{\\partial f}{\\partial x} = \\displaystyle\\frac{\\partial q}{\\partial x} \\displaystyle\\frac{\\partial f}{\\partial q} = -4\\) 因为反向计算梯度, \\(\\displaystyle\\frac{\\partial f}{\\partial q}\\) 称为上游梯度(upstream gradient), 而 \\(\\displaystyle\\frac{\\partial f}{\\partial x}\\) 称为下游梯度(downstream gradient), \\(\\displaystyle\\frac{\\partial q}{\\partial x}\\)​​ 称为局部梯度(local gradient), 你可以在计算图上看到, 边的上部为函数计算结果, 边的下部为各部分梯度 注意上游梯度和下游梯度是相对的, 根据当前的计算而定, 这种传播就不需要考虑全局的函数结构, 只需要计算自己的一小部分 有时候我们不用把整个式子拆成单步的运算, 例如蓝框的部分为激活函数 \\(\\sigma(x) = \\displaystyle\\frac{1}{1+e^{-x}}\\), 它的梯度有: \\(\\displaystyle\\frac{\\partial}{\\partial x} \\left[\\sigma(x)\\right] = (1 - \\sigma(x))\\sigma(x)\\) , 所以注意 \\(\\sigma(x)\\) 前后的上下游梯度, 有 0.2 = (1-0.73)*0.73*1 对于一些简单运算有一些简单的算梯度方法, 可以见下图 现代的 API 将各种类型的门, 以及反向传播时不同门的梯度计算方法封装起来, 这样修改 loss function 时只需要修改 loss function 的代码, 其他地方不需要任何修改, pytorch 都帮你定义好了, autograd 真是好用啊 我们刚刚讨论了最简单的输入为实数的情况, 但若数据为向量甚至高维张量时怎么做呢？ 当 \\(x \\in \\mathbb{R}^N, y \\in \\mathbb{R}\\) 时, 导数是梯度, \\(\\displaystyle\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^N\\), 每个分量表示 \\(y\\) 对 \\(x_i\\) 的偏导 当 \\(x \\in \\mathbb{R}^N, y \\in \\mathbb{R}^M\\) 时, 导数是雅可比矩阵(Jacobian matrix), \\(\\displaystyle\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{N \\times M}\\) , \\(\\displaystyle\\frac{\\partial y_n}{\\partial x_m}\\) 表示 \\(y_n\\) 对 \\(x_m\\) 的偏导 以向量为例, 设 \\(x, y, z\\) 的维度分别为 \\(D_x, D_y, D_z\\), 注意到损失 \\(L\\) 一直是一个标量, 数据的计算结果与梯度维度是对应的 雅可比矩阵一般都是稀疏的, 以 ReLU(x) = max(0,x) 为例: 因为 ReLU 函数是元素对元素(elementwise)的, 所以其雅可比矩阵仅在对角线处有取值, 对于上述情况, 矩阵乘法可以简化为: \\[ \\left( \\displaystyle\\frac{\\partial L}{\\partial x}\\right)_i = \\left\\{ \\begin{matrix} \\left(\\displaystyle\\frac{\\partial L}{\\partial y}\\right)_i\\;,\\;\\;if\\;x_i &gt; 0\\\\ 0,\\;\\;otherwise \\end{matrix} \\right. \\] 假设我们正在做矩阵乘法, 有 \\(y = xw\\), 其中 \\(y\\) 的形状为 (N, M), \\(x\\) 的形状为 (N, D), \\(w\\) 的形状为 (D, M), 假设要算 \\(\\displaystyle\\frac{\\partial L}{\\partial x} = \\displaystyle\\frac{\\partial L}{\\partial y} \\displaystyle\\frac{\\partial y}{\\partial x}\\), 则雅可比矩阵 \\(\\displaystyle\\frac{\\partial y}{\\partial x}\\) 的形状为 (N,D,N,M) 或者说是 (N*D,N*M), 显然不可接受。 但因为 \\(y_{i,j} = \\sum x_{i,k}w_{k,j}\\), 又因为 \\(\\displaystyle\\frac{\\partial y}{\\partial x_{i,j}}\\) 由 \\(\\displaystyle\\frac{\\partial y_{1,1}}{\\partial x_{i,j}}, \\displaystyle\\frac{\\partial y_{1,2}}{\\partial x_{i,j}}, ..., \\displaystyle\\frac{\\partial y_{n,m}}{\\partial x_{i,j}}\\) 组成, 又实际上由 \\(\\displaystyle\\frac{\\partial y_{i,1}}{\\partial x_{i,j}} = w_{j,1}, \\displaystyle\\frac{\\partial y_{i,2}}{\\partial x_{i,j}} = w_{j,2}, ..., \\displaystyle\\frac{\\partial y_{i,m}}{\\partial x_{i,j}} = w_{j,m}\\) 构成(除 \\(y_i\\) 行外其他行无贡献), 所以有 \\(\\displaystyle\\frac{\\partial y}{\\partial x_{i,j}} = w_j\\), 有 \\(\\displaystyle\\frac{\\partial L}{\\partial x_{i,j}} = \\displaystyle\\frac{\\partial y}{\\partial x_{i,j}} \\displaystyle\\frac{\\partial L}{\\partial y} = w_j \\cdot \\displaystyle\\frac{\\partial L}{\\partial y_i}\\), 所以有 \\(\\displaystyle\\frac{\\partial L}{\\partial x} = \\displaystyle\\frac{\\partial L}{\\partial y}\\; w^T\\) , 在形状上看为 (N, D) = (N, M) * (M, D), 同理有 \\(\\displaystyle\\frac{\\partial L}{\\partial w} = x^T \\displaystyle\\frac{\\partial L}{\\partial y}\\) 最好还是看看讲义 PPT, 我们相当于做了 reverse-mode automatic differentation 来避免算一些抽象大小的雅可比矩阵 反向传播的思想也可以用来计算二阶梯度黑塞矩阵, 也就是梯度变化的速度 这里仅做了解 Assignment 2 线性分类器 &amp; 双层神经网络 线性分类器 写一个在 CIFAR-10 上的线性分类器, 分别尝试使用两种损失函数: SVM 和 Softmax SVM 先要算 loss 以及损失函数 \\(L\\) 的梯度 dW, 因为对于单张图片 \\(L_i = \\sum\\limits_{j \\neq y_i} {\\rm max} (0,W_{j,:} \\cdot x_i - W_{y_i,:} \\cdot x_i + 1)\\) , 所以对于 \\(N\\) 张图片的一个 batch, 有: 123456789101112131415161718192021222324252627282930313233def svm_loss_naive(W, X, y, reg): &quot;&quot;&quot; 构造使用循环的 SVM loss 函数 输入的维度为 D, 有 C 种分类, batch 大小为 N 输入: - W: shape (D, C), 为权重.(不是 (C, D), 故 f = xW) - X: shape (N, D), 为数据. - y: shape (N,), 为训练标签; y[i] = c &lt;=&gt; X[i] 标签为 c, 0 &lt;= c &lt; C. - reg: (float), 为正则化因子 输出: - loss: 标量 - dW: shape (D, C), 损失函数的梯度 &quot;&quot;&quot; dW = torch.zeros_like(W) # 初始化梯度为 0 num_classes = W.shape[1] # [C] (指 shape 为 (C), 后文一样) num_train = X.shape[0] # [N] loss = 0.0 for i in range(num_train): scores = W.t().mv(X[i]) # [C, D] * [D] = [C] correct_class_score = scores[y[i]] # 标量 for j in range(num_classes): # [C] if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # Δ = 1 if margin &gt; 0: loss += margin dW[:,j] += X[i] # [D], 梯度, 导一下就明白了 dW[:,y[i]] -= X[i] # 平均化后加入正则化项 loss /= num_train loss += reg * torch.sum(W * W) dW /= num_train dW += reg * 2 * W return loss, dW 当然这样很慢, 我们向量化数据, 继续利用广播思想, 不写显式循环, 有: 12345678910111213141516def svm_loss_vectorized(W, X, y, reg): loss, N = 0.0, X.shape[0] dW = torch.zeros_like(W) # [D, C], 初始化全 0 scores = X.mm(W) # [N, C] idx0 = torch.arange(N) # tensor([0,1,2,...,N-1]) correct_class_score = scores[idx0, y].reshape(N, 1) # [N, 1] margin = scores - correct_class_score + 1 # [N, C] margin[margin &lt; 0] = 0 margin[idx0, y] = 0 loss = torch.sum(margin)/N + reg*torch.sum(W*W) # scaler # margin[i,j] &gt; 0 &lt;=&gt; dW[:,j] += X[i], dW[:,y[i]] -= X[i] fir, sec = torch.nonzero(margin&gt;0).t() # 提取 i, j 的 list dW[:, sec] += X[fir].t() # 为了 shape 符合, 要开 .t() 转置 dW[:, y[fir]] -= X[fir].t() dW = dW/N + reg * 2 * W return loss, dW 不写显式循环在我的机子上能快上最高 160 倍 广播, 张量套张量后的 shape 乱成了一坨, 没事可以输出 .shape 来调试 写完 loss 后写分类器就简单了, 但是你会发现准确率 10% 都不到, 还不如纯蒙呢, 于是要去调超参了 这部分建议看看作业代码, 我们先把线性分类器封装起来, 再封装 SVM loss 类损失函数(因为下文用softmax), 超参数主要是 learning_rate 和 regularization_strengths, 把迭代次数调小一点, 跑个二重循环来找一下合适的超参数就行, 但是艹了, 我的模型准确率 30% 都不到, 根据 SVM 写法的不同比别人的差挺多 Softmax 对于单张图片 \\(x_i\\) , \\(L_i = - {\\rm log} \\left( \\displaystyle\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}}\\right) = -s_{y_i} + {\\rm log} \\sum\\limits_j e^{s_j} = -W_{y_i}x_i + {\\rm log} \\sum\\limits_j e^{W_j x_i}\\) 我们开导, 对于每一个 \\(y_i\\) 项, 有一个 \\(-x_i\\), 后面的 log, sum 一坨导出来都一样, 为 \\(\\displaystyle\\frac{e^{W_j x_i}}{\\sum\\limits_j e^{W_j x_i}} \\cdot x_i\\) , 这个 \\(\\displaystyle\\frac{e^{W_j x_i}}{\\sum\\limits_j e^{W_j x_i}}\\) 其实就是概率, 设为 \\(p_{i,j}\\), 故: \\[ \\displaystyle\\frac{\\partial L}{\\partial W_j} = \\sum\\limits_{i=1}^{N} \\left\\{ \\begin{matrix} (p_{i,j}-1)x_i,\\;\\;if\\;j=y_i\\\\ p_{i,j} x_i,\\;\\;otherwise \\end{matrix} \\right. \\] 正确分类 \\(i = y_i\\) 的概率会多减一个 \\(x_i\\) 在编写代码前还有一些数值计算的问题, 直接算 \\(e^{W_jx_i}\\) 显然不可接受(过大), 根据给出的扩展资料, 有: \\[ \\displaystyle\\frac{e^{W_j x_i}}{\\sum\\limits_j e^{W_j x_i}} = \\displaystyle\\frac{Ce^{W_j x_i}}{C\\sum\\limits_j e^{W_j x_i}} = \\displaystyle\\frac{e^{W_j x_i+ {\\rm log}\\;C}}{\\sum\\limits_j e^{W_j x_i+{\\rm log}\\;C}} \\] 假设我们选择 \\({\\rm log}\\;C = - {\\rm max}_js_j\\), 按照最大值左移 \\(s\\) 张量, 则 \\(s_i \\leq 0\\), \\(e^{s_i} \\leq 1\\) 不会越界(过小省去无问题) 这里我们直接给出无循环的代码, 各种符号定义与 SVM 一致: 1234567891011121314def softmax_loss_vectorized(W, X, y, reg): loss, N = 0.0, X.shape[0] dW = torch.zeros_like(W) # [D, C], 初始化全 0 scores = X.mm(W) # [N, C] max_score = torch.max(scores, dim=1, keepdim=True)[0] # [N, 1] scores = scores - max_score # left shift exp_scores = torch.exp(scores) # [N, C] idx0 = torch.arange(N) # tensor([0,1,2,...,N-1]) p = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True) # [N, C] loss = torch.sum(-torch.log(p[idx0,y])) # 原始定义 loss = loss/N + reg*torch.sum(W * W) # 平均化后加入正则项 p[idx0,y] -= 1 # 处理正确分类 dW = X.t().mm(p)/N + 2*reg*W return loss, dW 同理封装后调超参, softmax 需要的 learning_rate 和 regularization_strengths 偏低, 最后我调出来了 37.48% 的准确率, 我很满意了…… 双层神经网络 写一个在 CIFAR-10 上的双层神经网络, 使用 Softmax 损失函数 总体而言, \\(f = W_2 {\\rm max}(0,W_1x+b_1) +b_2\\), 设 \\(h = {\\rm max}(0,W_1x+b_1),\\;s=f\\), 我们正向传播算 loss, 反向传播算梯度, 先算 ds, 再算 dW2, db2, 最后算 dW1, db1, 这里着重讲一下反向算梯度的部分 根据 \\(L_i = -s_{y_i} + {\\rm log} \\sum\\limits_j e^{s_j}\\), 有: \\(\\displaystyle\\frac{\\partial L}{\\partial s_j} = \\sum\\limits_{i=1}^{N}\\left\\{\\begin{matrix} p_{i,j}-1,\\;\\;if\\;j=y_i\\\\p_{i,j},\\;\\;otherwise\\end{matrix}\\right.\\) 根据 \\(s = W_2h+b_2\\), 有 \\(\\displaystyle\\frac{\\partial L}{\\partial W_2} = h \\times \\displaystyle\\frac{\\partial L}{\\partial s}, \\displaystyle\\frac{\\partial L}{\\partial b_2} = 1 \\times \\displaystyle\\frac{\\partial L}{\\partial s}, \\displaystyle\\frac{\\partial L}{\\partial h} = W_2 \\times \\displaystyle\\frac{\\partial L}{\\partial s}\\), 记得加上正则化项 如果把 shape 列出来, 有 \\(\\underline{s}_{[N,C]} = \\underline{h}_{[N,H]} \\underline{W_2}_{[H,C]} + \\underline{b_2}_{[C,]}\\) 注意到 $ $ 和 $ $ 大小对不上, 但是你仔细一想发现 \\(\\displaystyle\\frac{\\partial L}{\\partial s}\\) 其实是被广播到 \\([N,C]\\) 大小的, 其原始大小只有 \\([C,]\\) 于是你还原回去就行了, 在求其他局部梯度时也要考虑 shape 问题 根据 \\(h = {\\rm max}(0,W_1x+b_1)\\), 假设 \\(h' = W_1x+b_1\\), 我们先根据反向传播运算表去掉了 \\(\\rm max\\), 得到了 \\(\\displaystyle\\frac{\\partial L}{\\partial h'}\\) , 有: \\(\\displaystyle\\frac{\\partial L}{\\partial b_1} = \\displaystyle\\frac{\\partial L}{\\partial h'}, \\displaystyle\\frac{\\partial L}{\\partial W_1} = x \\times \\displaystyle\\frac{\\partial L}{\\partial h'}\\), 这里 \\(\\displaystyle\\frac{\\partial L}{\\partial b_1}\\) 要对 \\(\\displaystyle\\frac{\\partial L}{\\partial h'}\\) 的第零维(dim=0)取平均 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def nn_loss_part2(params, X, y=None, reg=0.0): &quot;&quot;&quot; 为双层神经网络计算 loss 和 参数梯度 输入: - params: 参数列表, 包含: - W1: shape (D, H), 为第一层权重 - b1: shape (H,), 为第一层偏移量 - W2: shape (H, C), 为第二层权重 - b2: shape (C,), 为第二层偏移量 - X: shape (N, D), 为数据 - y: shape (N,), 为训练标签; y[i] = c &lt;=&gt; X[i] 标签为 c, 0 &lt;= c &lt; C. - reg: (float), 为正则化因子 输出: - loss: 标量 - grads: params 中所有参数的梯度 &quot;&quot;&quot; # 从 params 中解包参数 W1, b1 = params['W1'], params['b1'] W2, b2 = params['W2'], params['b2'] N, D = X.shape C = b2.shape[0] scores, h = nn_loss_part1(params, X, y, reg) # 先计算得分, scores = s = f; shape 分别为 [N, C], [N, H] # 如果没有给出 y, 说明计算完毕 if y is None: return scores loss = None # 算 loss max_score = torch.max(scores, dim=1, keepdim=True)[0] # [N, 1] scores = scores - max_score # left shift exp_scores = torch.exp(scores) # [N, C] idx0 = torch.arange(N) # tensor([0,1,2,...,N-1]) p = exp_scores / torch.sum(exp_scores, dim=1, keepdim=True) # [N, C] loss = torch.sum(-torch.log(p[idx0,y])) # 原始定义 loss = loss/N + reg*torch.sum(W1*W1) + reg*torch.sum(W2*W2) # 平均化后加入两个正则项 # 反向传播算梯度 grads = {} p[idx0,y] -= 1 # [N, C] ds = p # [N, C] db2 = ds.mean(dim = 0) dW2 = h.t().mm(ds) dW2 = dW2/N + 2*reg*W2 # [H, C], 加入 W2 的正则项 dh = ds.mm(W2.t()) # [N, H] hh = X.mm(W1)+b1 # [N, H], 即 h' dhh = dh # [N, H], 即 dh' dhh[hh&lt;0] = 0 db1 = dhh.mean(dim = 0) # 在第零维取平均 dW1 = X.t().mm(dhh) dW1 = dW1/N + 2*reg*W1 # [D, H], 加入 W1 的正则项 grads['b2'] = db2 grads['W2'] = dW2 grads['b1'] = db1 grads['W1'] = dW1 return loss, grads 这里的参数 H 是隐藏层(hidden layer)的大小, H 越大(网络容量越大), 拟合越好 如果初始模型在训练集和验证集上的表现非常相似, 这说明模型是欠拟合的, 如果我们增加网络容量, 其性能可能会提高 上面是 H = 2, 8, 32, 128, 512 时在训练集和验证集上的准确率, 每一个 epoch 代表使用训练集的全部数据进行一次完整训练 模型训练和验证准确度之间的小差距的另一个解释是正则化, 如果正则化系数(reg)过高, 则模型可能无法拟合训练数据, 同理, 学习程度(learning rate)也会对模型产生影响 接下来我们就要调试这三个超参, 封装啊, 三重循环啊, 最后在测试集跑出了 51.35% 准确率 Lecture 7 卷积神经网络 CNN 将二维图像拍平为一个 3072 大小的一维数组并不尊重图像的二维空间结构, 为了解决这个问题, 我们引入一些可以用在二维空间上的结构/运算, 例如卷积层、池化层、标准化层等 全连接层(fully-connected layer)完全破坏了空间结构 而卷积层(convolution layer)不同, 它接受一个三维的张量(通道数(Channels)/高度/宽度)作为输入图像, 将其与卷积核/滤波器(filter)进行点积运算, 卷积核在输入图像上移动, 计算每个位置的点积, 从而生成特征图(feature map) 卷积核在每个位置上的运算会返回一个数字(点积得到) \\(w^Tx+b\\), 所有的数字构成一层激活图(activation map), 可能有不同的卷积核, 所有的激活图构成特征图 具体来讲, 若输入的 shape 为 \\((N,C_{in},H,W)\\), 则卷积核的 shape 为 \\((C_{out},C_{in},K_w,K_h)\\), 输出的 shape 为 \\((N, C_{out},H',W')\\), 在这里我们没有额外操作, 于是 \\(H' = H-K_h+1, W'=W-K_w+1\\) 卷积神经网络不是在学习图像的模板, 而是学习图像的边缘与颜色 上图是卷积层的架构, 我们仍要加入激活函数否则卷几次都和卷一次没区别, 我们可视化了 Alexnet 第一层的 64 个卷积核, 有水平边缘、垂直边缘, 有不同的频率, 有对立颜色。你可以理解为卷积输出了特征向量的网格 为了防止图像越卷越小, 最终无法开卷, 我们需要在图像边缘添加额外宽度的填充(padding), 一般填 0 就行, 设填充宽度为 P, 则对于 H 有 \\(H' = H-K_h+1+2P\\), 对于 W 也一样, 所以一般设置 \\(P=(K-1)/2\\), 这样图像大小不会变化 另一个有用的概念是感受野(receptive fields), 输出图像的一个元素实际上只有输入图像的部分区域(K*K)决定, 这块区域就叫做感受野。当我们堆叠卷积层时, 假设有 L 层, 则感受野为(1+L*(K-1)), 如果图像过大(1024*1024)而卷积核较小, 我们可能需要堆叠大量卷积层才能让最终输出的一个元素\"感受\"到整个图像 这又引入了步长(stride)的概念, 我们默认的步长为 1, 卷积核必须遍历原始图像每个(K*K)的子图像进行点积运算, 如果提高步长, 跳过一些子图像, 感受野会增长得很快, 图像也会缩小得很快 假设输入宽度为 W, 卷积核大小为 K, 填充宽度为 P, 步长为 S, 输出宽度为 (W-K+2P)/S+1, 设置 S 的时候一般要保证输出宽度为整数, 不行的话就要有截断了 有的时候我们会用 1*1 大小的卷积核, 这就相当于做线性操作了, 这种结构叫做Network In Network, 可以实现多个特征图(feature map)的线性组合, 实现跨通道的信息整合 有二维的卷积, 也有一维、三维的卷积, 定义都类似, 一维可以处理文本和音频, 三维可以处理点云数据 在 pytorch 中有专门的 Conv2d 函数来处理二维卷积 下面介绍池化层(pooling layers), 池化层可以在神经网络中下采样(downsampling), 卷积层也可以下采样, 但是池化层不学习任何新的参数, 只有三个超参数在起作用: 核大小, 步长和池化函数(max,avg,...) 上图是一个池化的例子, 图中核大小与步长一样为 2, 池化区域不重叠, 我们使用 max pooling 作为池化函数, 也就是取最大值出来 max 操作具有不变性, max pooling 相当于允许\"模型中的少量平移\", 对特定的任务有效 max pooling 引入了非线性, 理论上我们以及不用紧接着引入激活函数了, 但一般还是会引入 有了这些层后我们就能构建简单的卷积神经网络了, 以经典的 LeNet-5 为例, 一般是一些卷积层后面跟上一些全连接层: 可以注意到在我们经过整个网络时, 我们倾向于减小空间尺寸并增加通道尺寸 标准化层(normalization layers)帮助我们更好地训练神经网络(加速), 有了标准化就可以提高学习程度 如果我们想要使层符合一些目标分布, 比如说平均值为 0, 单位方差(unit variance)(将特征的尺度统一为相似的范围), 这叫做批量归一化(Batch Normalization)(BN) 最后为了防止除以 0 在分母加入了 \\(\\epsilon\\) 直接归一化会使得模型的学习能力受到一定限制, 因为输出被严格限制在均值为 0 和方差为 1 的分布中, 为了恢复模型的表达能力, BN 还引入了两个可学习的参数: 缩放参数 \\(\\gamma\\) 和偏移参数 \\(\\beta\\) 这两个参数负责将归一化后的数据进行线性变化: \\(y_{i,j} = \\gamma_j \\hat{x}_{i,j} + \\beta_j\\), 若训练出来后发现 \\(\\gamma = \\sigma, \\beta = \\mu\\), 则说明不需要归一化 批量归一化存在缺陷, 在测试阶段, 由于无法像训练阶段那样获取 batch 的均值和方差, 因此使用的是在训练阶段累积的全局均值和方差, 此时 \\(\\gamma, \\beta\\) 是常量, 所以这其实是一种线性运算, 可以和卷积操作放一起 批量归一化在理论上还没被完全理解, 而且容易被不平衡的数据破坏, 可能因为各种原因在实践上产生 bug 还有一些标准化方法, 例如 layer norm(在特征维度标准化), instance norm(仅在空间维度平均) 和 group norm, 有一张图很好体现了它们的区别, 蓝色部分为标准化部分: 不同标准化方法有不同的使用场景, 这里按下不表 一个可视化的在 CIFAR-10 上的卷积神经网络 demo Lecture 8 CNN 架构 卷积神经网络的组成元素主要包含卷积层、池化层、全连接层、激活函数和归一化方法, 然而怎么结合这些元素构成较好性能的网络是一个问题, 这节课以 ImageNet 每年的比赛为线索, 介绍前人经典的卷积神经网络(CNN) Alexnet: 五层卷积层、Max Pooling 方法、三层全连接层、ReLU 激活函数、分在两个 GPU 上跑 图中省略了紧跟在卷积层(conv)后的激活函数 ReLU 这里的 params 指可学习参数的数量, 以 conv1 层举例, \\({\\rm params} = \\underline{C_{out} \\times C_{in} \\times K \\times K}_{weight\\;shape} + {\\rm biases}= 64 \\times 3 \\times 11 \\times 11 + 64 = 23296 \\approx 23k\\) 这里的 flop 指浮点运算数(一次乘法加上一次加法算一次浮点运算) ${} = {} = (C_{out} H' W') (C_{in} K K) $ \\({\\rm flop} = (64 \\times 56 \\times 56) \\times (3 \\times 11 \\times 11) = 72855552 \\approx 73M\\) 卷积层主要开算, 而池化层需要的计算较少以至于被舍去了 在第一个全连接层(fc1)中, params 突然大幅增加, 因为展平后数量级上升了, 9216 * 4096 = 37748736 左边层数的架构怎么想出来的？硬试出来的！这我学个毛 我们可以学习的是, 右边的内存、params 和 flop 显现出一些趋势 内存主要前期用, params 在全连接层多, flop 在卷积层多 在 Alexnet 后 ImageNet 比赛下一年的冠军是 ZFnet, 基本就是大一号的 Alexnet, 体现一种趋向: 更大的网络更好 从 2014 年开始, 人们不再追求这种 Ad hoc 的网络配置, 手动调整每一层的大小, 而是转向一种更规范的设计理念, 下面介绍 VGG 网络架构 卷积层都是 3*3 的卷积核, 步长与填充为 1, 池化层都是 2*2 的核大小, 步长为 2 的 固定经历五个阶段的卷积(conv-conv-pool), 在池化层后翻倍通道数量 这个网络的可解释性更强一点, 比起用一个 5*5 的卷积核, 花费 25, 不如用两个 3*3 的卷积核代替, 效果一样(插入更多 ReLU 后效果甚至更好), 但花费为 2*3*3 = 18, 同理, 7*7 的卷积核不如三个 3*3 的卷积核, 我们就不用关心核大小了 在池化后翻倍通道数量, 这时候内存会变为原来的 1/2 而不是 1/4, 和前面的卷积层相比 flop 不变, 这里的设计思想就是希望让每个卷积层有相同数量的 flop 运算 和 Alexnet 相比, VGG 是一个更加巨大的网络 再介绍一下其他网络架构, GoogleNet 希望更高的网络效率, 于是它进行了快速的下采样: 原始图像从 224*224 到 28*28, GoogleNet 的开销平均仅有 VGG 的 1/10 左右 GoogleNet 还引入了一种模块叫做 inception module, 你可以在上图的右侧见到这个模块有并行的计算分支 为了避免核大小这个超参数, 和 VGG 不同的是, GoogleNet 同时进行了不同核大小的卷积, GoogleNet 还使用了 1*1 大小的卷积核来减少通道数量 与 max pooling 不同, GoogleNet 使用了全局平均池化(global average pooling), 让池化层的核大小与此时的图像大小相同, 每个通道只保留了一个元素, 相当于替代了一层全连接层, 这样可以减少全连接层巨大的可学习参数 GoogleNet 的时代没有批量归一化, 为了训练大型网络引入了丑陋的辅助分类器(auxiliary classifier), 生成不同的分数来帮助 GoogleNet 收敛 2015 年出现了残差神经网络(Residual Neural Network), ResNet 赢得了那年的 ImageNet 比赛, 一年内神经网络的层数从 22 变成了 152, 这是怎么做到的？ 假设我们只用批量归一化来训练神经网络, 你会发现 56 层的网络比 20 层的网络表现更差, 但 56 层网络应该能模拟 20 层网络, 只要弃用一些层让它们为恒等函数就行, 欠拟合说明了它不能完成这些学习, 也就是不能学习恒等函数 ResNet 主要使用残差块(residual block)构建, 我们加入了一些\"捷径\", 如果这些卷积层的权重为 0, 则我们就能模拟恒等函数来输出 x, 这样也更有利于梯度的反向传播, ResNet 也借鉴了 GoogleNet, 使用了激进的下采样 残差块也有多种, 普通块就是由两个 3*3 的卷积层加上捷径构成的, 瓶颈块(Bottleneck block)由 1*1, 3*3, 1*1 的三个卷积层构成, 并在开头和结尾改变通道数 下图是五种 ResNet 的数据: 我们可以继续改进残差剩余网络, 比如将多个瓶颈块并行以获得更高的性能 ImageNet 比赛终止于 2017 年, 你现在可以在 Kaggle 网站 上自己打比赛 还有一些有趣的块结构, 比如 DenseNet 将两两层之间全部用\"捷径\"连接, MobileNet 致力于减少计算的复杂度 现在还有一些有趣的想法, 比如让神经网络来训练一个子神经网络的架构, 这个想法的问题是太耗算力了, 有一些案例, 比如说 NasNet 如果你要解决某些问题, 就用现成的架构吧, 除非你有一万张卡什么的才能够自己设计网络架构 Lecture 9 硬件和软件 深度学习: NVIDIA YES 课程的时代(2019)最好的设备还是 Nvidia Titan RTX (16.3 TFLOP)(4608 cores, 并行任务更好) 和 Ryzen 9 3950X (4.8 TFLOP) 在 RTX Titan 中有专门的 Tensor Core 来算深度学习中的矩阵乘法 Wx+b, 极大增加 TFLOP Nvidia 显卡可以进行 cuda 编程来写 GPU 运算的代码 Google Cloud 提供云计算服务 ReLU 这样简单的函数主要吃显存带宽, 即运算单元与显存之间的通信速率 软件方面, 主要的深度学习框架是 PyTorch 和 Tensorflow 我们希望框架提供库(轮子)、会自动算梯度、GPU 上跑得快 Pytorch 的基础概念有 Tensor(GPU 上的数组), Autograd(自动算梯度), Module(更大的模块抽象化, 比如说一层神经网络) 在后续的 Assignment 中会使用更高程度的抽象(Autograd 和 Module) 在创建 Tensor 时, 通过设置 requires_grad = True, PyTorch 就会帮你建立计算图算梯度, 你可以使用 .grad 方法来调用, 这部分可以见 PPT 47-61 页解释 PyTorch 允许我们写 tensor 的函数, 比如可以写个 sigmoid 函数, 但这种简单的激活函数 PyTorch 里面已经有实现了, 自己写可能会有数值计算的问题 PyTorch 的 nn 模块允许我们直接自行连接一些封装好的层来构建模型 PyTorch 的 optim 模块包含常见的最优化算法, 让你直接用 Adam PyTorch 也允许你自己定义模块, 一个双层神经网络只要这么写就可以了 1234567891011class TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_predmodel = TwoLayerNet(1000,100,10) # (D_in, H, D_out)optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) 梯度用 Autograd 就行了 PyTorch 的 dataloader 模块能够快速分割数据构成不同集合, 取 batch 之类的 PyTorch 还提供出名的预训练模型, 例如 AlexNet 和 ResNet PyTorch 的 jit 模块帮助获得静态和动态的计算图并加速推理, 看教程 PyTorch JIT 即时编译 静态和动态的计算图各有好坏, 动态计算图一般用在 RNN 上(输入取决于输出) TensorFlow 1.0 和 2.0 大版本区别挺大, 例如 1.0 默认使用静态计算图而 2.0 默认用动态计算图 TensorFlow 中有 Keras 库提供高级 API, 用处和 PyTorch 中的 nn 模块类似 TensorFlow 可以使用 TensorBoard 来进行训练中数据的可视化, 十分好用, PyTorch 也有对应的 TensorBoard Lecture 10 训练神经网络(上) 感谢 爱吃蛋黄派 - 知乎 的笔记与解析, 本节课主要讲神经网络训练前的一些技巧 激活函数 sigmoid 激活函数: \\(\\sigma(x) = \\displaystyle\\frac{1}{1+e^{-x}}\\) 非常流行 它的好处是将输入压缩到 [0,1] 之间的概率, 可以解释为神经元的放射率之类的 它的坏处是: 当 x 过大或过小时梯度几乎为 0, 无法学习 函数的输出不是以 0 为中心的(&gt;0), 这会导致所有梯度同正或同负, 在实践中因为有 batch 所以影响不大 指数计算 \\(e^x\\) 较为昂贵(对于手机端或 CPU 而言, GPU 的速度瓶颈不在这里) 另一个常用的激活函数是 tanh: \\({\\rm tanh}(x) = \\displaystyle\\frac{e^x-e^{-x}}{e^x+e^{-x}} = 2 \\sigma(2x) - 1\\), 它的取值在 [-1,1], 然而还是有梯度的问题 ReLU 是最简单的激活函数, \\({\\rm ReLU}(x) = {\\rm max}(0,x)\\), 输出不是以 0 为中心的, 当 x&lt;0 时激活会完全失败, 这一般称为 dead ReLU, 永远不会被更新, 一种小技巧是给初始的偏差 \\(b\\) 一个极小的正权值 \\({\\rm Leaky\\;ReLU}(x) = {\\rm max}(0.1x,x)\\) 解决了梯度问题, 运算也快, 但是存在超参数(小于 0 是的乘法参数, 这里为 0.1), 一种想法是让这个超参数变为可学习参数, 在每层都不同 \\({\\rm ELU}(x)\\) (见上图) 更加光滑, 能够更高对抗噪音, 但是需要设置超参数 \\(\\alpha\\)​, 以及指数计算较为昂贵 最抽象的激活函数是这个 SELU, 它有一种 \"自归一化\" 的性质, 这是用 91 页的论文附录算出来的 最后总结, 不要用 sigmoid 或 tanh, 一般用 ReLU 就行, 如果强调 0.1% 的性能再用别的 一般激活函数都是单调的, 也存在不单调的激活函数 GELU 下面来谈论一下数据预处理(data preprocessing) 简单的数据预处理是让数据以 0 为中心(zero-centered) 或标准化的, 其原因仍然是希望梯度有正有负 伪代码为 X -= np.mean(X, axis=0) 以及 X -= np.std(X, axis=0) 还有一些预处理方法, 比如去相关化(decorrelation)和白化(whitening) 数学术语解释: 协方差(covariance)可以用来表示两个变量(向量)间的相关性, 因为相关性越大, 代表两个变量不是完全独立的, 有重复的信息。\\({\\rm Cov}(X,Y) = E[(X-E[X])(Y-E[Y])]\\) 协方差矩阵(covariance matrix)可以用来表示多个变量的相关性, 例如, 给定数据集 \\(X \\in \\mathbb{R}^{m \\times n}\\), 该矩阵有 m 个样本, 每个样本是 n 维的向量, 我们可以得到协方差矩阵 \\(C = E[{\\rm x x^T}] = \\displaystyle\\frac{1}{m}\\sum\\limits_{i=1}^m {\\rm x_i x_i^T}\\) 去相关化就是要把数据的协方差矩阵变为对角矩阵, 我们设协方差矩阵为 \\(C\\), 做特征值分解, \\(C = U \\Lambda U^T\\), 设处理前的数据为 \\(x\\), 则去相关后的数据 \\(y = U^{-1}x\\) (即 \\(Uy = x\\), \\(y\\) 为 \\(x\\) 在 \\(U\\) 的列空间中的坐标) 白化建立在去相关化的基础上, 去相关化后数据集之间线性无关, 但是 \\(\\Lambda\\) 对角元素并不相等, 将 \\(\\Lambda\\) 变为单位矩阵的过程就是白化, 公式为 \\(y = \\Lambda^{\\frac{1}{2}}U^{-1}x\\)​, 具体推导见这里 在数据预处理前, 其可能对于权重非常敏感 (eg. 线性分类器, 数据远离原点, 权重矩阵的微小变化会极大影响远处边界的划分), 预处理后会使得权重矩阵易于被优化 AlexNet 对于数据计算了平均图像并减去, VGGNet 减去了每个通道的平均值, ResNet 减去了每个通道的平均值并除以每个通道的标准值 一般人们会同时做批量归一化和数据预处理 权重初始化 不能把初始权重全部设置为 0 或某个常数, 这缺少了某种对称性破缺(symmetry breaking), 使得无法进行学习 我们最常见的做法是拿小的随机数(符合高斯分布, std=0.01)来做初始化, 这对浅层的网络很有用, 但对深层网络不是很有用 例如上图是一个用 tanh 的六层网络, 梯度正在逐步向 0 崩溃(梯度消失), 就没得学了, 把 std 调大一点则可能会梯度爆炸(梯度随着层数指数型增长) 一种好用的初始化方法叫做 Xavier Initialization, 设置 W = np.random.randn(Din, Dout) / np.sqrt(Din) 没有超参数, \\({\\rm std} = 1/\\sqrt{n}\\), 其中 \\(n\\)​ 是输入参数的个数 这背后的数学思想是我们想要使输入和输出的方差相等, 这部分推导可以见 PPT P59 这样设置对于 tanh 很好, 但是对于 ReLU 和其他激活函数, 仍然可能梯度消失或爆炸 上图是 ReLU 应用了 \\({\\rm std} = 1/\\sqrt{n}\\) 的情况, 我们要重新进行数学推导, 此时要设置 \\({\\rm std} = 2/\\sqrt{n}\\) (正态分布 + ReLU 时), 可以见这篇文章的介绍, 这叫做 MSRA Initialization, 也叫做 Kaiming Initialization 又是你何恺明 对于残差神经网络, 为了防止直接将输入增加到输出上时方差在 block 间不断变大 \\({\\rm Var}(F(x)+x) &gt; {\\rm Var}(x)\\), 我们将第一个卷积层做 MSRA, 而将第二个卷积层置为 0 我们已经介绍过 L2 正则化来防止模型过拟合了, 但还有一些其他的正则化想法, 下面介绍 Dropout Dropout 法如其名, 在前向传播的过程中, 我们随机将一些神经元置 0 以防止神经元适应相同的特征, 强迫网络产生冗余 每个神经元被 \"drop\" 的概率为超参数, 一般设置为 0.5 例如在上图中我们就能学习更多猫的特征 在测试环境是我们需要让 Dropout 有一定稳定性, Dropout 引入了随机掩码(mask) \\(z\\), 所以我们最后的输出 \\(y = f(x,z) = E_z[f(x,z)] = p \\times f(x)\\), 其中 \\(p\\) 为 drop 的概率 因为可学习参数大多都在全连接层中, 我们一般就在那时使用 Dropout, 对于一些其他网络(GoogleNet, ResNet), 它们使用了全局平均池化, 就不用 Dropout 了, 现代网络一般还是用 L2 正则化的核心思想: 训练时加入一定随机性, 测试时去除随机性(通过平均化或其他方法) 还有一种随机性引入的方法是数据增强(data augmentation), 例如图像反转, 图像裁剪, 图像旋转, 颜色抖动(color Jitter), 添加像素偏移量(offset)等等 其他概念有 DropConnect, Fractional Max Pooling, Stochastic Depth, Cutout, Mixup, ..., 自己试试吧 Lecture 11 训练神经网络(下) 感谢 爱吃蛋黄派 - 知乎 的笔记与解析, 本节课主要讲神经网络训练时以及训练后的一些技巧 每种最优化算法都有学习程度(learning rate)这个超参数, 如果设置低了, 训练时长会增加, 如果设置高了, 可能快速收敛到一个不够低的 loss, 一般我们以高 learning rate 开始, 当 loss 稳定后转为低 learning rate 一种策略是步进(Step)降低 learning rate, 例如每 30 个 epoch 将 learning rate 乘以一个 0.1 的系数, 你会注意到当 learning rate 降低时 loss 会骤减, 最终不再骤减并稳定, 这种方法显然引入了一坨超参数 设置 learning rate 还有一种余弦策略(Cosine), 根据图中的式子设置 learning rate: 一般训练时间越长越好, 你就只用调整初始 learning rate 就行 相似的策略为线性衰减(linear decay) \\(\\alpha_t = \\alpha_0(1-t/T)\\) 以及逆平方衰减(inverse sqrt) \\(\\alpha_t = \\alpha_0 / \\sqrt t\\)​ ​逆平方衰减用的不多因为 learning rate 掉的太快了 最常用的策略其实还是设置一个 learning rate 常量, 用上 Adam 最优化就能达到很好效果 你应该训练你的模型多久？下面介绍早停策略(Early Stopping): 训练集的准确率会不断增加, 但验证集的准确率会先增后减, 你应该在验证集的准确率减少时立即停止训练 你应该如何选择超参数？ 可以网格搜索(grid search), 为每个超参数选择几个值(一般是对数线性的), 然后把所有组合全试一遍 可以随机搜索(random search), 为每个超参数划一个选择区间, 每次在区间内随机试一个组合 一般随机选择更好, 因为随机搜索可以尝试更多重要超参数的值, 这篇论文论证了这点 还有一种策略是用梯度下降的方法学习超参数, 目前还过于昂贵 像我们这样的穷鬼怎么选超参数呢？ (1). 检查最初损失, 例如 Softmax 函数的初始 loss 约为 log(C) (2). 在小样本上(5~10个)关闭正则化并过拟合至 100%, 确保最优化成功 (3). 找到合适的学习率(learning rate)使得 loss 下降, 做 100 次循环左右 (4). 在一个很小的超参数网格(每个参数选 2~3 个)上进行网格搜索, 训练 1~5 epochs, 可以尝试学习率衰减 (5). 选择 (4) 中的最好模型, 在没有学习率衰减前提下训练 10~20 epochs (6). 观察学习曲线, 你需要观察 loss 图像以及 acc 图像, 巨大的训练集和验证集准确率差距意味着过拟合, 而过小的差距意味着模型不够拟合, 尝试使用更大的模型 (7). 返回 (5). 还有一些经验方法来查看事情是否出错, 例如权重更新值/权重值应该在 1e-3 左右 现在你训练出来了一些模型 一种简单提高准确率的方法是将多个模型的输出取平均, 会提高 1%~2% 的准确率 一种抽象的方法是在训练时设置学习率的循环, 最后相当于使用多个模型在训练时的快照 另一种抽象方法是测试时不使用最终的训练权重, 而是选择训练参数的运行平均值 机器学习术语解释: 迁移学习(Transfer learning)是属于机器学习的一种研究领域, 它专注于存储已有问题的解决模型, 并将其利用在其他不同但相关问题上。比如说, 用来辨识汽车的知识(或者是模型)也可以被用来提升识别卡车的能力 在计算机视觉领域, 迁移学习十分重要, 以训练 CNN 为例, 其思想如下: (1). 在一个大数据集上(例如 ImageNet)训练模型 (2). 删除最后一层(分类层), 冻结剩下层的权重, 相当于用 CNN 提取了图像特征 (3). 加入我们需要的分类层, 在任务的数据集上训练(可选) 微调(Fine-Tuning): 如果我们拥有较大的数据集, 我们可以在原来网络的基础上进行微调训练, 可以冻结层数较低的层以节省训练资源, 建议以原始学习率的 1/10 进行训练 如果数据集较小(无法微调), 并且与 ImageNet 差距很大的话, 迁移学习效果就不好了 还有一种分布式训练(distributed training)的思想 你可以以数据并行的方式在每一个 GPU 上复制模型, 原理大致见下: 如果你有很多 GPU, 可以通过并行 GPU 来提高采样的数量(Large-Batch Training), 这样可以加速训练, 但注意批归一化只能在 GPU 内部进行 在具体实现上, 如果有 k 个 GPU, 我们将 batch size 改为 kN 并把 learning rate 改为 kα, 为了防止学习率过高, 我们让其从 0 开始在前 5000 个迭代中慢慢增长 Assignment 3 MLP &amp; CNN MLP 用模块化的方法在 CIFAR-10 上实现全连接神经网络(多层感知器 MLP) 总体来讲先要实现一个线性层的类(class Linear), 类里面有前向传播和反向传播的方法 1234567891011121314def linear_forward(x, w, b): newx = x.reshape(x.shape[0],-1) #[N, d_1, ..., d_k] =&gt; [N, D] out = newx.mm(w)+b.reshape(1,b.shape[0]) #[N, M] cache = (x, w, b) return out, cacheLinear.forward = linear_forwarddef linear_backward(dout, cache): # dout 为上游梯度, cache 为 (x,w,b) 的 tuple x, w, b = cache db = dout.sum(dim = 0) # 返回 sum, 在后续处理 dx = dout.mm(w.t()).reshape(x.shape) # [N, d1, ..., d_k] newx = x.reshape(x.shape[0],-1) # [N, D] dw = newx.t().mm(dout) return dx, dw, dbLinear.backward = linear_backward 然后实现一个 ReLU 的类(class ReLU), 包含前向传播和反向传播, 略去 这样结合一下, 我们就有了一个 Linear_ReLU 类, 再弄个 softmax_loss 函数算 loss 和 dx, 下面开始封装一个 linear - ReLU - linear - softmax 架构的两层神经网络类, 大概是这样的 1234567891011121314151617181920212223242526class TwoLayerNet(object):# linear - ReLU - linear - softmax 架构def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0, dtype=torch.float32, device='cpu'): self.params, self.reg = {}, reg self.params['W1'] = weight_scale*torch.randn(input_dim, hidden_dim, device=device) self.params['b1'] = torch.zeros(hidden_dim, device=device) self.params['W2'] = weight_scale*torch.randn(hidden_dim, num_classes, device=device) self.params['b2'] = torch.zeros(num_classes, device=device) # 初始化权重和偏置量, 权重根据 weight_scale 比例高斯分布def loss(self, X, y=None): W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] out1, cache1 = Linear_ReLU.forward(X,W1,b1) # 前向传播 linear - ReLU scores, cache2 = Linear.forward(out1,W2,b2) # 前向传播 linear if y is None: return scores loss, grads = 0, {} loss, ds = softmax_loss(scores,y) # softmax 算 loss, d(scores) loss += self.reg*torch.sum(W1*W1) + self.reg*torch.sum(W2*W2) # 正则化 dh, dW2, db2 = Linear.backward(ds,cache2) # 反向传播 linear dX, dW1, db1 = Linear_ReLU.backward(dh,cache1) # 反向传播 linear - ReLU dW2 += 2*self.reg*self.params['W2'] # 正则化 dW1 += 2*self.reg*self.params['W1'] # 正则化 grads['b2'], grads['W2'] = db2, dW2 grads['b1'], grads['W1'] = db1, dW1 return loss, grads 一个多层的 MLP 架构为 (linear - ReLU - [dropout]) x (L - 1) - linear - softmax, 其中 dropout 是可选项, L 为网络总层数 先写好 dropout, 这里要求写 inverted dropout, 不要在测试时激活缩放系数 p, 而是写进训练阶段, 具体好处见这里 12345678910111213141516171819def dropout_forward(x, dropout_param): p, mode = dropout_param['p'], dropout_param['mode'] mask, out = None, None if mode == 'train': p = 1-p # 这里的 p 是 drop 概率而不是保留概率, 故 p = 1-p mask = (torch.rand(x.shape)&lt;p)/p # 除以 p, 使得训练和测试期望值相同 out = x * mask elif mode == 'test': out = x # test 环境不 dropout cache = (dropout_param, mask) return out, cacheDropout.forward = dropout_forwarddef dropout_backward(dout, cache): dropout_param, mask = cache mode = dropout_param['mode'] dx = None if mode == 'train': dx = dout * mask elif mode == 'test': dx = dout return dxDropout.backward = dropout_backward 然后我们可以模仿两层的 MLP 来封装这个 FullyConnectedNet 类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class FullyConnectedNet(object): def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10, dropout=0.0, reg=0.0, weight_scale=1e-2, seed=None, dtype=torch.float, device='cpu'): &quot;&quot;&quot; 初始化 FullyConnectedNet 类 输入: - hidden_dims: python list, 包含每个隐藏层的大小 - input_dim: 输入的大小 - num_classes: 需要分类的种类数 - dropout: 0~1 的标量, 代表 drop 的概率 - reg: (float), 为 L2 正则化因子 - weight_scale: 初始权重根据 weight_scale 比例高斯分布 - seed: 随机数种子 - dtype: 数据种类 - device: 设备种类, 'cpu' 或 'cuda' &quot;&quot;&quot; self.use_dropout = dropout != 0 self.reg, self.dtype = reg, dtype self.num_layers = 1 + len(hidden_dims) self.params = {} self.params['W1'] = weight_scale*torch.randn(input_dim, hidden_dims[0], device=device, dtype=dtype) self.params['b1'] = torch.zeros(hidden_dims[0], device=device, dtype=dtype) L = self.num_layers for i in range(2,L): self.params['W'+str(i)] = weight_scale*torch.randn(hidden_dims[i-2], hidden_dims[i-1], device=device, dtype=dtype) self.params['b'+str(i)] = torch.zeros(hidden_dims[i-1], device=device, dtype=dtype) self.params['W'+str(L)] = weight_scale*torch.randn(hidden_dims[L-2], num_classes, device=device, dtype=dtype) self.params['b'+str(L)] = torch.zeros(num_classes, device=device, dtype=dtype) self.dropout_param = {} if self.use_dropout: self.dropout_param = {'mode': 'train', 'p': dropout} if seed is not None: self.dropout_param['seed'] = seed def loss(self, X, y=None): # 算 loss 和梯度 X = X.to(self.dtype) mode = 'test' if y is None else 'train' if self.use_dropout: self.dropout_param['mode'] = mode L, h = self.num_layers, X caches, drop_caches = [], [] for i in range(1,L): # 正向传播 Layer 1 ~ L-1 w, b = self.params['W'+str(i)], self.params['b'+str(i)] h, ci = Linear_ReLU.forward(h,w,b) caches.append(ci) if(self.use_dropout): # Dropout h, di = Dropout.forward(h, self.dropout_param) drop_caches.append(di) # 正向传播 Layer L scores, cL = Linear.forward(h,self.params['W'+str(L)], self.params['b'+str(L)]) caches.append(cL) # If test mode return early if mode == 'test': return scores loss, grads = 0.0, {} loss, ds = softmax_loss(scores,y) # softmax 算 loss, d(scores) for i in range(L): loss += self.reg*torch.sum(self.params['W'+str(i+1)] ** 2) dout, dw, db = Linear.backward(ds,caches[L-1]) # 反向传播 linear, dw = dWL grads['W'+str(L)] = dw + 2*self.reg*self.params['W' + str(L)] grads['b'+str(L)] = db for i in range(L-1,0,-1): # 反向传播 linear - ReLU, 从 layer L-1 ~ 1 if self.use_dropout: dout = Dropout.backward(dout,drop_caches[i-1]) # 有 dropout 先 dropout 反向 dout, dw, db = Linear_ReLU.backward(dout,caches[i-1]) grads['W'+str(i)] = dw + 2*self.reg*self.params['W' + str(i)] grads['b'+str(i)] = db return loss, grads 发现 dropout 效果真不错啊真不错, hidden_size 越大, 效果也是越好 之后照着 Lecture 4 的代码抄抄又实现了 SGD+Momentum、RMSProp 和 Adam, 发现 Adam 还真好用 CNN 原作业建议在 colab 上完成, CNN 要通过 google drive 导入一部分上文 MLP 的代码, 在本地完成作业的我直接把 MLP 的 .py 代码全导进去再跑一遍了, 希望没有什么副作用...... 第一步是实现卷积类(class Conv) 先写卷积层的前向传播, 先 padding, 这里用到 torch.nn.functional.pad 函数, 输入张量和一个 tuple, 对最后的 tuple/2 维度进行左右 padding, 我只省了 N 的显式循环, 写了剩余的三重循环来做激活图上的一个元素 12345678910111213141516171819202122232425262728293031323334def conv_forward(x, w, b, conv_param): &quot;&quot;&quot; 显式循环实现卷积层的前向传播, 输入包含 N 张图片, 每张 C 个通道, 高 H, 宽 W 有 F 个卷积核, 每个 C 个通道, 高 HH, 宽 WW 输入: - x: 输入图片, shape (N, C, H, W) - w: 卷积核, shape (F, C, HH, WW) - b: 偏置 shape (F,) - conv_param: 存以下参数的字典 - 'stride': 步长 - 'pad': 填充, 四周都要填 输出 - out: 特征图 shape (N, F, H', W'), H' 和 W' 由以下式子决定 H' = 1 + (H + 2 * pad - HH) / stride W' = 1 + (W + 2 * pad - WW) / stride - cache: (x, w, b, conv_param) &quot;&quot;&quot; pad = conv_param['pad'] stride = conv_param['stride'] pad_x = torch.nn.functional.pad(x,(pad,pad,pad,pad)) N, C, H, W = pad_x.shape F, C, HH, WW = w.shape nh, nw = 1+(H-HH)//stride, 1+(W-WW)//stride out = torch.full((N,F,nh,nw),0,dtype=torch.float64) for idx in range(F): for h in range(0,H,stride): for j in range(0,W,stride): if h+HH&gt;H or j+WW&gt;W: break i1, i2 = h//stride, j//stride out[0:N,idx,i1,i2] = torch.sum(pad_x[:,:,h:h+HH,j:j+WW]*w[idx],dim=[1,2,3])+b[idx] out = out.cuda() cache = (x, w, b, conv_param) return out, cacheConv.forward = conv_forward 现在想想反向传播怎么写, 我们还是去观察 shape, 有 x[N,C,H,W], W[F,C,HH,WW], b[F,], out[N,F,H',W'] 先发现 db 为 out 在 [0,2,3] 为上求 sum, 再发现每个 x, w 都是多次乘法累加出来的, 我们模仿上文先写好三重循环, 倒着模拟前向传播, 乘上上游梯度就行了 注意一开始先 padding 后卷积, 所以要反向传播完后\"收缩\"回去 1234567891011121314151617181920212223242526272829def conv_backward(dout, cache): &quot;&quot;&quot; 输入: - dout: 上游梯度 - cache: 前向传播中的 tuple (x, w, b, conv_param) 输出: dx, dw, db &quot;&quot;&quot; x, w, b, conv_param = cache w = w.cuda() db = dout.sum(dim=[0,2,3]) pad = conv_param['pad'] stride = conv_param['stride'] pad_x = torch.nn.functional.pad(x,(pad,pad,pad,pad)) dx, dw = torch.full(pad_x.shape,0), torch.full(w.shape,0) dx, dw = dx.cuda(), dw.cuda() N, C, H, W = pad_x.shape F, C, HH, WW = w.shape for idx in range(F): for h in range(0,H,stride): for j in range(0,W,stride): if h+HH&gt;H or j+WW&gt;W: break i1, i2 = h//stride, j//stride # 前向传播贡献 out[0:N,idx,i1,i2] ddout = dout[:,idx,i1,i2].reshape(N,1,1,1) ww=w[idx].reshape(1,C,HH,WW) dx[:,:,h:h+HH,j:j+WW] += ww*ddout # reshape 后广播为 [N,C,HH,WW] dw[idx] += torch.sum(pad_x[:,:,h:h+HH,j:j+WW]*ddout,dim=0) dx = torch.nn.functional.pad(dx,(-pad,-pad,-pad,-pad)) # 收缩 return dx, dw, dbConv.backward = conv_backward 然后写池化层(这里是 max pooling)的前向传播和反向传播, pytorch 的 max 函数比它的 sum 函数蠢多了, 我只能写四重循环, 注意反向传播的时候索引的处理, 看到一种很优美的写法, 我们可以抽象出来每次卷积的区域, 后面再接上 [row, column] 对区域的下标赋值, 有: dx[img, c, i*stride:i*stride+HH, j*stride:j*stride+WW][row, column] = dout[img,c,i,j] 更多代码略 实际的工程代码用我们这种多重显式循环的写法就完蛋了, 作业中给出了 torch.nn 的写法, 一个 torch.nn.Conv2d 就解决了, 还帮你做了一下速度比对, 但我电脑跑不出来因为时间太短导致计算加速倍率时会除以 0 ...... 这下我们又能抽象出来 Conv_ReLU 层 和 Conv_ReLU_pool 层, 下面作业先要求搭一个三层的卷积网络, 架构为 conv - ReLU - 2x2 max pool - linear - ReLU - linear - softmax, 又要求搭一个多层卷积网络, 和上文 MLP 作业类似, 虽然难写但总能磨出来, 代码同样略去 对于多层卷积网络, 我们要写一个 Kaiming initialization 进行权重初始化防止梯度爆炸, 主要分四种情况, 有无 ReLU 以及是卷积层还是线性层 12345678def kaiming_initializer(Din, Dout, K=None, relu=True, device='cpu', dtype=torch.float32): gain = 2. if relu else 1. weight = None if K is None: # 线性层 weight = torch.randn((Din, Dout), device=device, dtype=dtype) / math.sqrt(Din/gain) else: # 卷积层 weight = torch.randn((Din, Dout, K, K), device=device, dtype=dtype) / math.sqrt((Din*K*K)/gain) return weight 这里的 gain 还会因为正态分布或是均匀分布有所不同, 具体情况见 Lecture 10 现在就可以调超参了, 在 CIFAR-10 的测试集上最终跑出了 72.9% 的准确率, 真是感人啊！ 接下来就要实现 batch normalization(BN) 层的前向传播和反向传播 建议直接用 torch.var 算方差, torch.mean 算均值, 自已幽默手写的精度会差一点, 一个小细节是要在用 torch.var 时设置 unbiased = False, 否则会使用贝塞尔修正 对于 \\(\\hat x_{i,j} = \\displaystyle\\frac{x_{i,j} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}}\\) 怎么做反向传播, 作业让我们翻原论文去, 我们在第四页可以找到 主要代码见下, 注意 eps, 在 test 时候使用的是在训练阶段累积的全局均值和方差, 代码不同: 123456789mode, x, xhat, gamma, beta, mean, var, eps = cache if mode == 'train': N = x.shape[0] dxhat = dout*gamma # [N, D] dvar = (dxhat*(-1/2)*(x-mean)*((var+eps)**(-1.5))).sum(dim=0) # [D] dmean = dvar*((2*mean-2*x).mean(dim=0)) - (dxhat/torch.sqrt(var+eps)).sum(dim=0) #[D] dx = dxhat/(torch.sqrt(var+eps)) + dvar*(2*(x-mean))/N + dmean/N #[N, D] dgamma = (dout*xhat).sum(dim=0) #[D] dbeta = dout.sum(dim=0) #[D] 如果找不到式子就只能推计算图了, 这篇文章给出了 BN 层的计算图, 可以参考 实战中我们可以换一种简单的计算图表达出 \\(x\\), 因为 \\(\\mu = \\displaystyle\\frac{1}{N}\\sum\\limits_{k=1}^N x_k, \\; v = \\displaystyle\\frac{1}{N}\\sum\\limits_{k=1}^N(x_k-\\mu)^2\\) 再抽象出一些有用的值, 比如 \\(\\sigma = \\sqrt{v + \\epsilon}\\), 以及标准分 \\(\\;y_i = \\displaystyle\\frac{x_i - \\mu}{\\sigma}\\)​ 来推式子, 我们可以画出新的计算图 故有 \\(\\displaystyle\\frac{\\partial L}{\\partial x} = \\displaystyle\\frac{\\partial L}{\\partial y} \\times (\\displaystyle\\frac{\\partial y}{\\partial \\sigma} \\times \\displaystyle\\frac{\\partial \\sigma}{\\partial v} \\times (\\displaystyle\\frac{\\partial v}{\\partial x} + \\displaystyle\\frac{\\partial v}{\\partial \\mu} \\times \\displaystyle\\frac{\\partial \\mu}{\\partial x}) + \\displaystyle\\frac{\\partial y}{\\partial \\mu} \\times \\displaystyle\\frac{\\partial \\mu}{\\partial x})\\), 这里我们收到了上游梯度 $ {} = $ 我们从简单的开始算, \\(\\displaystyle\\frac{\\partial \\mu}{\\partial x} = \\frac{1}{N}\\), \\(\\displaystyle\\frac{\\partial v}{\\partial x} = \\frac{2}{N}(x-\\mu)\\), \\(\\displaystyle\\frac{\\partial v}{\\partial \\mu} = \\frac{2}{N}(\\mu - x)\\) 再算后面 \\(\\displaystyle\\frac{\\partial \\sigma}{\\partial v} = \\frac{1}{2\\sigma}\\), \\(\\displaystyle\\frac{\\partial y}{\\partial \\sigma} = -\\frac{x-\\mu}{\\sigma^2}\\), \\(\\displaystyle\\frac{\\partial y}{\\partial \\mu} = -\\frac{1}{\\sigma}\\) 故 \\(\\displaystyle\\frac{\\partial \\sigma}{\\partial x} = \\displaystyle\\frac{\\partial \\sigma}{\\partial v} \\times \\displaystyle\\frac{\\partial v}{\\partial x} = \\frac{x-\\mu}{N\\sigma}\\), \\(\\displaystyle\\frac{\\partial y}{\\partial x} = \\displaystyle\\frac{\\partial y}{\\partial \\sigma}\\displaystyle\\frac{\\partial \\sigma}{\\partial x} + \\displaystyle\\frac{\\partial y}{\\partial \\mu}\\displaystyle\\frac{\\partial \\mu}{\\partial x} = \\dots\\) (这么写不严谨, 意思到了就行……) 代码可以再化简几步, 注意 dx 要乘上一个 gamma: 123456789mode, x, xhat, gamma, beta, mean, var, eps = cache N = x.shape[0]std = torch.sqrt(var+eps) # σdgamma = (dout*xhat).sum(dim=0) #[D]dbeta = dout.sum(dim=0) #[D]dmu = torch.sum(dout, dim=0)/Ndvar = 2/N*torch.sum((x-mean)*dout, dim=0)dstd = dvar/(2*std)dx = gamma*((dout - dmu)*std - dstd*(x-mean))/std**2 在 BN 的基础上, 我们进一步提出空间批量归一化(Spatial Batch Normalization), 普通的 BN 层是 [N, D] =&gt; [N, D] 的, 而卷积层的归一化是 [N, C, H, W] 的, 我们希望对各个特征(通道) \\(C\\)​ 进行归一化, reshape 成 [N*H*W, C] 后再 reshape 回来即可 抽象出 Conv_Batch_ReLU_Pool 层后改改 class DeepConvNet(object) 开始训练, 发现 BN 层可以减少网络对学习率的依赖 CNN Assignment 的代码大头还是在 class DeepConvNet(object) 中(依赖于几乎所有我们上文实现过的类), 整个 Assignment 写了挺久, 有一些是copy借鉴网上的, 但我也的确 get my hands dirty 了 Lecture 12 循环神经网络 RNN 目前我们在课堂上讨论的网络都属于前馈神经网络(feed forward network)(FNN), 在这种网络中, 信息仅在一个方向上流动, 从输入节点流向输出节点, 中间可能经过多个隐藏层, 但不会形成任何循环或回路 更具体的, 我们的图像分类网络是 one to one 的, 一个图像对应一个分类标签, 还有一些其他任务, 比如图像字幕(one to many), 视频分类(many to one), 机器翻译(many to many) 等…… 循环神经网络(recurrent neural network)(RNN) 就是用来处理序列数据用的 RNN 的核心思想是其内部有一些状态(internal state)随着序列的处理而更新 我们在每个时间步(time step)上应用递归公式 (Recurrence Formula)来处理向量序列 \\(x\\), 注意在每个时间步上权重矩阵都是相同的 对于 many to many 任务, 计算图是这样的, 只存在 \\(y_T\\) 就是 many to one 任务, 只存在 \\(x_1\\) 就是 one to many 任务 一种应用是机器翻译, 它是一种序列到序列(seq2seq)问题, 一般我们将一个 many to one 和一个 one to many 的 RNN 接起来, 它们各有一个权重 W, 一个叫做编码器(encoder), 一个叫做解码器(decoder) 另一种有趣的应用是机器生成文本, 假设我们训练好了一个模型可以根据喂进去的字母预测下一位字母, 我们可以继续把它预测的字母喂进去, 最终得到完整的语句 在这种任务中, 我们初始喂进去的字母是独热(one-hot)编码的, 只有对应的字母位为 1, 剩下位都是 0, 假设中文有十万字, 这时的权重会是一个过大的稀疏矩阵, 所以我们要引入嵌入层(embedding layer)的概念 嵌入层在输入层与隐藏层之间, 将稀疏矩阵变为稠密矩阵, 同时也能体现输出的相关性, 总的网络长这样: 还有一种有趣应用是图像字幕(image captioning), 我们先把图片喂给 CNN, 再把 CNN 提取出的特征喂给 RNN, 让它一个词一个词描述图像, 一般在 RNN 中有一个 start token 开始, 一个 end token 结尾(如果网络预测到了 end token 就立即停止输出) 在 RNN 中怎么做反向传播？如果序列非常长的话, 计算图可能过大, 我们一般使用时间截断反向传播(truncated backpropagation through time)(TBPTT)算法, 我们将序列分为不同的区块(chunk), 在每个区块中展开计算图计算 loss 并反向传播至那个区块, 直到遍历完整序列 下面观察一下 RNN 的梯度流 当多个 RNN 单元连接时, 我们会重复乘以同样的权重矩阵 W, 要么会梯度爆炸, 要么会梯度消失 当梯度爆炸时, 我们可以做梯度裁剪(gradient clipping), 设置一个梯度上限, 大于上限就乘以一个小系数, 幽默但有用 但是梯度消失怎么办呢？下面介绍一种新的 RNN 架构: 长短期记忆(Long Short Term Memory)(LSTM) LSTM 通过把权重矩阵从梯度流中排除解决了上面的问题 与普通 RNN 每时间步一个状态 \\(h_t\\) 不同, LSTM 有两个状态, 细胞状态(cell state) \\(c_t\\), 和隐藏状态(hidden state) \\(h_t\\) 在每个时间步中我们将用 \\(h_{t-1}\\) 和输入 \\(x_t\\) 来计算四个不同的门值(gate value) \\(i,f,o,g\\) 来更新新的细胞状态和隐藏状态, 图中的 \\(\\odot\\) 表示两个矩阵对应元素进行乘积 我们不直接把 \\(W \\displaystyle\\binom{h_{t-1}}{x_t}\\) 拿来做下一个隐藏状态, 相反, 我们将输出分为四个向量: 输入(\\(i\\))、遗忘(\\(f\\))、输出(\\(o\\))、门(\\(g\\)), 这四个向量中, \\(i,f,o\\) 使用 sigmoid 函数激活([0, 1])而 \\(g\\) 通过 tanh 激活([-1, 1]) 这里 \\(c_t = f \\odot c_{t-1} + i \\odot g\\) 代表 \\(c_t\\) 中使用 \\(f\\) 先部分遗忘 \\(c_{t-1}\\) 的权重, 再尝试输入 \\(i\\), 其中 \\(g\\) 决定程度(+/-) 然后 \\(h_t = o \\odot {\\rm tanh}(c_t)\\) 代表 LSTM 可以通过 输出 \\(o\\), 选择展现每个时间段的部分细胞状态 这时候我们就能注意到反向传播时不会经过权重 W, 只会乘以遗忘 \\(f\\) 的系数, 没有非线性也没有矩阵乘法, 一般我们用 \\(h_t\\) 做预测, \\(c_t\\) 更像是一种单元的私有变量 目前我们讨论的都是单层 RNN, 通过在隐藏序列上再应用一个 RNN, 我们可以得到一个二层 RNN, 一层处理原始数据得到隐藏序列, 一层处理隐藏序列得到结果, 同理你可以构造多层 RNN 还有一些不同的 RNN 架构, 例如门控循环单元(Gated Recurrent Unit)(GRU)介绍见此, 类似于一种简化的 LSTM Google 曾使用强化学习去寻找 RNN 架构 Lecture 13 注意力 Attention ATTENTION IS ALL YOU NEED! 对于 seq2seq 问题, 我们之前使用从编码器 (encoder)中上下文向量 (context vector) \\(c\\) 作为解码器 (decoder) 的输入, 但是有时候一个向量可能不足以解决我们的问题(eg. 翻译一本书) 所以我们选择在编码器的每个时间步上计算一个上下文向量, 解码器可以选择上下文向量, 或者说关注输入序列的不同部分, 我们称之为注意力机制(Attention) 首先, 我们还是要算隐藏层 \\(h_t = f_W(x_t,h_{t-1})\\) 以及最初的解码器状态 \\(s_0\\) 然后我们要通过 \\(s_0\\) 计算对齐分数 (Alignment Scores), \\(e_{t,i} = f_{att}(s_{t-1},h_i)\\), 它是一个标量, 这个分数代表模型认为此处的隐藏状态 \\(h_i\\) 对后文生成有多少用处 之后 \\(e_{t,i}\\) 经过一个 softmax 函数归一对齐得到注意力权重(attention weights), \\(a_{t,i}\\) 可以看成有关的概率 再之后我们就可以计算当前解码器需要的上下文向量, \\(c_t = \\sum\\limits_i a_{t,i}h_i\\) 这里的直觉是, 我们生成的每个单词都取决于输入句子中的一个或多个单词, 图中生成了 \"estamons\"(葡萄牙语), 这个词的意思是 \"我们是\", 它很有可能由 \"we\" 和 \"are\" 生成, 它们的权重应当比较高 下一个时间步, 我们会通过 \\(s_1\\)​​ 算出新的隐藏分数, 然后做注意力权重, 然后生成上下文向量, 以此往复 这一整套流程都是可微分的, 不需要任何监督(不告诉要注意什么), 只需要做反向传播就行了 我们还注意到这种注意力机制并不关心输入本身的顺序性, 它只是在一个无序集合中选最高的的分数罢了, 所以这种架构我们可以用在其他问题上 对于图像描述问题, 我们先使用 CNN 计算得到一张图像的特征, 即一个 \\(h_{i,j}\\) 的二维表格, 然后拿 \\(s_0\\) 算 \\(e_{t,i,j}\\), softmax 得到 \\(a_{t,i,j}\\), 和 \\(h\\) 一起喂进去得到上下文向量 \\(c\\)......, 都是类似的 你可以可视化这些注意力来获得一些直觉, 这里每张图只有一块注意力, 这需要一些额外的技术实现 因为注意力太好用了, 我们想要抽象出一个新的神经网络层, 叫做注意力层(attention layer) 对于单个询问, 我们输入查询向量(query vector) \\(q\\) (形状 [D]) 以及输入向量 \\(X\\) (形状 [N, D]) 然后计算相似性(similarities) \\(e_i = q \\cdot X_i / \\sqrt D\\) (形状 [N], 使用缩放点积(scaled dot-product attention)防止梯度爆炸) 对于多个询问, 有 \\(Q\\) (形状 \\([N_Q, D_Q]\\)), \\(X\\) (形状 \\([N_x, D_Q]\\)), 我们先计算相似向量矩阵 \\(E = QX^{T}, E_{i,j} = Q_i X_j / \\sqrt{D_{Q}}\\) 然后算注意力权重 \\(A = {\\rm softmax}(E, {\\rm dim = 1})\\), 输出 $Y = AX, Y_i = j A{i,j}X_j $ 注意到我们的输入向量 \\(X\\) 被使用了两次, 所以我们要进行拆分, 增加两个不同的可学习的权重矩阵 \\(W_K, W_v\\), 用 \\(K = XW_K\\) 来做相似性, 用 \\(V = XW_V\\) 来做输出, 总的流程如下: 自注意力层(self-attention layer)是注意力层的一个特例, 我们只有一个输入向量 \\(X\\), 将每一个输入向量与其他所有输入向量进行比较(将输入向量转化为询问向量 \\(Q\\)​), 以此确定它们之间的相互关系 如果我们改变输入的顺序, 可以注意到输出的顺序也会同步改变, 这说明自注意力层是有置换同变性(permutation equivariance)(PE)的, 它不关心输入的顺序, 如果我们想要关心顺序的话, 我们要给每个 \\(X_i\\)​ 引入位置编码 (positional encoding), 你可以在这篇文章查看具体细节 掩码自注意力层(masked self-attention layer)是自注意力层的一个特例, 有时我们希望防止模型知道未来的时间步的信息, 所以要相应更改权重掩码(mask) 在一些语言任务中, 我们要对句子进行填充(padding)使它们长度相同, 因为填充部分不参与计算, 所以要 mask 为负无穷, 这样 softmax 后得到 0 的概率 多头自注意力层(multihead self-attention layer)也是自注意力层的一个特例, 我们将输入向量分割成多个部分, 并在多个并行的自注意力层上独立的处理这些部分, 从而增加模型的表示能力 这包含两个超参数, 内部区域的查询维度 \\(D_Q\\), 以及使用的注意力头的数量 \\(H\\)​ 我们目前处理序列的三种方式各有优劣 RNN适合处理长序列但是不能并行计算隐藏状态 1维卷积不适合处理长序列 (需要的卷积层太多)，但是具有高度并行性 自注意力模型既适合处理长序列也具有高度的并行性，但是需要大量的内存 怎么解决这些问题？ATTENTION IS ALL YOU NEED! 我们需要建立新的一种块: Transformer 块 在 transformer 块中, 输入向量 \\(X_i\\) 仅在自注意部分互相交互, 在其余部分都是独立的 这里用层归一化(layer normalization)而不是 batch normalization 是因为: - 它不依赖于批次大小, 这在处理可变大小的输入或在推理时使用不同的批次大小时非常有用。相比之下, 批量归一化可能在小批次或不一致的批次大小下不稳定性 - 层归一化适应顺序数据, 而 transformer 模型通常用于处理顺序数据(如文本或时间序列) - 层归一化更容易与自注意力机制结合使用, 由于自注意力机制处理的是序列中的每个元素, 层归一化的逐元素归一化策略与之更加兼容 transformer 块连一起就搭成了 transformer 模型, 对自然语言处理(Natural Language Processing)(NLP)任务非常有用, 我们可以预训练一个非常大的 transformer 模型, 在此基础上微调(finetune)就可以完成你的 NLP 任务了 2019 年的课程没有讲到 2020 年的 Vision Transformer(ViT), 论文在此 当拥有足够多的数据进行预训练的时候, ViT 的表现会超过CNN, 可以在下游任务中获得较好的迁移效果 但是当训练数据集不够大的时候, ViT 的表现通常比同等大小的 ResNet要差一些 你可以在这篇知乎文章查看其简单介绍 Lecture 14 可视化与理解 神经网络内部寻找的中间特征是什么？我们怎样能有更深的理解？可视化其内部层可以给我们一些神经网络的直觉 对于 CNN, 我们之前可视化过其第一层的卷积核, 它学习了一些基本的边缘与颜色的特征 但是对于更高层的卷积核, 其权重由更高维的信息确定, 我们需要其他技术来获得直觉 我们可以先直接尝试理解最后的全连接层在做什么, 例如对于 AlexNet, 它的 FC7 层有 4096 个特征向量, 我们可以做 kNN, 将每张测试集图像与 4096 个特征向量进行搜索, 理解分类器如何在特征空间学习彼此的接近程度 以第二行的大象为例, 尽管背景有极大区别, 但是 Alexnet 编码出了 \"大象\" 的概念, 很好完成了分类 另一种可视化方法是降维方法 人类难以理解 4096 维空间, 就降维而言, 主成分分析(Principal Component Analysis)(PCA)法十分经典 PCA 的思想很直观, 假设我们要将 n 维特征映射到 k 维, 第一维选择原始数据中方差最大的维度, 第二维选取是与第一维正交的\"平面\"中方差最大的维度, 依次得到 k 维 我们可以用协方差矩阵来做这个东西, 具体见这篇文章 但是还有一种 t-SNE 算法(t 分布随机邻域嵌入)(t-Distributed Stochastic Neighbor Embedding) 专门来做高维数据到二维或三维的降维并可视化, 它是非线性的并且擅长保留局部结构, 不过对超参数很敏感并且运算开销很大, 这里有一篇粗略的 t-SNE 科普, 而这里是一个 t-SNE 可视化 CNN 的样例 我们也可以激活中间的卷积层, 因为激活函数的存在, 可能会有大量的纯黑色图像, 对于一些非零的卷积核, 我们可以将其与图片对齐以猜测其响应了什么特征 例如这个卷积核可能对应人脸 我们接下来引入最大激活区域(maximally activationg patches)的概念, 它描述的是在给定神经网络层级中, 能产生最大激活值的图像片段和区域 例如我们可以对于输入图片输出相应最高的图块, 根据输出的图块猜测网络在寻找什么特征 这种方法也可以来估计哪些输入像素对于网络是真正重要的, 我们比较原图和原图遮蔽(occlusion)后的神经元的响应, 通过相应的区别大小就可以估计输入像素的重要性, 对整张图片的每个部分都尝试抹去, 就可以得到一个神经元的感受野 我们还可以通过反向传播的方式获得输入图像中每个像素的梯度, 这告诉我们对于每个像素其对于决定分类分数的重要性 这其实还引出了一个根据像素梯度来分割图像主体的想法 同理, 我们也可以用反向传播去寻找中间特征, 这里用的是一种 guided backprop, 对上游传过来的负梯度设置为 0, 不知道为什么这会让图像更好看 我们不一定要用测试图像来做可视化, 还可以去合成一个最大化神经元输出的图像, 这引入了梯度上升(gradient ascent)的概念 我们初始化图像为全 0 像素, 通过反向传播来生成图像, 不要忘记加入正则化函数来让图像拥有可解释性 公式是这样的 \\(I^* = {\\rm arg\\;max}_I \\underline{f(I)}_{神经元输出} +\\underline{R(I)}_{正则化}\\)​ 生成的图像会长这样, 这叫做显著图(saliency maps): 更好的正则化函数会让函数更自然, 所选的梯度上升的神经元越高层, 生成的图像就更有语义 但是这其实不完全是 CNN 在真实寻找的, 课程教授更倾向于选择更简单的正则化函数 对抗性样本(adversarial examples)是在一个分类的图片加入另一个分类最大化输出的噪声生成的, 即使噪声人类完全无法看出来, 但神经网络会改变其预测结果 这篇文章介绍了对抗性攻击及其防御措施 下面介绍一下特征反转(feature inversion)的想法 和梯度上升类似, 我们通过前向传播得到图像的特征, 并定义损失函数为另一张图片的特征与我们得到的特征的差异, 尝试最小化这个损失会让我们生成一些奇妙图像 例如, 浅层的图像会与原始图像相似, 越深层学习的特征就越抽象, 越来越多的低级信息丢失(纹理和颜色), 但是保留了图像的整体结构和形状 Google 的 DeepDream 项目尝试放大图像中的特征, 思想是令 \\(I^* = {\\rm arg\\;max}_I \\sum_I f_{i}(I)^2\\), 这样图像会越来越像网络的 \"梦境\", 网络将其学到的所有特征都叠加在原始图像上 例如学习动物的神经网络可以从云彩中看出各种动物来 计算机图形学中有纹理合成(texture synthesis)的概念, 使用 kNN 就能在简单材质上做得很好 若使用上神经网络, 类似特征反转的思想, 我们希望生成与输入图像的局部纹理特征的整体空间匹配的材质 我们使用格拉姆矩阵(Gram matrix), 思想是, 我们通过网络运行纹理图像, 对所有的特征两两之间做外积并平均化得到格拉姆矩阵, 这个矩阵扔掉了所有空间信息但是告诉了我们哪些特征两两关联 通过使用梯度上升匹配目标的格拉姆矩阵, 我们期望得到一些相同纹理的图片 整个流程的思想是这样的: 一些人将纹理合成与特征重建结合起来, 一个提供特征, 一个提供空间结构, 这就弄出来了神经风格迁移(neural style transfer) 你可以把多个风格图像放一起应用在内容图片上, 现在的风格迁移已经足够快够实时迁移视频了 总结一下: 我们可以用 kNN, 降维, 最大激活, 遮蔽, 显著图, 类可视化, 噪声, 特征反转等方法来理解 CNN DeepDream 和 Style Transfer 是我们在可视化过程中得到的一些有趣应用 Assignment 4 自动梯度 &amp; 图像字幕 &amp; 可视化 从本次作业开始我们化身调库侠来发挥 pytorch 的潜力 自动梯度 一般先 import 一些常用方法并弄个缩写 123import torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optim torch.nn 是直接抽象神经网络的层, 而 torch.nn.functional 抽象的是函数 一个双层神经网络前向传播直接这样写: 12345x = flatten(x) # shape: [batch_size, C x H x W]w1, b1, w2, b2 = paramsx = F.relu(F.linear(x, w1, b1))x = F.linear(x, w2, b2)return x 一个三层 CNN 的前向传播这样写, 看 pytorch 文档 来学 conv2d 怎么用: 1234x = F.relu(F.conv2d(x,conv_w1,conv_b1,padding=2))x = F.relu(F.conv2d(x,conv_w2,conv_b2,padding=1))x = flatten(x)scores = F.linear(x, fc_w, fc_b) pytorch 内置了 Kaiming initialization, eg. nn.init.kaiming_normal_(torch.empty(3,5)) 就可以用 Kaiming initialization 初始化权重 W, 记得 W, b 初始化后都要 .requires_grad = True 手动构造张量并用 .requires_grad 跟踪对于大型网络还是太复杂了, 我们直接用 nn.Module 来自动跟踪梯度, 为了完成这种操作, 我们定义的网络应该是 nn.Module 的继承类, 需要包含 __init__() 和 forward() 函数 记得在 __init__() 内做一下 super().__init__() 以下是一个双层神经网络: 12345678910111213class TwoLayerFC(nn.Module): def __init__(self, input_size, hidden_size, num_classes): super().__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, num_classes) nn.init.kaiming_normal_(self.fc1.weight) nn.init.kaiming_normal_(self.fc2.weight) nn.init.zeros_(self.fc1.bias) nn.init.zeros_(self.fc2.bias) def forward(self, x): x = flatten(x) scores = self.fc2(F.relu(self.fc1(x))) return scores 相当于我们直接用 nn.Linear 定义层, 层有 .weight 和 .bias 成员 之后你可以用这个类来定义模型, 直接 print 输出模型架构: 1234567model = TwoLayerFC(input_size, 42, 10)print(model)#---------------------------TwoLayerFC( (fc1): Linear(in_features=768, out_features=42, bias=True) (fc2): Linear(in_features=42, out_features=10, bias=True)) CNN 也同理, 略 torch.optim 可以让我们选择优化器, 例如 SGD, Adam 以 optim.SGD() 为例, 查看官方文档, 要导入模型的参数(model.parameters()), learning rate, 动量, 正则化的衰减等等 一般你要自己写个 train 函数, 把 model 和 optimizer 都扔进去来训练, 以 CNN 为例是这样的: 123model = ThreeLayerConvNet(C,channel_1,channel_2,num_classes)optimizer = optim.SGD(model.parameters(),lr=learning_rate,weight_decay=weight_decay)train(model, optimizer) 如果觉得 nn.Module 还不够方便, 那就用 nn.Sequential 吧, 它把 __init__() 和 forward() 都集成了, 相对而言灵活性不足 这是使用 nn.Sequential 的一个双层神经网络的实现, 你可以给每部分取名: 1234567891011121314model = nn.Sequential(OrderedDict([ ('flatten', Flatten()), ('fc1', nn.Linear(C*H*W, hidden_layer_size)), ('relu1', nn.ReLU()), ('fc2', nn.Linear(hidden_layer_size, num_classes)),]))print(model)#-------------------------------Sequential( (flatten): Flatten() (fc1): Linear(in_features=3072, out_features=4000, bias=True) (relu1): ReLU() (fc2): Linear(in_features=4000, out_features=10, bias=True)) 如果你不想一次搭起来整个网络, 也可以用类似 model.add_module('flatten', Flatten()) 一层层加起来 有了这些模块化, 我们可以搭起一种精简化的 ResNET 的网络叫做 PreResNet 一共要自己实现两种块(block): Plain Block, 结构为 [BatchNorm - ReLU - Conv] * 2, batchnorm 用 nn.BatchNorm2d 即可 Residual Block, 在 Plain Block 的基础上加入了残差链接, nn.Identity() 就是我们需要的恒等映射, 需要根据 Plain Block 下采样与否改变残差链接的形式 然后封装一下就可以实现 ResNet 类了, 具体代码见 class ResNet, 很幽默的是我的 Plain Block 网络和 Residual Block 网络发挥类似, 都在 10 个 epoch 后在 CIFAR-10 上跑到了 80% 的准确率 图像字幕 我们将使用 2014 年的微软 COCO 数据集, 每张图有最多十五个词语的一句话描述, 以 &lt;START&gt; 开头并以 &lt;END&gt; 结尾, 一些罕见词会被替换为 &lt;UNK&gt;, 一些过短描述的 &lt;END&gt; 后会加入 &lt;NULL&gt; 作为 padding 一个句子举例: &lt;START&gt; two bunches of fresh bananas hanging from &lt;UNK&gt; &lt;END&gt; Vanilla RNN 我们先需要实现 RNN 的一个时间步的前向传播和反向传播 前向传播根据 \\(h_t = {\\rm tanh}(W_{h}h_{t-1} + W_{x}x_t + b)\\) 写就可以, 注意对齐 shape 反向传播要开导 \\({\\rm tanh}\\) 了, 计算可得 \\({\\rm tanh'}(x) = (\\displaystyle\\frac{e^x-e^{-x}}{e^x+e^{-x}})' = 1 - {\\rm tanh}^2(x)\\)​​ , 然后也简单, 主要根据 shape 凑就行了, 一个 shape 可能有多种凑法, 多试试 对于整个 RNN 的前向传播和反向传播, 就要写一些显式循环了, 前向传播很容易, 但反向传播时作业给你的 shape 为 [N, T, H] 的 dh 表示所有隐藏状态 \\(h_t\\)​ 的上游梯度, N 是 batch size, T 是时间步, H 为 hidden size 如果你观察 Lecture 12 的 many to many RNN 图示, 你发现它给的是 Loss 的上游梯度, 还缺少来自 \\(h_{t+1}\\) 的梯度, 之前的单步 backward 需要把两个都加起来得到真正的梯度 算了, 弄不清也没关系, 作业随手一个 autograd 就把 error 打到 1e-15 数量级了, 还是学这个有用 我们使用 pytorch 中的 MobileNet v2 来进行图片的特征提取, 作业已经帮你完成了 FeatureExtractor() , 为了表示词汇, 我们还要做词嵌入(word embedding), 即用向量表示词汇, 假设每个词是一个 D 维向量, 每个句子有 T 个词, 我们要做一个 shape 从 [T] 到 [T D] 的变化, 一行 out = self.W_embed[x] 就行, 整个 x 相当于下标传进去了, 出来的 shape 也是对的 每个时间步我们需要过一个仿射层(affine layer)将 RNN 的隐藏向量变成词汇库中每个词的分数, 这部分就是给 nn.linear 的事 每个时间步还要算一个 softmax loss, 直接用 torch.nn.functional.cross_entropy 就行 有了这些就能写 class CaptioningRNN 这个模型了, 一定要弄清不同的大小和 shape, 输入的特征向量是 D 维的, 单词向量是 W 维的, 词汇集大小为 V, 每个句子有 T 个词(时间步), hidden size 为 H, batch size 为 N, 输入图片集的 shape 为 [N,3,112,112], 字幕(captions)的 shape 为 [N,T], 在最后一维上前面 T-1 个为输入, 后面 T-1 个为输出, 这是因为输出会作为下一个时间步的输入 总的流程大体可见这张图, 最好和 Lecture 12 带嵌入层的图一起看 注意 h 到 y 的过程中要先 embedding 得到下一层的 h 后再算 y 和 loss 训练的前向传播一共是五步: CNN 特征到 h0, embedding, 算新的 h, 算得分, 算 softmax loss __init__() 中是这样的: 123456789101112131415161718192021def __init__(self, word_to_idx, input_dim=512, wordvec_dim=128, hidden_dim=128, cell_type='rnn', device='cpu', dtype=torch.float32): &quot;&quot;&quot; 输入: - word_to_idx: 词汇集对应索引的 dict. 共 V 项, 每项对应一个 [0,V) 间整数 - input_dim: 输入的特征向量 D 维特征向量 - wordvec_dim: W 维的词汇向量 - hidden_dim: RNN 的 H 维隐藏状态 - cell_type: RNN 种类, 'rnn' 还是 'lstm' &quot;&quot;&quot; super().__init__() vocab_size = len(word_to_idx) # V self.feature = FeatureExtractor(pooling=True,device=device,dtype=dtype) #得到特征 self.affine = nn.Linear(input_dim,hidden_dim).to(device=device,dtype=dtype) #得到 h0 nn.init.kaiming_normal_(self.affine.weight) nn.init.zeros_(self.affine.bias) self.embed = WordEmbedding(vocab_size,wordvec_dim,device=device,dtype=dtype) #embed self.rnn = RNN(wordvec_dim,hidden_dim,device=device,dtype=dtype) # h [N, T-1, H] self.outp = nn.Linear(hidden_dim,vocab_size).to(device=device, dtype=dtype) # scores nn.init.kaiming_normal_(self.outp.weight) nn.init.zeros_(self.outp.bias) 训练的 captioning_forward() 是这样的, 反向传播 autograd 会自动帮我们算好: 1234567891011def captioning_forward(self, images, captions): captions_in = captions[:, :-1] captions_out = captions[:, 1:] feature = self.feature.extract_mobilenet_feature(images) #[N, H(1280)](pooled) h0 = self.affine(feature) #[N, H] =&gt; [N, V] embed = self.embed.forward(captions_in) #[N, T-1, W] h = self.rnn.forward(embed,h0) #[N, T-1, H] scores = self.outp(h) #[N, T-1, H] =&gt; [N, T-1, V] loss = temporal_softmax_loss(scores,captions_out) return lossCaptioningRNN.forward = captioning_forward 在测试时, 我们显然无法访问真实的字幕 captions, 所以我们在每个时间步从词汇表的分布中采样, 并在下一个时间步时将样本作为输入提供给 RNN, 这要求我们写一个 sample_caption() 函数 弄出来 h0 后, 一开始我们要喂 RNN 一个 &lt;START&gt; token, 之后每个时间步我们需要干四件事: 将上一个词 embed, 用上一个 h 和 embed 后的词得到新的 h, 仿射得到分数, 选出最高得分的词并加入 captions 中 作业中用 max_length 的限制代替了 &lt;END&gt; 后强制停机的限制 注意我们的所有操作都是在一个 N 大小的 batch 上进行的 1234567891011121314def sample_caption(self, images, max_length=15): N = images.shape[0] captions = self._null * images.new(N, max_length).fill_(1).long() feature = self.feature.extract_mobilenet_feature(images) #[N, H(1280)](pooled) h = self.affine(feature) #[N, H] =&gt; [N, V] token = torch.full((N,1),self._start).reshape(N) # 挺幽默的写法, 因为 size 必须为 ints... for i in range(max_length): embed = self.embed.forward(token) # 喂 token h = self.rnn.step_forward(embed,h) # next_h &lt;- embed &amp; prev_h scores = self.outp(h) #[N, V] token = torch.max(scores,1)[1] # 或用 argmax captions[:,i]=token return captionsCaptioningRNN.sample = sample_caption 弄好后跑一遍你会发现训练集上的句子准确率惊人(GT 即 Ground Truth, 基准事实): 而测试集的句子简直错到奶奶家去了, 但好歹语法是对的 LSTM &amp; Attention LSTM 与普通 RNN (又称 vanilla RNN) 相同的是, 每个时间步我们接受输入 \\(x_t \\in \\mathbb R^D\\)​ 以及之前的隐藏状态 \\(h_{t-1} \\in \\mathbb R^H\\)​, 不同的是, 我们还要接受之前的细胞状态(cell state) \\(c_{t-1} \\in \\mathbb R^H\\)​, 同时因为有四个门值, 有 $W_x R^{4H D} $​ , $ W_h R^{4H H}$​ 偏移量 \\(b \\in \\mathbb R^{4H}\\)​ LSTM 的反向传播全部用 autograd 实现, 我们先写一个时间步上的前向传播, 根据 Lecture 12 的图来就行: 设一个中间量 \\(a \\in \\mathbb R^{4H}, a = W_xx_t+W_hh_{t-1} +b\\), 即 \\(a = W \\displaystyle\\binom{h_{t-1}}{x_t}\\) , 有 \\[ \\begin{align*} i = \\sigma(a_i) \\hspace{2pc} f = \\sigma(a_f) \\hspace{2pc} o = \\sigma(a_o) \\hspace{2pc} g = \\tanh(a_g) \\end{align*} \\] 这里的 \\(\\sigma\\) 就是 sigmoid 函数, 再做: \\[ c_{t} = f\\odot c_{t-1} + i\\odot g \\hspace{4pc} h_t = o\\odot\\tanh(c_t) \\] 就行了, 注意在代码中是用 batch 的, 所以实际上是 \\(A \\in \\mathbb R^{N \\times 4H}, X_t \\in \\mathbb R^{N \\times D}\\)​ 12345678def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b, attn=None, Wattn=None): A = x.mm(Wx) + prev_h.mm(Wh) + b #[N, 4H] N, H = prev_h.shape [i,f,o] = torch.sigmoid(A[:,:3*H]).split(H,dim=1) #[N, 3H] g = torch.tanh(A[:,3*H:4*H]) #[N, H] next_c = f*prev_c + i*g #[N, H] next_h = o*torch.tanh(next_c) #[N, H] return next_h, next_c 然后写整个序列上的前向传播, 和之前一样, 代码略, 再改改 class CaptioningRNN 类, 发现结果并无区别...... 所以说, ATTENTION IS ALL YOU NEED, 相交于 LSTM, 还需要一个注意力输入 \\(x_{attn}^t \\in \\mathbb R^H\\), 这个 \\(x_{attn}^t\\) 是通过 CNN 的特征 \\(\\tilde A \\in \\mathbb R^{H \\times 16}\\) 和注意力权重 \\(\\tilde M_{attn}^t\\) 得到的 注意力权重 \\(\\tilde M_{attn}^t\\) 就是 Lecture 13 的相似向量矩阵 \\(M_{attn}^t\\) flatten + softmax 的结果, \\(\\tilde A\\) 是 \\(A\\) flatten 的结果 有 \\(M_{attn}^t=h_{t-1}A/\\sqrt{H} \\in \\mathbb{R}^{4\\times4}\\) , 所以 \\(\\tilde M_{attn}^t \\in \\mathbb{R}^{16}\\), 最后 \\(x_{attn}^t=\\tilde{A}\\tilde M_{attn}^t \\in\\mathbb{R}^H\\) , 具体解释可以见作业, 这部分有点抽象, 我们实际上是在实现一个缩放点积函数 然后写一个时间步上的前向传播, \\(a=W_xx_t + W_hh_{t-1}+W_{attn}x_{attn}^t+b\\)​​​, 其他都一样, 改一行就行 然后写整个序列上的前向传播, 也很简单, 然后改改 class CaptioningRNN 类, 发现自己做不到 1e-7 的 loss diff, 但训练一下, loss 下降的比 1e-12 的 diff 都快, 搞不懂 Attention LSTM 做图像字幕会更准确 可视化 &amp; 风格迁移 在可视化部分主要是可视化显著图(saliency maps), 做对抗攻击以及类可视化 显著图需要我们计算正确分类的 loss 之和, 然后反向传播求输入的梯度, 取最大绝对值通道来可视化 需要用到 torch.gather() 函数, 将标签 y 作为索引, 从所有分数中获取正确分数, 可以见这篇文章了解其用法 123456789101112131415161718def compute_saliency_maps(X, y, model): &quot;&quot;&quot; 输入: - X: 输入图像, shape (N, 3, H, W) - y: X 的标签, shape (N,) - model: 预训练的模型 输出: - saliency: 显著性张量, shape (N, H, W) &quot;&quot;&quot; model.eval() X.requires_grad_() scores = model(X) #[N, 1000] all classes(C = 1000) correct_scores = torch.gather(scores, 1, y.view(-1,1)) # gather only correct class loss = torch.sum(correct_scores) loss.backward() grad = X.grad #[N, 3, H, W] saliency = torch.abs(grad).max(dim=1)[0] return saliency 在对 saliency 可视化后得到的显著图是这样的: 对抗性攻击要求我们通过微调输入图像 X, 通过梯度上升使其分类为 target_y 我们把 target_y 对应的分数反向传播, 让 X 往 dX 方向移动 12345678910111213def make_adversarial_attack(X, target_y, model): X_adv = X.clone() X_adv = X_adv.requires_grad_() learning_rate = 1 for i in range(1,10000): score = model(X_adv) #[1, 1000] if score.argmax(dim=1)==torch.tensor(target_y): break score[0,target_y].backward() grad = X_adv.grad #[1, 3, H, W] dX = learning_rate * grad / torch.norm(grad,2) with torch.no_grad(): # 原地操作时不要算梯度 X_adv += dX return X_adv 可视化后是这样的: 类可视化要求我们从随机噪声开始执行梯度上升生成目标类的图像, 目标是 \\(I^* = {\\rm arg\\;max}_I (s_y(I) - \\lambda \\|I\\|_2^2)\\), 其中 \\(s_y(I)\\) 是图像在 \\(y\\) 分类的分数, 我们设 \\({\\rm loss} = s_y(I) - \\lambda \\|I\\|_2^2\\) 后做反向传播即可, 代码略, 可视化略 风格迁移靠的也是 loss 函数, 这个 loss 由三部分构成, content loss, style loss 和 total variation loss(去噪平滑用) content loss 主要保留特征, 获得原图的特征图和目标的特征图后算一下 L2 距离, 乘一个 loss 权重就行 loss = content_weight * torch.sum((content_current-content_original)**2) style loss 保留风格, 用 Gram 矩阵放大特征, 然后算 Gram 矩阵的 L2 距离即可, 回顾 Lecture 14 的图, Gram 矩阵本质就是一个 \\(A^TA\\), 代码略 total variation loss 使图像更平滑, 所以我们要惩罚相邻像素值的波动, 总目标为 \\(L_{tv} = w_t \\times \\left(\\sum_{c=1}^3\\sum_{i=1}^{H-1}\\sum_{j=1}^{W} (x_{i+1,j,c} - x_{i,j,c})^2 + \\sum_{c=1}^3\\sum_{i=1}^{H}\\sum_{j=1}^{W - 1} (x_{i,j+1,c} - x_{i,j,c})^2\\right)\\)​ 就是横竖各在 RGB 通道上来一遍, 乘上一个总权重​, 代码很简洁 1234H, W = img.shape[2], img.shape[3]loss = torch.sum((img[0,:,:H-1,:]-img[0,:,1:,:])**2)\\ + torch.sum((img[0,:,:,:W-1]-img[0,:,:,1:])**2)return loss * tv_weight 然后你就可以实现最简单的风格迁移了, 个人觉得效果糊糊的不是很好 Lecture 15 目标检测 除图像分类外, 计算机视觉的其他任务还有很多, 本节课着重关心目标检测(object detection)任务 我们输入一张 RGB 图像, 期望得到一个对象的集合, 对于每个对象包含标签以及一个边界框(bounding box), 这就是目标检测任务, 难度在于多分类输出, 多种输出(标签和框), 以及要求处理更高分辨率的图像 如果只检测单个对象, 我们只需要对网络加入新的分支, 用一层全连接层将特征向量降为四维(即边界框的 [x,y,width,height]), 用另一个损失函数训练这个, 并设置总损失为两损失之和即可 如果要检测不定个数的对象, 可以使用滑动窗口(sliding window)算法, 假设有一个训练好的 CNN, 我们可以让它对输出图像的子区域(窗口)进行分类, 每个子窗口输出一个类别, 如果想检测 N 个类别, 总共会输出 N+1 个类别(包含背景) 我们如何知道有哪些子区域需要分类呢？总的可能性数量是 H*W 的, 再考虑所有的窗口大小, 复杂度可能是 \\(O(H^2W^2)\\)​ 的, 这种算法需要某种外部算法提供一组有更高可能性的候补区域(region proposals) 2014 年的 R-CNN 论文很好完成了图像分类任务, 其中 R 代表 Region, 代表这是一个基于区域的 CNN 我们从输入图像开始, 然后运行候补区域算法, 大概会生成 2000 个区域图像, 我们将这 2000 个图像 resize 为 224*224 的图像(warped image), 对于每个图像独立跑一遍 CNN, 并给出分类以及边界框 R-CNN 需要额外学习边界框, 施加某种变化(transform), 希望边界框更好地包含图像, 这是一个回归(regression)的想法 这其中有很多指标和细节, 我们来下面一一介绍 比较边界框需要用到交并比(Intersection over Union)(IoU)的概念, 将边界框的交集(预测框与基准框)面积除以并集面积得到 IoU 上图左下和右上的紫色部分是作图的失误 评价指标为: IoU &gt; 0.5, 还行; IoU &gt; 0.7, 挺好; IoU &gt; 0.9, 几乎完美 对象检测算法可能对于相同对象输出多个检测框, 此时我们需要应用非极大值抑制(Non-Maximum Suppression)(NMS)算法来消除冗余的边界框 NMS 听起来高级, 其实就是一类贪心, 以这张图为例, 我们选择最高分的框, 把与它 IoU &gt; 0.7 的框全丢了就行, 然而对于高度重叠的多个对象这方法就不好用了 我们如何评价我们的对象检测器？这引入了平均精度(mean Average Precision)(mAP)指标, 流程是这样的: (1). 跑一遍对象检测器 (2). 对每个分类计算精度(AP), 为准确度-召回率曲线(Precision Recall Curve)(PR curve)下的面积, 在这里我们一般认为 IoU &gt; 0.5 为判断正确, 此时我们会减少一个对应的基准事实(GT)边界框 数学术语解释: 见 freeman449 - 知乎 科普, 先给四个定义: True Positive (TP)：预测为正, 实际为正(判断正确) True Negative (TN)：预测为负, 实际为负(判断正确) False Positive (FP)：预测为正, 实际为负(判断错误) False Negative (FN)：预测为负, 实际为正(判断错误) 所以准确率为 \\({\\rm p=presicion} = \\displaystyle\\frac{TP}{TP+FP}\\), 表示在所有预测为是这一类的样本中, 有多少确实是这一类 召回率 \\({\\rm r = recall} = \\displaystyle\\frac{TP}{TP+FN}\\)​, 表示在所有实际上是这个类的样本中, 有多少被判断出来了 如果让一个已经训练好的模型在测试集上运行一次, 准确度和召回率是确定的, 以横轴为 r, 纵轴为 p 可以画出 PR 曲线(累计形式) 目标是获得同时具有高准确度和召回率的分类器, 在图形上的表现是曲线与坐标轴围成的区域面积尽可能大 (3). 得到 mAP , 即所有类别 AP 的平均 这个指标平衡了准确率和召回率, 被广泛接受 上文的 R-CNN 显然会因为做 2000 个 CNN 而很慢, 我们可以通过交换 warped image 和 CNN 部分, 共享一部分计算来提高效率 这种算法叫 Fast R-CNN, 因为大部分运算是在所谓的 backbone network (骨干部分, 对于 Alexnet, 指其中 CNN 的部分) 中完成的 这其中还有对候选区域裁剪的反向传播问题, 我们引入 感兴趣区域池化(Rol Pool) 这种池化方式, 对不固定尺寸输入图输出固定尺寸特征图, 本质上就是先将区域先映射到特征图上再划分为需要的输出部分进行 max pooling, 可以想象两次映射会丢失很多边缘的数据 Rol Align 可以解决这个问题, 大概思想是不对其单元格, 在区域内采样对应输出 size 的点做双线性插值最终得到输出, 这样也能解决反向传播问题, 可以见这篇文章介绍 Fast R-CNN 的训练时间能做到 R-CNN 的 1/10, 测试时间为 1/40 左右, 并且大部分时间都拿来算候补区域了 如果我们用 CNN 来算候补区域, 引入一个很小的区域候选网络(Region Proposal Network)(RPN), 这就叫做 Faster R-CNN 名字真烂 这个 RPN 是这样工作的: 先想象 backbone network 已经输出了特征图, 有一个固定大小的锚框(anchor box)在特征图上滑动, RPN 其实是在训练一个二分类问题, 即锚框是否包含一个对象 固定大小的框很蠢, 我们还要训练一种边界框变化的参数(box transforms), 把锚框变为边界框(上图黄框) 实践中, 有多种大小的锚框, 有很多超参数要调(锚框数量、比例、大小......) 总结一下, 我们的 Faster R-CNN 有四种 loss, 分别是 RPN 和 CNN 的分类与回归(regression) loss 如果想要继续优化 Faster R-CNN, 我们需要考虑能不能将定位和分类两个部分放到一起, 这叫做单阶段目标检测, 代表算法有 SSD 和 YOLO, 我们直接输出所有类别和边界框变化的方法叫做类别特定回归, 让模拟对每个类别专门回归会使得表现更好 如果遍历所有的方法(截至 2017 年), 所有的骨干网络, 你会得到一些观察, 一般双阶段目标检测更好, 但单阶段目标检测更快, 骨干网络肯定是越大越好 当年最大的 mAP 只有 40 不到, 现在在 COCO 最高已经有 66 了(DETRs), 可以自己去看排行榜 自己实现目标检测十分困难, 你应该去用别人的轮子 Lecture 16 检测与分割 先补充一下上节课笔记, 如何训练这些 R-CNN 算法？ 在原始的 R-CNN(slow R-CNN) 中我们使用一个 CNN 对候选区域分类, 有正类、负类和中性类, 它们可以通过与 GT 的 IoU 来判断, 这里的阈值也是超参数 一般训练时会忽略中性框, 然后将剩下候选区域裁剪, reshape, 将这些部分单独过一个 CNN 来预测类别和边界框回归变换, 对于负类(例如背景)只用预测类别, 没有回归损失(regression loss) 下面介绍一种不依赖于锚框的单阶段目标检测方法 CornerNet, 主要思想是改变边界框的参数化方式, 只用左上角和右下角来表示边界框 对于我们要预测的每类对象, 我们会预测出一个 \"左上角概率热图\" 和 \"右下角概率热图\", 对于每个像素还预测一个嵌入向量来决定哪些左上角和右下角匹配 语义分割(semantic segmentation)任务也是计算机视觉的一大分支, 目标是标记图中的每个像素到一个类别中, 并不关心同一类别的不同对象区别 仍然可以用最简单的滑动窗口法来完成这个问题, 但一般我们使用全连接卷积网络(fully convolutional network)(FCN)来做这个问题 全连接卷积网络只有卷积层, 最终输出是通道数为分类数的三维张量 为了做出更好的决策, 我们希望决策依赖于输入图像中一块较大的区域, 如果用 3*3 的卷积核需要很多层才能积累到较大的感受野, 这样的计算可能在高精度图像上非常昂贵 于是我们需要通过下采样(downsampling)和上采样(upsampling)来加速过程 上采样又被称为反池化, 有很多想法, 例如单纯填 0, 或者做最近邻算法(复制粘贴), 聪明一点的方法是做双线性插值 这样显然更平滑, 我们还可以做三次近似, 这一般是默认的算法 还有一种 Max Unpooling 算法, 它记住了 Max Pooling 时最大数的位置, 在反池化时将数填到对应的位置上, 其它位置置 0 上采样方法一般取决于下采样方法, 我们上文提到的上采样方法没有可学习的参数 一种可学习的上采样方法叫做转置卷积(transposed convolution), 步长大于 1 的卷积可以理解为一种可学习的下采样, 我们如果使得步长小于 1, 岂不是就在做可学习的上采样, 输出大于输入了？ 它的实现是这样的, 我们用输入张量的元素乘以卷积核得到 3*3 的输出, 将输出加到输出张量上, 对重叠部分求和, 然后移动一格输入, 移动两格输出, 继续反卷积下去 在计算机视觉和语义分割中, 有 \"stuff\" 和 \"thing\" 的概念 \"stuff\" 是不可数的, 通常为背景的物体, 例如草地, 天空, 没有明确的形状大小 \"thing\" 是可数的, 独立存在的物体, 例如人, 车, 有明确形状和大小, 可以被区别为不同的实例 语义分割任务更进一步就到了实例分割(instance segmentation), 要求把所有 \"thing\" 的实例都分割出来, 例如牛群中的每头牛, 这个任务基于目标检测算法, 对于每个检测出的对象, 要输出一个分割掩码(segmentation mask) 这叫做 Mask R-CNN, 建立在 Faster R-CNN 的基础上, 对每个检测出的对象上再跑一个 FCN 结构, 跑得非常好 这些分割算法可以应用在人体位姿估计, 在人体各处设置多个关键点, 在 Mask R-CNN 上再加一个头(Head)来预测关键点就行 加个 \"头\" 真的十分有用, 我们甚至可以从 2D 转向预测 3D 形状, 下节课会讲述 3D 视觉方面的工作, stay tuned! Assignment 5 目标检测 19 年的部分代码在 google colab 上都跑不起来了……, 例如 next() 要改成 __next__(), cv2.rectangle() 要导入整数坐标, 感谢 deepseek 解决这些问题 单阶段目标检测 YOLO 图像 shape 为 [B, 3, 224, 224], 其中 B 代表一个 Batch 内有 B 张图 GT shape 为 [B, N, 5], N 代表图像中有 N 个物体, 最后一维为 [x_tl, y_tl, x_br, y_br, class], 包含边界框左上(tl)右下(br)的坐标和物体分类 本部分作业由 CNN 激活图(7 * 7)定义坐标系, 左上为 (0, 0), 右下为 (7, 7), 所有坐标为浮点精度 backbone network 用的 MobileNet v2 作业给了锚点框所有大小的 list, 以及帮我们把图像分割为了 7 * 7 样式, 我们先弄出来所有锚点框的坐标 shape 为 [B, A, H', W', 4], A 为锚点框 list 的 index, 最后一维为 [x_tl, y_tl, x_br, y_br], [b, h, w] 为锚点框中心位置 123456789A = anc.shape[0]B, H, W, _ = grid.shapeanchors = torch.zeros((B,A,H,W,4), device=anc.device)for i in range(A): xs = grid[:,:,:,0] ys = grid[:,:,:,1] dx, dy = anc[i] anchors[:,i,:,:,:] = torch.stack((xs-dx/2, ys-dy/2, xs+dx/2, ys+dy/2), dim = 3)return anchors 这里的锚框还不是最后的边界框, 对于 YOLO 算法, 设 \\(-0.5\\leq t^x,t^y\\leq 0.5, t^w, t^h \\in (-\\infty, \\infty)\\), 则我们变换 \\[ x_c^p = x_c^a + t^x,\\; y_c^p = y_c^a + t^y,\\; w^p = w_a exp(t^w),\\; h^p = h_a exp(t^h) \\] 来得到最终边界框, 这里的 \\(w, h\\) 来源于我们将 \\((x_{tl}^a, y_{tl}^a, x_{br}^a, y_{br}^a)\\) 变为了中心点加上长宽的 \\((x_c^a,y_c^a,w^a,h^a)\\) 形式 现在来算 IoU, 边界框 shape 为 [B, A, H', W', 4], GT shape 为 [B, N, 5], 我们要求输出一个 [B, A*H'*W', N] 的 IoU 矩阵, 矩阵下标 [b, i, n] 的元素表示第 b 个边界框和下标为 [b, n] 的 GT 的 IoU 问题是给你两个矩形的左上与右下坐标, 交面积怎么算？ 答: 两个左上点选出最大的横纵坐标作为新的左上点, 两个右下点选出最小的横纵坐标作为新的右下点即可 理解这个后代码就很好写了, 略 然后作业帮你写了个挺长的函数来确定锚点的分类标签+正负类, 这部分要自己读代码去理解 接下来就要自己架一个预测网络了, 对于 7*7 特征网格中的每个位置要输出 C 个数字对应 C 类的分类分数, 同时对于每个位置的 A 个锚点要输出变化参数和一个置信度分数, 所以整个网络的 shape 变化是 [B, 1280, 7, 7] =&gt; [B, 5A+C, 7, 7], 这里的 1280 是 backbone network 最后一层的大小 作业给出的架构建议是 1x1conv - dropout - LeakyReLU - 1x1conv, 直接上 nn.Sequential 吧 具体还是见作业中 PredictionNetwork 类的实现, 我猜想因为 python 库的原因, 直接复制网上题解出来都没有 1e-7 的 error 了……, 注意训练和推理的前向传播逻辑不同, 训练实现要多套一层函数 损失函数弄好后, 就可以实现 class SingleStageDetector 了 读入图像和 GT 的边界框后, 前向传播一共有五步 图像特征提取 - 网格与锚生成 - IoU 计算, 正负类锚划分, GT信息获取 - 跑预测网络 - 算总 loss 123456789101112131415B = images.shape[0]features = self.feat_extractor(images) #[B, 1280, 7, 7]grid = GenerateGrid(B) anchors = GenerateAnchor(self.anchor_list,grid) #[B, A, H', W', 4]anchors = anchors.to(bboxes.device)anc_per_img = torch.prod(torch.tensor(anchors.shape[1:-1])) B, A, H, W, _ = anchors.shapeiou = IoU(anchors,bboxes)act_anc_ind, neg_anc_ind, GT_conf_scores, GT_offsets, GT_class, _, _ =\\ReferenceOnActivatedAnchors(anchors,bboxes,grid,iou,neg_thresh=0.2)conf_scores, offsets, class_scores = self.pred_network(features, act_anc_ind, neg_anc_ind)conf_loss = ConfScoreRegression(conf_scores,GT_conf_scores)reg_loss = BboxRegression(offsets,GT_offsets)cls_loss = ObjectClassification(class_scores,GT_class,B,anc_per_img,act_anc_ind)total_loss = w_conf*conf_loss + w_reg*reg_loss + w_cls*cls_loss 事实上每步就几行代码, 感谢封装 然后训练模型, 这次模型就不好练了, 跑 50 个 epoch, 拿 google colab 会快很多…… 下面来实现一下 NMS, 作业让我们参考 pytorch 的 /torchvision/csrc/cpu/nums_cpu.cpp 来做这个, 其流程是三步: 选最高分框 - 扔框 - 如果剩余框, 回到第一步 这里需要 torch.argsort 函数, 让 bboxes 按照 scores 从高到低排序 直接按照参考写二重循环根本跑不出来, 毕竟 python 不是 c++, 需要加入一点并行运算 12345678910111213141516171819def mini(x, ys): # Calc IoUs of x and ys if len(ys.shape) == 1: ys = ys.reshape(1,4) ws = torch.minimum(x[2],ys[:,2]) - torch.maximum(x[0],ys[:,0]) hs = torch.minimum(x[3],ys[:,3]) - torch.maximum(x[1],ys[:,1]) ws[ws&lt;0]=0 hs[hs&lt;0]=0 si = ws*hs s1 = (x[2]-x[0])*(x[3]-x[1]) s2 = (ys[:,2]-ys[:,0])*(ys[:,3]-ys[:,1]) return si/(s1+s2-si) #[nums]idx, keep = torch.argsort(scores,descending=True), []while idx.shape[0]: keep.append(idx[0]) ious = mini(boxes[idx[0]],boxes[idx]) idx = idx[torch.where(ious &lt;= iou_threshold)]if topk != None: keep = keep[:topk] # 是否只保留前 k 个keep = torch.tensor(keep, **to_long_cuda).to(scores.device)return keep 现在来完成推理时的前向传播, 和训练的前向传播类似, 区别是我们先只保留 conf_score &gt; thresh 的锚框, 然后将剩下的锚框过一遍 NMS(torchvision.ops.nms), 注意在 torch.no_grad() 上进行 最后的测试我这边因为 matplotlib 版本需要改一下 main.py 的部分代码, 在炫酷的窗口动画跑完后会得到 10% 左右的 mAP, 2019 年的最高记录是 80% mAP 左右, 我训练 500 个 epoch 也到不了啊…… 双阶段目标检测 Faster R-CNN 先写区域候选网络 RPN, 第一部分是 class ProposalModule() 对于 ProposalModule, 只要预测偏移量(offsets)(reg layer) 以及建议得分(proposal scores)(cls layer) 这里的 K 即上文的 A (锚数量), 架构建议是 3x3conv - dropout - LeakyReLU - 1x1conv, 对于每个位置的 A 个锚点要输出变化参数和两个置信度分数(物体类还是背景), 整个网络的 shape 变化是 [B, 1280, 7, 7] =&gt; [B, 6A, 7, 7] ProposalModule() 的前向传播和上文 PredictionNetwork() 类似, 略 然后写 RPN 的前向传播, 相当于不带分类预测的 SingleStageDetector 的前向传播, 慢慢磨吧 对于第二阶段, 引入的 RoI Align 还算有趣, 其他并无太大不同, 整个双阶段目标检测的前向传播一共五步: RPN 部分 - RoI Align 和 meanpool - 通过分类层得到类概率 - 算 cls_loss 和 rpn_loss - 算总 loss 主要要学一下 pytorch roi_align 的用法, 其他都类似, 最后能跑出 16% 的 mAP Lecture 17 3D 视觉 本节课将专注于两类 3D 问题, 分别是预测 3D 形状, 以及根据 3D 形状做分类/分割任务 3D 视觉涉及的几何更多, 该领域存在更多非深度学习的传统算法 3D 形状有很多表示方法, 各有优劣, 下面一一介绍 深度图(depth map)是最基础的表示方式, 对于输入图像中的每个像素, 它分配相机到该像素的距离, 这不是一个完整的 3D 表示, 因为它不能捕捉被遮挡的结构, 可以被称为 RGB-D 图像或 2.5D 图像 这里有一个相关的任务叫做深度预测, 这其实是一个 FCN 的想法 这个任务的问题是尺度/深度歧义(scale/depth ambiguity), 对于一张图片无法区别远处大物体和近处小物体 道理我都懂, 但是鸽子为什么这么大？ 这个问题可以通过改变损失函数来解决, 如果网络预测的深度为整个深度的一个固定比例, 我们认为这是 0 loss 的, 这里的相关论文会更详细介绍这个 loss 函数 另一种 3D 形状表示叫做表面法线(surface normals), 想法上和深度图很相似, 只不过我们给每个像素分配一个垂直于该表面的单位向量(法线), 也可以弄一个 FCN 来训练, 损失函数为法向量间的角度差异 缺点还是不能表示被遮挡的部分 体素网格(voxel grid)没有这样的问题, 它把三维空间分为了一系列的小立方体(体素), 每个体素都能存储信息(颜色、密度、表面法线......), 这不就是 minecraft 吗 体素(Voxel) 是 体积(Volume) 和 像素(Pixel) 的结合词, 相当于三维空间的像素 这种表示对于高分辨率需要大量的存储空间(1024^3 的体素网格需要约 4GB), 处理速度会很慢 处理体素网格分类问题需要三维卷积, 一个三维卷积核(立方体)会滑来滑去来计算内积 通过 RGB 图像预测体素网格需要在 CNN 后将 2D 特征转化为 3D 特 征, 我们通过一个全连接层来完成这点 3D 卷积非常昂贵, 有时人们用体素管(voxel tubes)的思想只使用 2D CNN 来完成这个任务, 但有很多限制, 略 为了解决体素网格占用空间过大的问题, 我们可以引入多重分辨率的体素, 使用低分辨率体素去构建主要结构, 高分辨率体素去构建细节, 一般用八叉树(Oct-trees)来实现这点 另一种技巧是嵌套形状层(nested shape layers), 我们把整个形状用多个稀疏的体素层相加减来表示 下面介绍隐式表面(implicit surface)的 3D 形状表示法, 我们想要将 3D 形状表示为一个隐式函数, 所以我们要学习一个函数, 使得输入一些坐标可以输出这个位置被占据的概率, 例如概率为 1/2 代表为表面 其思想是这种函数给出了三维空间中的点到表面的欧几里得距离, 距离的正负决定了点在对象的内外, 我们使用神经网络来学习这个函数 这有一大部分属于计算机图形学的内容, 这里有一篇简单的科普 点云(point cloud)的思想是将物体表示为三维空间中的点集, 其具有丰富的空间信息, 但是缺少语义信息 点云较体素网格稀疏很多, 这里的可视化是将点当成小球来处理的 在自动驾驶中一般就用点云来表示周围的环境, 然后来做出对应的决策 处理点云的著名网络叫做 PointNet, 读入一组点, 其中点的顺序不重要, 然后输出特征, 再做分类 最好还是看一点科普吧...... 我们同理也可以对于一张 RGB 图像, 获得其点云输出, 这里的损失函数十分有趣, 需要输出两个点集的相似性, 还要可微 这里的损失函数用上了倒角距离(chamfer distance), 公式为 \\[ d_{CD}(S_1,S_2) = \\sum\\limits_{x \\in S_1} \\underset{y \\in S_2}{\\rm min}||x-y||_2^2 + \\sum\\limits_{y \\in S_2} \\underset{x \\in S_1}{\\rm min}||x-y||_2^2 \\] 直观理解是这样的, 我们先对所有蓝点找到最近的橙点, 然后计算 L2 距离之和, 之后对所有橙点找到最近的蓝点, 然后计算 L2 距离之和, 再把两者相加 所以 loss = 0 的唯一可能性是两个点云完全重合, 这个 loss 函数与点的顺序完全无关, 符合我们的要求 最后我们介绍计算机图形学中常用的三角网格(triangle mesh)的概念, 三维物体的表面被划分为许多小的三角形片, 它有适应性, 支持几何运算, 复杂平面表示, 但同时也代表我们很难用神经网络来弄三角网格, 这个领域的经典网络是 MeshCNN 以及 Pixel2Mesh 我们介绍一下后者 这里面有几个思想: 一是迭代精细化(iterative refinement), 我们不从头创建网格对象, 而是有一些初始模板(例如上图的椭球), 然后通过不断的更新输出匹配的三角网格 二是图卷积(graph convolution), 层的输入是一个图以及附加到图各个顶点的特征向量, 输出是各个顶点新的特征向量, 对于一个顶点, 其特征向量应该取决于其局部感受野的输入特征向量 三是顶点对齐特征, 我们希望将图像信息混合到图卷积神经网络(GCN)中, 这里我们将三角网格投影到图像上, 对于每个投影点用双线性插值, 使得顶点的特征向量与图像平面中特征的位置对齐, 类似于 RoI-Align 四是损失函数, 一种形状有多种表示, 我们希望损失函数不受不同表示方式的影响, 所以我们将 GT 采样为点云(离线), 同时将预测网格采用为点云(在线), 然后跑 chamber distance 在二维中我们用 IoU 来比较形状, 到三维时我们可以用上文的那种方式, 算 chamber distance 来比较 3D 图形 问题是 chamber distance 对离群值十分敏感, 一般我们用 F1 score 来比较 3D 形状 数学术语解释: 回顾一下 Lecture 15 的介绍 准确率 \\({\\rm p=presicion} = \\displaystyle\\frac{TP}{TP+FP}\\), 表示在所有预测为是这一类的样本中, 有多少确实是这一类 召回率 \\({\\rm r = recall} = \\displaystyle\\frac{TP}{TP+FN}\\), 表示在所有实际上是这个类的样本中, 有多少被判断出来了 F1 score 为精确率和召回率的调和平均数, \\(F_1 = 2 \\times \\displaystyle\\frac{p \\cdot r}{p + r}\\) 我们想象将每个预测点扩大为一个球体, 若 GT 点落在内部则认为预测正确 如果更关心精确率或召回率可能需要别的指标 在 3D 形状中, 我们需要选取坐标系, 一般可以用规范坐标系(canonical coordinates), 对每类物体规定一个标准方向, 例如椅子的前方就代表 Z 轴正方向 另一种选择是视图坐标系(view coordinates), 以相机为原点, 更容易实现 规范坐标系倾向于过拟合, 而视图坐标系泛化更好 3D 形状有一些常用的数据集, 例如 ShapeNet(大型数据集, 合成, 孤立物体) 以及 Pix3D(小型数据集, 真实, 更多上下文) Mesh R-CNN 是 R-CNN 的三维版本, 给出 RGB 图像中每个对象的 3D 形状 我们希望得到一个三角网格, 但是初始模板会限制一些变形(拓扑上不一致), 所以我们可以先生成一个粗糙的体素预测, 然后将其变为三角网格, 再进行精细化迭代 最后需要一些正则化来帮助可视化, 如果结果不尽如人意, 可能是 2D 对象检测的问题, 例如在遮挡的部分猜错了 Lecture 18 视频 视频本质上就是一个四维张量, 两个空间维度, 一个时间维度, 一个通道维度, shape 为 [T, 3, H, W] 与图像分类识别物体(猫, 狗, 汽车, etc.)不同, 视频分类识别的是不同的动作, 例如游泳, 跳跃, 跑步等 这里的问题是视频很大, 1920x1080 的三十帧视频每分钟会产生 10GB 左右的数据 解决方法很多, 例如在 3~5 秒的短视频上训练, 对帧率下采样或者对分辨率下采样等…… 在测试时, 我们会在原始视频的不同片段上预测, 最终平均化预测并得到最终分类 最简单的 视频分类模型是单帧 CNN(Single-Frame CNN), 只独立的对视频的各个帧进行分类, 忽略上下文信息(时间结构), 我们一般拿这个模型作为基准(baseline), 其实已经足够好了 稍微复杂一点的模型叫做后期融合(Late Fusion), 在单帧 CNN 的基础上, 把所有单帧特征融合并 flatten, 然后在上面跑 MLP 分类 后期融合可能会过拟合, 小改进是用全局池化 + 线性层来做分类 后期融合的问题出在其难以学习帧期间的微小运动, 例如图中人物的抬脚 所以我们提出早期融合(Early Fusion), 先 reshape 输入的四维张量, 把通道与时间放到一维上, 输入一个 [3T, H, W] 的张量, 跑 CNN, 得到分类, 使用这种方法我们就能学习相邻帧之间的运动了, 但是这种学习只存在了一层, 可能不足够让模型学会 改为 3D CNN 可以捕捉视频的时间和空间信息, 但是会很慢, 训练也很困难 如果比较上述三种方法的架构和感受野, 会发现 Late Fusion 会在末尾突然扩张时间维度的感受野, Early Fusion 会在开头突然扩张时间维度的感受野, 3D CNN 则是慢慢扩张, 因此也称为 \"Slow Fusion\" 2D 卷积和 3D 卷积有明显不同, 在视频上的 2D 卷积没有时移不变性, 2D 卷积核在一小块空间以及整个时间轴上, 课程的例子是如果我们要学习图像颜色由橙到蓝的转变, 并且一个视频在多个时间点有这个转变, 我们的卷积核只能学习一个时间点上的转变, 即使这种转变是时移不变的, 对于每个时间点要重新弄一个卷积核 对比而言, 3D 卷积核仅在一个空间和时间的小区域上, 然后进行滑动, 所以我们可以仅用单个卷积核识别由橙到蓝的转变 视频领域的常用数据集有 Sports-1M, 对一百万个运动视频的运动种类(共 487 种)进行了标注, 分的很细, 所以仅辨别 \"running\" 是不行的……, 仅用单帧 CNN 就能达到 77% 的分辨率, 比 Early Fusion 还高一点 一个三维视觉中有名的模型是 C3D 模型, 可以理解为 VGG + 3D CNN, 这个网络的启示是我们能不能让网络对运动更敏感, 因为人类能很容易从运动本身获取很多信息, 下面是一个由三个点表示的打篮球和跳舞的例子, 你能看出来吗? 为了具体描述运动信息, 我们介绍光流(optical flow)的概念 光流法计算视频相邻帧之间的位移矢量场, 能够突出场景中的局部运动 这方面的一篇著名文章提出了双流网络(Two-Stream Networks)的框架, 它有两个并行独立的 CNN 栈, 一个处理空间, 仅对单帧预测分类; 另一个 CNN 仅处理运动信息, 通过 Early Fusion 将所有光流场处理分类, 最后我们取预测的平均值 上述的这些架构都只能说是短期结构(仅 5 秒左右/仅相邻帧), 对于这种序列问题, 我们为什么不用 RNN 呢？ 一种想法是我们先用 CNN 在每个时间点提取局部特征, 然后用某种 RNN 架构(LSTM?) 融合信息, 我们其实在 Lecture 12 笔记提到过用 RNN 做 many to one 的视频分类 这里的小技巧是仅在 RNN 上反向传播, CNN 仅作为一个特征提取器, 这样就可以在一个长时间上训练模型 多层 RNN 的想法也可以在这里应用 这种想法其实可以叫做 Recurrent CNN, 但这种架构无法并行处理, 可能会在长序列时较慢 我们可以继续考虑注意力机制, 这里有名的工作是 2018 CVPR 的 Non-Local Network 我们先得到 3D 版本的自注意力层, 用 3D CNN 获得特征后算 \\(Q, K, V\\), 得到注意力权重后再过一个 1x1x1 的 conv, 再加上残差链接后就得到了自注意力层, 在论文中叫做非局部块(Nonlocal Block) 因为有残差链接, 这个非局部块可以插入 3D CNN 中微调应用, 非常有用, 最后架构大概是这样的 下面介绍几种好用的 3D CNN 架构, 一个想法是将成熟的 2D CNN 架构膨胀(inflating)为 3D CNN 架构 将二维卷积改为三维卷积, 二维池化改为三维池化, 加一个额外的时间维就行了 更进一步的想法是膨胀预训练权重, 将图像集上的训练权重在时间维度上复制后继续训练可以减少模型的收敛时间, 事实上这样的训练效果会更好 2019 年的 SlowFast Network 取消了对外部光流的依赖, 仅对原始像素操作, 这个网络分为快慢两个分支, 慢分支以非常低的帧率运行, 但是使用大量通道; 快分支在高帧率上运行, 使用很少的通道, 同时有快分支指向慢分支的横向连接来融合信息, 最后用一个全连接层来预测 视频领域的另一个任务是时空动作定位(Temporal Action Localization)不但识别出来多个动作, 还确定各个动作发生的时间跨度, 这个任务可以用类似 Faster R-CNN 的架构来解决 更有挑战性的任务是空间时间检测(Spatio-Temporal Detection), 这包括目标检测, 目标动作分类, 确定各个人各个动作发生的时间跨度 可视化视频模型和之前的可视化类似, 对于分类找到它的最大得分输出, 这部分可以见视频 67mins 处, 可以来通过输出猜猜对应的运动 Lecture 19 生成式模型(上) 没有数理基础这部分会很难懂, 我会后续补充这部分推导 -2025.3.23 我们首先要分清楚监督学习(supervised learning)和无监督学习(unsupervised learning) 监督学习需要数据集(需要人类注释), 包含数据(x)和期望输出(y), 监督学习的目标就是学习一个 f: x-&gt;y 无监督学习只有数据(x), 无需人类注释, 目标是学习数据的某种隐藏结构, 这里的任务有聚类、降维(PCA / t-SNE)、特征分析(自编码器)、密度估计等 我们想要区分判别模型(discriminative models)和生成模型(generative models), 它们的一处不同在于概率结构 在判别模型中, 我们想要学习 p(y|x), 然而生成模型想要学习 p(x), 其中条件生成模型想要学习 p(x|y) 以分类任务为例, 因为 p(x) 是归一化的( \\(\\sum p(x) = 1\\)​​​ ), 这会带来某种图像的\"竞争\", 在 p(y|x) 中, 存在的是标签(y)的竞争而不是图像(x)的竞争, 判别模型必须为图像选择一个分类, 即使图像本身不在分类中, 这可以说是对抗性攻击的一个基础 而生成式模型需要有对视觉世界有深刻理解, 从而拒绝不合理的图片为一个现实中的分类, 比如说现实中不存在两条腿的狗 根据贝叶斯公式 \\(P(x|y) = \\displaystyle\\frac{P(y|x)}{P(y)}P(x)\\)​, 这些模型之间不是独立的, 说明条件生成模型可以由判别模型和生成模型构造, 关键还是如何架构生成式模型 生成式模型可以检测离群值, 我们可以对生成式模型采样来生成新的图像, 条件生成模型则进一步能根据要求的分类(y)来生成图像(AIGC?) 生成式模型的主要分类为显式密度函数(能计算 p(x))和隐式密度函数(不能计算 p(x), 但能采样得到 p(x)) 这其中又有更细的分类, 我们会讨论其中的三种生成模型 自回归模型(autogegressive models)算是最简单的生成式模型, 思想是输入数据(x)和可学习的权重(W), 然后获得概率密度函数 \\(p(x) = f(x, W)\\) 对于显式密度函数, 我们希望数据集的可能性(likelihood)被最大化 再用 \\(\\sum\\) 把 \\(\\prod\\) 转化一下后可以得到损失函数 \\({\\rm loss} = {\\rm arg\\;{max}}_W \\sum_i {\\rm log} f(x^{(i)}, W)\\) 我们的自回归模型就是这个 \\(f\\) 假设原始数据 \\(x = (x_1, x_2, x_3, \\dots, x_T)\\), 由很多小部分组成(例如图片由像素组成), 则 \\(p(x) = p(x_1, x_2, x_3, \\dots, x_T) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\dots = \\displaystyle\\prod\\limits_{t=1}^Tp(x_t|x_1,\\dots, x_{t-1})\\) 这部分可以用 RNN 处理, 我们以 PixelRNN 举例来介绍自回归模型在图像生成上的应用 从左上角开始, 我们对每个像素用 LSTM 算 RGB, 然后往右边和下方扩展, 每个像素是显式依赖于左边和上方的像素, 并隐式依赖于左上方所有的像素的 这里的 RNN 很慢并且开销很大, 不适合生成高清图像, 所以论文中还提出了 PixelCNN 的架构, 这里有一篇科普 从左上开始, 我们用一个卷积核想要在有限的感受野内预测当前像素, 然而因为当前右下的像素还未生成, 卷积核的右下部分要 mask 一下 然而因为自回归模型要对像素一一采样, 生成速度还是较慢 变分自编码器(Variational Autoencoders)(VAE)要抽象一点, 这里是论文 在 VAE 中我们不能计算概率密度函数的精确值, 但可以求出它的一些下界, 所以我们的目标转变为最大化概率密度函数的下界 什么是自编码器(autoencoders)? 普通的自编码器是一种无监督学习方法, 旨在学习图像的特征表示 我们希望通过输入 \\(x\\) 训练一些特征 \\(z\\), 如果能从 \\(z\\) 中重建出输入 \\(\\hat{x}\\), 说明特征 \\(z\\) 是有效的 从 \\(x \\rightarrow z\\) 的过程叫做 encode, 从 \\(z \\rightarrow x\\) 的过程叫做 decode, 我们要训练两个神经网络 encoder 和 decoder, 结合起来就叫做自编码器, 这里的损失函数为 \\(||\\hat{x} - x||_2^2\\), 这里的 \\(z\\) 要相对于 \\(x\\) 而言非常低维, 否则就不能叫做特征了 在训练结束后, 我们就不需要 decoder 了, 而是使用 encoder 来完成其他下游任务, 它是无监督的学习模型, 不是概率性的, 所以无法生成新图像; 另一方面来说, 它的表现也堪忧 所以 VAE 引入了概率, 我们希望学习特征 \\(z\\), 并且能够从训练好的模型中采样生成新数据 在训练结束后, 我们对特征 \\(z\\) 赋予一个先验分布概率, 然后 decoder 会输出图像的概率分布, 这里的输出概率是通过输出每个像素的平均值 \\(\\mu_{x|z}\\) 和协方差矩阵对角线 \\(\\sum_{x|z}\\) 完成的(假设潜在分布满足正态分布, 不输出整个矩阵因为过大了) 如何训练这个模型？我们还是要最大化 \\(\\sum\\log p(x^{(i)})\\) 如果对于 \\(x\\) 我们能观察 \\(z\\), 我们可以训练条件生成模型 p(x|z), 但我们观察不到 \\(z\\), 一个尝试是边缘化(marginalise)未观察的 \\(z\\), \\(p_\\theta(x) = \\displaystyle\\int p_\\theta(x,z){\\rm dz} = \\displaystyle\\int p_\\theta(x|z)p_\\theta(z){\\rm dz}\\) 但是我们无法积分, 寄, 所以回到贝叶斯公式, \\(p_\\theta(x) = \\displaystyle\\frac{p_\\theta(x|z)p_\\theta(z)}{p_\\theta(z|x)}\\) 这里的问题是没法算 \\(p_{\\theta}(z|x)\\), 所以我们尝试训练另一个网络(encoder)学习 \\(q_\\phi(z|x) \\approx p_\\theta(z|x)\\) KL 散度是两个分布之间差异的度量, 一通推导后 \\(\\log p_\\theta(x)\\) 函数可以表示为 \\[ \\begin{aligned} \\log p_\\theta(x) &amp;= E_{z \\sim q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{KL}(q_{\\phi}(z|x), p_\\theta(z)) + D_{KL}(q_{\\phi}(z|x), p_\\theta(z|x)) \\\\ &amp;\\geq E_{z \\sim q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{KL}(q_{\\phi}(z|x), p_\\theta(z)) \\end{aligned} \\] 最后一项我们没法算, 所以我们通过使得 \\(q_\\phi(z|x) \\approx p_\\theta(z|x)\\) 把这项直接扔掉了, 这就是我们不能计算概率密度函数的精确值的原因, 这是一种经典的概率论技巧 Lecture 20 生成式模型(下) 训练 VAE 的流程是这样的, 我们想要最大化 \\(E_{z \\sim q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)] - D_{KL}(q_{\\phi}(z|x), p_\\theta(z))\\) 先取一个 batch 的输入 x, 跑 encoder 网络后会得到符合隐变量 z 的分布, 用这个分布来算 \\(D_{KL}(q_{\\phi}(z|x), p_\\theta(z))\\) 我们选择的这些分布都是某种高斯分布, 所以 DL 散度的计算可以展开, 见 PPT P16 然后从预测分布中采样获得 z, 喂给 decoder, 希望采样获得 \\(\\hat x\\)(数据重建), 并计算 \\(E_{z \\sim q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]\\) 图中蓝色和绿色的部分相互矛盾, 蓝色希望更接近原始数据, 而绿色部分希望限制 z 满足某种高斯分布 之后我们就能生成新数据了 z 代表某种高维信息, 通过调整 z, 我们可以做到调整图像的不同特征, 例如人的表情, 物体的朝向, VAE 论文中有这样的可视化 VAE 的优点是快速, 有丰富的隐变量 z, 缺点是生成的图像有点模糊, 最大化的是下界而非 p(x), 我们不禁想它能不能与自回归模型结合一下, VQ-VAE2 模型做到了平衡, 大致思想是 VAE(离散映射 z) + PixelCNN(在隐变量空间 z 跑), 效果特别好 本节课主要介绍生成对抗网络(Generative Adversarial Networks)(GAN), 完全放弃显式建模密度函数, 只关心采样 假设数据 \\(x_i\\) 来自分布 \\(p_{data}(x)\\), 我们想要训练一个模型可以从 \\(p_{data}\\) 中采样 我们引入隐变量 \\(z\\), 假设其有先验分布 \\(p(z)\\), 我们采样一些 \\(z\\), 将它们传入生成网络 \\(G\\) 中, 使得 \\(x = G(z)\\) 所以可以称作 \\(x\\) 来自分布 \\(p_G\\), 目标是训练 \\(p_G = p_{data}\\) 为了达到这个目标, 我们引入第二个网络——判别器网络(discriminator network), 记作 \\(D\\), 它将被训练来分辨来自生成器的样本以及来自数据集的真实样本(对于判别器网络这是一个监督学习任务(二元分类)), 所以我们要训练生成器来欺骗判别器, 这就是\"对抗\" 如果两个网络都会良好工作, 我们希望 \\(p_G\\) 会收敛到 \\(p_{data}\\) 这里的目标函数为: \\[ \\underset{G}{\\rm min} \\; \\underset{D}{\\rm max}(E_{X \\sim p_{data}}[\\log D(x)] + E_{z \\sim p(z)}[\\log (1 - D(G(z)) )]) \\] 我们用 1/0 表示数据为 真实/虚假 的, \\(D\\) 想要使真实数据(\\(x\\))输出 1 并且错误数据( \\(G(z)\\) )输出 0 \\(G\\) 求的是 \\(\\rm min\\) , 这说明其希望 \\(D(G(z)) = 1\\), 即错误数据输出 1 我们的训练是一个循环, 设目标函数为 \\(\\underset{G}{\\rm min} \\; \\underset{D}{\\rm max}\\;V(G,D)\\), 在每个时间步上, 先更新 \\(D = D + \\alpha_D \\displaystyle\\frac{\\partial V}{\\partial D}\\), 再更新 \\(G = G + \\alpha_G \\displaystyle\\frac{\\partial V}{\\partial G}\\), 注意我们不是最小化总 loss, 因为两种 loss 是矛盾的 在实践中, 我们一般训练 \\(G\\) 最小化 \\(- \\log ( D(G(z)) )\\), 避免开头的梯度消失问题使得训练无法开始 这种 minmax 形式能够到达全局最小值 \\(p_G = p_{data}\\), 这里的 ppt 有个证明, 我会后续详细补充 可以证明得到最好的 \\(D_G^*(x) = \\displaystyle\\frac{p_{data}(x)}{p_{data}(x) + p_G(x)}\\), 然而无法计算该值, 但可以把 \\(D_{G}^*(x)\\) 回代目标函数中 后面的推导大概是什么积分和期望的互换, 凑 KL 散度和 JS 散度, 最后目标函数为 \\(\\underset{G}{\\rm min} (2 \\times JS(p_{data},p_G)- \\log 4)\\), 这个函数的最小值正好落在 \\(p_G = p_{data}\\)​ 上, 而且是唯一的！ 当然有最小值不代表能够达到, 也许最小值正好落在 \\(G, D\\)​ 网络的架构之外, 也许根本收敛不了 这个领域自 2014 年提出后迅猛发展, DC-GAN 第一次很好结合了 CNN 和 GAN, 我们可以在 z 上进行插值, 可以生成平滑转化的插值图像, 同时可以做某种抽象的向量数学, 将图片作为某种\"向量\"相加减, 这是 NLP 中词向量的思想 另一些进步是更高的分辨率, StyleGAN 能够控制图像不同层次的特征, 还研究了特征解耦的问题 通过 \\(\\mathcal Z \\rightarrow \\mathcal W\\) 的变化, 使得在 \\(\\mathcal W\\) 到特征的过程中特征的线性被尽可能保留 能够生成特定物体的 GAN 称为 Conditional GAN, 自注意力机制也能引入 GAN, GAN 也能生成视频或进行图像编辑 在 2020 年, 扩散模型被提出, 次年 DALL-E 被提出, 现在你在网上随便就能生成 AI 图像, 当时的惊人成就现在看来已经平平无奇, 我后续可能会补充一下最新的生成式模型发展…… Lecture 21 强化学习 强化学习是一个很大的话题, 我们这节课仅作简要介绍 与监督学习和无监督学习范式不同, 强化学习(Reinforcement Learning)(RL)致力于构造能与环境交互的智能体(agent), 而不是建模从输入到输出的函数 智能体会与环境(environment)交互并作出操作, 基于这些操作智能体会得到奖励(reward), 最终目标是让智能体做出最大化奖励的操作 我们可以训练智能体, 但不能改变环境, 在强化学习中, 环境先提供给智能体一种状态 \\(s_t\\), 之后智能体做出某种操作 \\(a_t\\), 最后环境基于操作给出奖励 \\(r_t\\), 这是一个时间步上发生的 在多个时间步上展开时, 环境会因为操作而改变, 而智能体会学习 强化学习的一个经典问题叫做倒立摆问题, 车上有一根竖杆, 你要前后移动小车以让竖杆保持竖直 这里的系统状态有杆与竖直方向的夹角, 车速, 位置等; 智能体小车能做的操作(action)是在水平方向对车施加一个力; 环境的奖励可能是对于杆竖直的每一秒会给予一分 另一个强化学习的例子是机器人运动, 我们希望机器人像人一样行走, 这里的动作是对机器人各个关节施加的扭矩, 并对机器人站立以及机器人移动距离(站立是大前提)给予奖励 一种很明显的应用是让智能体学会玩电子游戏, 因为老式电子游戏一般都有游戏分数可以直接作为奖励使用, 动作也比较固定; 对于传统游戏而言, 最有名的任务就是围棋了, 早在 2017 年 AlphaGo 就已经让柯洁道心破碎了…… 如果把 state 换为 data, action 换为 prediction, reward 换为 loss, 强化学习和监督学习到底有什么区别？智能体和环境的交互不就正如模型和数据集的交互吗？ 完全不一样, 第一个原因是随机性(stochasticity), 状态和奖励可能是不完整的, 环境在时间步之间的变化也是非确定性的, 我们在 t 和 t+1 时间步上基于同样的状态做出同样的操作, 可能得到不同的奖励 第二个原因是信用分配(credit assignment), 奖励 \\(r_t\\) 并不完全依赖于操作 \\(a_t\\)​, 而是一系列操作的最终结果。当代理人收到奖励时, 它不知道奖励的来源, 信用分配即指在复杂学习系统中, 如何分配系统内部成员对结果的贡献, 而在监督学习中我们的 loss 直接来源于当时的 prediction 第三个原因是不可微分性(nondifferentiable), 一切都是不可微分的, 我们不是在学习函数 第四个原因是非平稳性(nonstationary), 状态 \\(s_{t+1}\\) 依赖于动作 \\(a_t\\), 状态数据的分布是可以随着时间改变的, 我们不可能把分布采样出来, 这类似于上节课 GAN 中的判别器 \\(D\\), 而监督学习一般使用静态数据集, 学习的是平稳分布 这一切问题使得 RL 更加困难, 也更加通用 从数学形式上来说, RL 是一个马尔可夫决策过程(Markov Decision Process)(MDP), 一个元组 \\((S,A,R,P,\\gamma)\\), 其中 \\(S\\) 为状态集, \\(A\\) 为动作集, \\(R\\) 为回报函数, \\(P\\) 为状态转移概率, 是当前状态和操作的函数, \\(\\gamma\\) 为折扣因子, 告诉我们智能体是只关注最大化立即奖励( $= 0 $ ), 还是最大化未来奖励( $= 1 $ ), 相当于一种权衡 RL 拥有马尔可夫性质, 即当 RL 在给定现在状态及所有过去状态情况下, 其未来状态的条件概率分布仅依赖于当前状态; 简单来说, 在给定现在状态时, 它与过去状态(即该过程的历史路径)是条件独立的 “在理想主义者面前，什么都是马尔科夫链。” 现在我们形式化智能体想要做什么, 它想学习一种策略(policy) \\(\\pi\\), 其中 \\(\\pi\\) 给出以状态为条件的操作分布, 我们的目标是找到 \\(\\pi^*\\) 最大化累计折扣奖励 \\(\\sum_t \\gamma^t r_t\\)​ 在 \\(t=0\\) 时, 环境采样初始状态 \\(s_0 \\sim p(s_0)\\) 符合某种先验分布, 之后在每一个时间步上, 智能体先选择操作 \\(a_t \\sim \\pi(a | s_t)\\), 然后环境采样奖励 \\(r_t \\sim R(r|s_t,a_t)\\), 然后环境采样下一状态 \\(s_{t+1} \\sim P(s|s_t,a_t)\\), 最后智能体接受奖励 \\(r_t\\) 和下一状态 \\(s_{t+1}\\) 我们介绍一个例子, 在二维网格中每个格子都是一个状态, 从随机状态开始, 智能体可以一次往一个方向走一步, 我们设置一些目标状态有正奖励, 每走一步有一定的惩罚 策略告诉智能体怎么行动, 糟糕的策略不关心状态, 而最优的策略就是建议智能体往最近的奖励点移动 为了去除寻找 \\(\\pi^*\\) 中的随机性, 我们最大化期望值 \\(\\pi^* = {\\rm arg\\;\\underset{\\pi}{max}} \\;\\mathbb E[\\;\\displaystyle\\sum\\limits_{t \\geq 0} \\gamma^t r_t | \\pi]\\) 下面介绍 Deep Q-learning 的思想 假设我们有一个策略 \\(\\pi\\), 执行该策略会给我们带来某种轨迹(trajectories): \\(s_0, a_0, r_0, s_1, a_1, r_1, \\dots\\) 我们想量化状态的好坏, 可以设价值函数 \\(V\\), 有(略去 \\(\\gamma\\)) \\(V^{\\pi}(s) = \\mathbb E [\\;\\displaystyle\\sum\\limits_{t \\geq 0} \\gamma^t r_t |s_0 =s, \\pi]\\) 代表从状态 \\(s\\) 开始执行策略 \\(\\pi\\) 的预期奖励 如果想量化 状态-操作对 的好坏, 就需要 \\(Q\\) 函数了, 有 \\(Q^{\\pi}(s,a) = \\mathbb E [\\;\\displaystyle\\sum\\limits_{t \\geq 0} \\gamma^t r_t |s_0 =s, a_0 = a, \\pi]\\) 代表从状态 \\(s\\) 开始然后采取行动 \\(a\\), 开始执行策略 \\(\\pi\\) 的预期奖励 为了找 \\(\\pi^*\\), 其对应的 \\(Q\\) 函数为 \\(Q^{*}(s,a) = \\underset{\\pi}{\\rm max} \\;\\mathbb E [\\;\\displaystyle\\sum\\limits_{t \\geq 0} \\gamma^t r_t |s_0 =s, a_0 = a, \\pi]\\), 则 \\(\\pi ^*(s) = {\\rm arg\\;max}_{a'} \\;Q(s, a')\\), 这样就把 \\(\\pi\\) 摘出去了 关于 \\(Q^*\\) 函数, 其满足贝尔曼方程 \\(Q^{*}(s,a) = \\mathbb E_{r,s'} [r + \\gamma \\; \\underset{a'}{\\rm max}\\;Q^{*}(s',a')]\\), 其中 \\(r \\sim R(s,a), s' \\sim P(s,a)\\) 这代表我们可以递归定义 \\(Q^{*}\\), 这其实是隐藏在数学形式化语言中的废话 关于贝尔曼方程很好的一点是: 满足它的任意 \\(Q(s,a)\\) 一定是最好的 \\(Q^{*}\\), 我们在这里略去证明 根据这个思想, 我们从随机的 \\(Q\\) 开始, 然后使用贝尔曼方程作为更新规则, 有 \\(Q_{i+1}(s,a) = \\mathbb E_{r,s'} [r + \\gamma \\; \\underset{a'}{\\rm max}\\;Q_{i}(s',a')]\\)​, 当 \\(i \\rightarrow \\infty\\) 时, \\(Q_i\\) 一定会收敛到 \\(Q^{*}\\), 我们同样略去证明 期望的计算需要对所有的 \\((s,a)\\) 对处理, 开销过大, 我们一般用神经网络去学习 \\(Q(s,a)\\), 用贝尔曼方程来定义 loss 对于权重为 \\(\\theta\\) 的神经网络, 我们想要学习 \\(Q^{*}(s,a) \\approx Q(s,a;\\theta)\\) 贝尔曼方程会告诉我们 \\(y_{s,a,\\theta} = \\mathbb E_{r,s'} [r + \\gamma \\; \\underset{a'}{\\rm max}\\;Q^{*}(s',a';\\theta)]\\), 可定义 loss, \\(L(s,a) = (Q(s,a;\\theta) - y_{s,a,\\theta})^2\\) 这就叫做 Deep Q-learning, Deep 即指神经网络, 因为 Q-learning 1989 年就提出了 一切问题看似都解决了, 但实践上还有 \\(Q(s,a)\\) 依赖于当前权重 \\(\\theta\\) , 并且 batch 不好取等问题 2015 年的 DQN 算法通过经验回放打破了数据间的关联性, 也帮助取了 batch, 这里有一篇介绍 强化学习领域的另一重要算法为策略梯度(Policy Gradients), 直接用神经网络学习 \\(\\pi_{\\theta}(a|s)\\) 期望奖励函数为 \\(J(\\theta) = \\mathbb E_{r \\sim p_{\\theta}} [\\displaystyle\\sum\\limits_{t \\geq 0} \\gamma^t r_t]\\) , 所以要最大化 \\(\\theta^{*} = {\\rm arg\\;max}_{\\theta} J(\\theta)\\) 来找 \\(\\pi^{*}\\) 这就是我们用梯度上升的地方, 但由于不可微分性, \\(\\displaystyle\\frac{\\partial J}{\\partial \\theta}\\) 没法算 我们还是要推一些式子, 做一些模拟, 来做某种采样算这玩意, 当然我根本不会所以你要去看 ppt 设 \\(J(\\theta) = \\mathbb E_{x \\sim p_{\\theta}} [f(x)]\\) 最终有 \\(\\displaystyle\\frac{\\partial J}{\\partial \\theta} = \\mathbb E_{x \\sim p_{\\theta}} [f(x) \\displaystyle\\sum\\limits_{t \\geq 0}\\displaystyle\\frac{\\partial}{\\partial \\theta} \\log \\pi_{\\theta}(a_t|s_t)]\\) 这里的 \\(f(x)\\) 是奖励, \\(\\log \\pi_{\\theta}(a_t|s_t)\\) 是模型预测分数关于权重的梯度 训练流程是这样的: 一开始随机化 \\(\\theta\\), 然后跑一会 \\(\\pi_{\\theta}\\) 来收集轨迹数据, 然后算 \\(\\displaystyle\\frac{\\partial J}{\\partial \\theta}\\), 梯度上升, 回到第二步 公式的部分解释是这样的, 当 f(x) 高时, 轨迹上的数据更有可能, 当 f(x) 较小时则相反 策略梯度算法需要大量数据来采样以完成信用分配, 可能真的很难训练 上图就是两种算法的总结, 我估计明早醒来就看不懂了, 但这就是强化学习的入门简单算法 还有很多算法, 例如演员-评论家算法(Actor-Critic), Model-Based 算法(学习环境如何改变), 模仿学习, 逆强化学习(通过观察专家来最大化奖励函数), 你还可以把对抗学习的思想用在 RL 上, 或用 RL 来训练拥有不可微部分的神经网络, 这是随机计算图的概念 Assignment 6 GAN 在 MNIST 上训练一个 GAN, 要完成生成网络 \\(G\\) 和判别网络 \\(D\\) 先写一个 sample_noise() 函数, 生成 (-1,1) 区间随机数, 注意 torch.rand() 只能生成 [0,1) 范围的, 所以要通过 *2-1 映射 判别网络 \\(D\\) 直接拿 nn.Sequential 架, 注意先要 flatten, 这一套流程都很熟悉了 生成网络 \\(G\\) 也这么直接架, linear-ReLU-linear-ReLU-linear-tanh 然后写 \\(D\\) 和 \\(G\\) 的 loss 函数, 有 \\[ \\ell_G = -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]\\\\ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right] \\] 这两个函数都是要最小化的, 我们假设 \\(D(x)\\) 给出一个 (0,1) 间的实数, 为了得到稳定的数值 loss, 需要调用一个很长的方法 torch.nn.functional.binary_cross_entropy_with_logits, 缩写为 bce_loss $ bce(s, y) = -y * ((s)) - (1 - y) * (1 - (s)) $, 通过操作 y=0/1 就可以把我们想要的项摘出来 12345# logits_real 为 D(x), logits_false 为 D(G(z))N = logits_real.shape[0]true_labels = torch.ones_like(logits_real, device=logits_real.device)false_labels = torch.zeros_like(logits_false, device=logits_false.device)loss = bce_loss(logits_real,true_labels) + bce_loss(logits_fake,false_labels) 以 \\(\\ell_D\\) 为例, loss 就这么写 然后以 Adam 为优化器, 一行 optimizer = optim.Adam(model.parameters(),lr=1e-3,betas=(0.5,0.999)) 搞定, 然后就可以开训了, 一开始是黑白噪音, 几千个迭代后就有明显数字结构了 下面我们介绍最小二乘 GAN(Least Square GAN)(LSGAN), 把目标函数改为平方误差, 其他不变, 可以证明优化 LSGAN 等价于优化皮尔森卡方散度, 效果更好, 这是论文, 这是科普, 此时有: \\[ \\ell_G = \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right] \\\\ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right] \\] 这里 \\(-\\infty &lt; D(x) &lt; \\infty\\) , loss 函数更好写了, 以 \\(\\ell_D\\) 为例, 一行就行: loss = torch.mean(((scores_real-1)**2 + scores_fake**2)/2) 下面来实现 DC-GAN 的部分思想, 用 CNN 做判别网络和生成网络以实现对图像\"尖锐边缘\"的处理, \\(D, G\\) 的架构都给你了, \\(G\\) 中要用到 nn.ConvTranspose2d 来扩大尺寸, 在 Lecture 16 中提到过, 这次跑的效果挺好, 没有之前的残余噪点 我们最后以一张隐变量 \\(z\\)​ 上插值的生成的 MNIST 数字图像来结束这次作业 Lecture 22 总结 与 开放问题 笔记本身就是总结了, 我再把课程的总结做成笔记岂不是总结的总结, 那也太蠢了, 自己看网课吧 接下来呢？深度学习视觉领域会如何发展？ 更新的模型是肯定的, Justin 在 2019 年说 Neural ODE 特别有趣, 它可以将神经网络表示为微分方程, 这是现在扩散模型的基础 更多的应用是肯定的, 像是医学成像、更多学科的识别问题; 更有趣的应用是拿深度学习改良传统计算机的数据结构, 比如拿神经网络做哈希表; 或者拿深度学习做数学, 例如数学证明, 在近几年这个领域(DL4ATP)正在飞速发展 深度学习也会需要更多的数据, 每几个月最新的模型需要的算力会增加一倍, GPU 在迅猛发展, 在 2025 年来看的确是这样的 人工智能也有很多问题亟须社区解决的问题 一是模型是有偏见的, 例如在我们提过的词向量中, 在大量学习后模型会拥有人类社会的偏见, 例如女性通常会对应家庭主妇, 男性会对应建筑师, 还有一些黑人会对应大猩猩; 大公司通常的训练集来源于西方世界, 例如一个分类器能够轻易辨别中产阶级家里的肥皂, 但却会在一些贫穷地区做分类时灾难性失败 二是我们需要新的理论, 神经网络的大部分权重可以被弃置, 但是效果不会变化, 甚至一个随机权重的神经网络在仅经过修建后能够完成分类任务; 模型会过拟合, 但是在随机标签的 CIFAR-10 上甚至能够完美拟合, 有时候还会出现双拟合现象, 在模型的复杂度超过一个临界点时, 突然就不过拟合了 三是我们需要更多的数据, 但收集数据的成本很高, 一个解决方法是做 low-shot learning, 在小样本上学习, 例如 Lecture 2 介绍过的 Omniglot 数据集; 另一个方法是做自监督学习, 在未标记数据上训练后在少部分标记数据上微调, 有关的任务是拼图、黑白图上色和图像扩充; 2020 年的 SimCLR 是这个领域的里程碑 四是语言模型缺少一种最基本的人类常识, 和它们聊天会发现它们不懂一些基础知识, 这个问题在近几年被解决了一部分, 但 Justin 也疑问仅仅靠更大的模型能让我们走向 AGI 吗; 在一些图像识别任务中, 模型可能无法处理一些反常事务, 如果在狭小房间里有一只大象, 人类可以轻易认出来, 但模型可能不行因为它从没被训练过谁的房间里有一只大象, 它可能把大象认成椅子 到此 EECS498 课程部分全部结束, 下面是一些个人感想 本门课可以排进我上过最好的课程前三名, 做到了把一位无知的同学(我)带入深度学习的大门。课程大致分为两部分, 前半截是一些基础原理, 而后半截是一些视野扩展和当下研究, 主讲老师 Justin 在这门 2019 年的课程中甚至介绍了当时还未正式发布的论文, ppt 中一堆 17-19 年的\"最新\"论文让我感受到这门学科的年轻, 同时也感叹 Justin 为什么近几年不再教这门课。目前关注最新进展的课可能就是 cs231n 了, 视频还是 17 年的但是可以下载 25 年的 ppt, 在完成这样一坨巨大的笔记后我会摆很长一段时间, 后续可能会做一些补充笔记 回到话题, 本节课的 Assignments 也给我打下了一定的 pytorch 基础, 个人认为 Assignment 4 最难, 学了调库但还是要手搓 RNN, LSTM 和 RNN 又长又难调, 前前后后写了有一周; Assignment 1 对新手最不友好, 广播和凑 shape 的思想没学会前抄了很多前人的代码; 最后写到最简单的 Assignment 6, 将那么高级的 GAN 只用几行代码架构时有种拨得云开见月明的感觉, Assignments 可以说是这门课程的核心, 希望大家做 Assignments 时也能有这种享受 从我在 CS自学指南 上看到这门课到下决定学习用的时间很短, 但学习的时间很长, 估计总共奔着 120h 去了。我比较蠢, 尤其是记忆不好, 为了辅助学习开了这份笔记, 如果它还能为某个有志于学习这门课的人提供一些帮助那就再好不过了","link":"/2025/03/27/EECS498-notes/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"编程竞赛","slug":"编程竞赛","link":"/tags/%E7%BC%96%E7%A8%8B%E7%AB%9E%E8%B5%9B/"},{"name":"matlab","slug":"matlab","link":"/tags/matlab/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"CV","slug":"CV","link":"/tags/CV/"}],"categories":[{"name":"编程技术","slug":"编程技术","link":"/categories/%E7%BC%96%E7%A8%8B%E6%8A%80%E6%9C%AF/"},{"name":"算法竞赛","slug":"算法竞赛","link":"/categories/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B/"},{"name":"人工智能","slug":"人工智能","link":"/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}],"pages":[{"title":"分类","text":"","link":"/categories/index.html"}]}